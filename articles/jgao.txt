hdinsight-administer-use-command-line.md:<properties linkid="manage-services-hdinsight-administer-hdinsight-using-command-line" urlDisplayName="HDInsight Administration" pageTitle="Administer HDInsight using using the Cross-Platform Command-Line Interface | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure" description="Learn how to use the Cross-Platform Command-Line Interface to manage HDInsight clusters on any platform that supports Node.js, including Windows, Mac, and Linux." umbracoNaviHide="0" disqusComments="1" writer="jgao" editor="cgronlun" manager="paulettm" title="Administer HDInsight using the Cross-platform Command-line Interface"/>
hdinsight-administer-use-command-line.md:# Administer HDInsight using the Cross-platform Command-line Interface
hdinsight-administer-use-command-line.md:In this article, you learn how to use the Cross-Platform Command-Line Interface to manage HDInsight clusters. The command-line tool is implemented in Node.js. It can be used on any platform that supports Node.js including Windows, Mac and Linux. 
hdinsight-administer-use-command-line.md:The command-line tool is open source.  The source code is managed in GitHub at <a href= "https://github.com/WindowsAzure/azure-sdk-tools-xplat">https://github.com/WindowsAzure/azure-sdk-tools-xplat</a>. 
hdinsight-administer-use-command-line.md:This article only covers using the command-line interface from Windows. For a general guide on how to use the command-line interface, see [How to use the Windows Azure Command-Line Tools for Mac and Linux][azure-command-line-tools]. For comprehensive reference documentation, see [Windows Azure command-line tool for Mac and Linux][azure-command-line-tool].
hdinsight-administer-use-command-line.md:* [Download and import Windows Azure account publishsettings](#importsettings)
hdinsight-administer-use-command-line.md:* [List and show clusters](#listshow)
hdinsight-administer-use-command-line.md:The command-line interface can be installed using *Node.js Package Manager (NPM)* or Windows Installer.
hdinsight-administer-use-command-line.md:**To install the command-line interface using NPM**
hdinsight-administer-use-command-line.md:2.	Click **INSTALL** and following the instructions using the default settings.
hdinsight-administer-use-command-line.md:3.	Open **Command Prompt** (or *Windows Azure Command Prompt*, or *Developer Command Prompt for VS2012*) from your workstation.
hdinsight-administer-use-command-line.md:4.	Run the following command in the command prompt window.
hdinsight-administer-use-command-line.md:	<p>If you get an error saying the NPM command is not found, verify that the following paths are in the PATH environment variable:
hdinsight-administer-use-command-line.md:5.	Run the following command to verify the installation:
hdinsight-administer-use-command-line.md:**To install the command-line interface using windows installer**
hdinsight-administer-use-command-line.md:2.	Scroll down to the **Command line tools** section, and then click **Cross-platform Command Line Interface** and follow the Web Platform Installer wizard.
hdinsight-administer-use-command-line.md:##<a id="importsettings"></a> Download and import Windows Azure account publishsettings
hdinsight-administer-use-command-line.md:Before using the command-line interface, you must configure connectivity between your workstation and Windows Azure. Your Windows Azure subscription information is used by the command-line interface to connect to your account. This information can be obtained from Windows Azure in a publishsettings file. The publishsettings file can then be imported as a persistent local config setting that the command-line interface will use for subsequent operations. You only need to import your publishsettings once.
hdinsight-administer-use-command-line.md:**To download and import publishsettings**
hdinsight-administer-use-command-line.md:1.	Open a **Command Prompt**.
hdinsight-administer-use-command-line.md:2.	Run the following command to download the publishsettings file.
hdinsight-administer-use-command-line.md:	The command shows the instructions for downloading the file, including an URL.
hdinsight-administer-use-command-line.md:3.	Open **Internet Explorer** and browse to the URL listed in the command prompt window.
hdinsight-administer-use-command-line.md:5.	From the command prompt window, run the following command to import the publishsettings file:
hdinsight-administer-use-command-line.md:After you have imported the publishsettings file, you can use the following command to create a storage account:
hdinsight-administer-use-command-line.md:If you have already had a storage account but do not know the account name and account key, you can use the following commands to retrieve the information:
hdinsight-administer-use-command-line.md:For details on getting the information using the management portal, see the *How to: View, copy and regenerate storage access keys* section of [How to Manage Storage Accounts][azure-manage-storageaccount].
hdinsight-administer-use-command-line.md:The *azure hdinsight cluster create* command creates the container if it doesn't exist. If you choose to create the container beforehand, you can use the following command:
hdinsight-administer-use-command-line.md:Once you have the storage account and the blob container prepared, you are ready to create a cluster: 
hdinsight-administer-use-command-line.md:Typically, you provision an HDInsight cluster, run jobs on it, and then delete the cluster to cut down the cost. The command-line interface gives you the option to save the configurations into a file, so that you can reuse it every time you provision a cluster.  
hdinsight-administer-use-command-line.md:##<a id="listshow"></a> List and show cluster details
hdinsight-administer-use-command-line.md:Use the following commands to list and show cluster details:
hdinsight-administer-use-command-line.md:Use the following command to delete a cluster:
hdinsight-administer-use-command-line.md:* [How to use the Windows Azure Command-Line Tools for Mac and Linux][azure-command-line-tools]
hdinsight-administer-use-command-line.md:* [Windows Azure command-line tool for Mac and Linux][azure-command-line-tool]
hdinsight-administer-use-command-line.md:[azure-command-line-tools]: /en-us/develop/nodejs/how-to-guides/command-line-tools/
hdinsight-administer-use-command-line.md:[azure-command-line-tool]: /en-us/manage/linux/other-resources/command-line-tools/
hdinsight-administer-use-command-line.md:[image-cli-account-download-import]: ./media/hdinsight-administer-use-command-line/HDI.CLIAccountDownloadImport.png 
hdinsight-administer-use-command-line.md:[image-cli-clustercreation]: ./media/hdinsight-administer-use-command-line/HDI.CLIClusterCreation.png
hdinsight-administer-use-command-line.md:[image-cli-clustercreation-config]: ./media/hdinsight-administer-use-command-line/HDI.CLIClusterCreationConfig.png
hdinsight-administer-use-command-line.md:[image-cli-clusterlisting]: ./media/hdinsight-administer-use-command-line/HDI.CLIListClusters.png "List and show clusters"
hdinsight-administer-use-management-portal.md:<properties linkid="manage-services-hdinsight-howto-administer-hdinsight" urlDisplayName="Administration" pageTitle="Administer HDInsight clusters with Management Portal | Windows Azure" metaKeywords="" description="Learn how to administer HDInsight Service. Create an HDInsight cluster, open the interactive JavaScript console, and open the Hadoop command console." metaCanonical="" services="hdinsight" documentationCenter="" title="Administer HDInsight clusters using Management Portal" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-administer-use-management-portal.md:Using the Windows Azure management portal, you can provision HDInsight clusters, change the Hadoop user password, and enable RDP so you can access the Hadoop command console on the cluster. There are also other tools available for administrating HDInsight in addition to the portal. 
hdinsight-administer-use-management-portal.md:- For more information on administering HDInsight using the Cross-platform Command-line Tools, see [Administer HDInsight Using Cross-platform Command-line Interface][hdinsight-admin-cross-platform]. 
hdinsight-administer-use-management-portal.md:- **Windows Azure storage account**. This storage account must be created in the same data center in which your HDInsight cluster is to be provisioned. Currently HDInsight clusters can only be provisioned in five data centers: Southeast Asia, North Europe, West Europe, East US and West US. So your Windows Azure storage account needs to be created in one of these data centers. For details on creating a Windows Azure storage account, see [How to Create a Storage Account][azure-create-storageaccount].
hdinsight-administer-use-management-portal.md:* [Change HDInsight cluster username and password](#password)
hdinsight-administer-use-management-portal.md:* [Open Hadoop command console](#hadoopcmd)
hdinsight-administer-use-management-portal.md:2. Click **+ NEW** on the bottom of the page, click **DATA SERVICES**, click **HDINSIGHT**, and then click **QUICK CREATE**.
hdinsight-administer-use-management-portal.md:3. Provide **Cluster Name**, **Cluster Size**, **Cluster Admin Password**, and a Windows Azure **Storage Account**, and then click **Create HDInsight Cluster**. Once the cluster is created and running, the status shows *Running*.
hdinsight-administer-use-management-portal.md:	<p>Important: Once a Windows Azure storage account is chosen for your HDInsight cluster, the only way to change the storage account is to delete the cluster and create a new cluster with the desired storage account.</p>
hdinsight-administer-use-management-portal.md:4. Click the newly created cluster.  It shows the landing page:
hdinsight-administer-use-management-portal.md:	![HDI.ClusterLanding][image-cluster-landing]
hdinsight-administer-use-management-portal.md:HDInsight works with a wide range of Hadoop components. For the list of the components that have been verified and supported, see [What version of Hadoop is in Windows Azure HDInsight][hdinsight-version]. HDInsight customization can be done using one of the following options:
hdinsight-administer-use-management-portal.md:- Some native Java components, like Mahout, Cascading, can be run on the cluster as JAR files. These JAR files can be distributed to Windows Azure Blob storage (WASB), and submitted to HDInsight clusters using Hadoop job submission mechanisms. For more information see [Submit Hadoop jobs programmatically][hdinsight-submit-jobs].
hdinsight-administer-use-management-portal.md:##<a id="password"></a> Change the HDInsight cluster username and password
hdinsight-administer-use-management-portal.md:**To change HDInsight cluster username and password**
hdinsight-administer-use-management-portal.md:2. Click **HDINSIGHT** on the left pane. You will see a list of deployed HDInsight clusters.
hdinsight-administer-use-management-portal.md:3. Click the HDInsight cluster that you want to reset the username and password.
hdinsight-administer-use-management-portal.md:6. Click **SAVE** on the bottom of the page, and wait for the disabling to complete.
hdinsight-administer-use-management-portal.md:8. Enter **USER NAME** and **NEW PASSWORD**.  These will be the new username and password for the cluster.
hdinsight-administer-use-management-portal.md:The credentials for the cluster that you provided at its creation give access to the services on the cluster, but not to the cluster itself through remote desktop. Remote Desktop access is turned off by default and so direct access to the cluster using it requires some additional, post-creation configuration.
hdinsight-administer-use-management-portal.md:2. Click **HDINSIGHT** on the left pane. You will see a list of deployed HDInsight clusters.
hdinsight-administer-use-management-portal.md:6. In the **Configure Remote Desktop** wizard, enter a username and password for the remote desktop. Note that the username must be different than the one used to create the cluster (*admin* by default with the Quick Create option). Enter an expiration date in the **EXPIRES ON** box. Note that the expiration date must be in the future and no more than a week from the present. The expiration time of day is assumed by default to be midnight of the specified date. Then click the check icon.
hdinsight-administer-use-management-portal.md:	The expiration date must be in the future, and at most seven days from now. And the time is the midnight of the selected date.
hdinsight-administer-use-management-portal.md:##<a id="hadoopcmd"></a> Open Hadoop command line
hdinsight-administer-use-management-portal.md:To connect to the cluster using remote desktop and use the Hadoop command line, you must first have enabled remote desktop access to the cluster as described in the previous section. 
hdinsight-administer-use-management-portal.md:**To open Hadoop command line**
hdinsight-administer-use-management-portal.md:2. Click **HDINSIGHT** on the left pane. You will see a list of deployed Hadoop clusters.
hdinsight-administer-use-management-portal.md:6. Enter your credentials, and then click **OK**.  Use the username and password you configured when you created the cluster.
hdinsight-administer-use-management-portal.md:8. From the desktop, double-click **Hadoop Command Line**.
hdinsight-administer-use-management-portal.md:	![HDI.HadoopCommandLine][image-hadoopcommandline]
hdinsight-administer-use-management-portal.md:	For more information on Hadoop command, see [Hadoop commands reference][hadoop-command-reference].
hdinsight-administer-use-management-portal.md:In this article, you have learned how to create an HDInsight cluster using the Windows Azure Management Portal, and how to open the Hadoop command line tool. To learn more, see the following articles:
hdinsight-administer-use-management-portal.md:* [Administer HDInsight Using Cross-platform Command-line Interface][hdinsight-admin-cross-platform]
hdinsight-administer-use-management-portal.md:[hdinsight-admin-cross-platform]: /en-us/manage/services/hdinsight/administer-hdinsight-using-command-line-interface/
hdinsight-administer-use-management-portal.md:[hadoop-command-reference]: http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html
hdinsight-administer-use-management-portal.md:[image-cluster-landing]: ./media/hdinsight-administer-use-management-portal/HDI.ClusterLanding.PNG "Cluster landing page"
hdinsight-administer-use-management-portal.md:[image-hadoopcommandline]: ./media/hdinsight-administer-use-management-portal/HDI.HadoopCommandLine.PNG "Hadoop command line"
hdinsight-administer-use-powershell.md:Windows Azure PowerShell is a powerful scripting environment that you can use to control and automate the deployment and management of your workloads in Windows Azure. In this article, you will learn how to manage HDInsight clusters using a local Windows Azure PowerShell console through the use of Windows PowerShell. For the list of the HDInsight PowerShell cmdlets, see [HDInsight cmdlet reference][hdinsight-powershell-reference].
hdinsight-administer-use-powershell.md:- Install and configure Windows Azure PowerShell. For the detailed instructions, see [Install and configure Windows Azure PowerShell][Powershell-install-configure].
hdinsight-administer-use-powershell.md:* [List and show clusters](#listshow)
hdinsight-administer-use-powershell.md:HDInsight uses a Windows Azure Blob Storage container as the default file system. A Windows Azure storage account and storage container are required before you can create an HDInsight cluster. 
hdinsight-administer-use-powershell.md:After you have imported the publishsettings file, you can use the following command to create a storage account:
hdinsight-administer-use-powershell.md:If you have already had a storage account but do not know the account name and account key, you can use the following commands to retrieve the information:
hdinsight-administer-use-powershell.md:For details on getting the information using the management portal, see the *How to: View, copy and regenerate storage access keys* section of [How to Manage Storage Accounts](/en-us/manage/services/storage/how-to-manage-a-storage-account/).
hdinsight-administer-use-powershell.md:Once you have the storage account and the blob container prepared, you are ready to create a cluster. In this version you need to explicitly specify subscription information and certificate for cmdlets.   
hdinsight-administer-use-powershell.md:##<a id="listshow"></a> List and show cluster details
hdinsight-administer-use-powershell.md:Use the following commands to list and show cluster details:
hdinsight-administer-use-powershell.md:Use the following command to delete a cluster:
hdinsight-administer-use-powershell.md:	# Run the job and show the standard error 
hdinsight-administer-use-powershell.md:	$wordCountJobDefinition | Start-AzureHDInsightJob -Cluster $clusterName | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | %{ Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $_.JobId -StandardError}
hdinsight-administer-use-powershell.md:For more information on developing and running MapReduce jobs, see [Using MapReduce with HDInsight][hdinsight-mapreduce].
hdinsight-administer-use-powershell.md:The Hive job will first show the Hive tables created on the cluster, and the data returned from the hivesampletable.
hdinsight-administer-use-powershell.md:* [Administer HDInsight using command-line interface][hdinsight-admin-cli]
hdinsight-administer-use-powershell.md:[hdinsight-admin-cli]: /en-us/documentation/articles/hdinsight-administer-use-command-line/
hdinsight-analyze-twitter-date-hive.md:<properties linkid="manage-services-hdinsight-howto-social-data" urlDisplayName="Analyze Data with HDInsight" pageTitle="Analyzing Twitter data with Hive - Windows Azure Services" metaKeywords="" description="In this tutorial you will query, explore, and analyze data from Twitter using the HDInsight Service for Windows Azure and a complex Hive example." metaCanonical="" services="" documentationCenter="" title="Analyzing Twitter Data with Hive" authors=""  solutions="" writer="wenming" manager="" editor=""  />
hdinsight-analyze-twitter-date-hive.md:In this tutorial you will query, explore, and analyze data from twitter using the Apache Hadoop-based HDInsight Service for Windows Azure and a complex Hive example. Social web sites are one of the major driving forces for Big Data adoption. Public APIs provided by sites like Twitter are a useful source of data for analyzing and understanding popular trends. This tutorial assumed that an HDInsight cluster has already been created through the [Windows Azure Portal](http://www.windowsazure.com).  
hdinsight-analyze-twitter-date-hive.md:1. [Get Twitter Feed using cURL and Twitter Streaming API](#segment1)
hdinsight-analyze-twitter-date-hive.md:## Get Twitter Feed using cURL and Twitter Streaming API 
hdinsight-analyze-twitter-date-hive.md:3. Create these two files using notepad, **get_twitter_stream.cmd** and **twitter_params.txt**  in the same folder as **curl.exe**, as follows:
hdinsight-analyze-twitter-date-hive.md:5. Edit the **get_twitter_stream.cmd** window command script, adding your twitter username in place of **USER** and password in place of **PASSWORD** on the following line:
hdinsight-analyze-twitter-date-hive.md:6. Execute the get_twitter_stream.cmd script from a command prompt as follows:
hdinsight-analyze-twitter-date-hive.md:7. You can stop the job by pressing **Ctrl+C**. You can then rename the file and then restart the script. You may leave the process running for 10 minutes to hours. For the purpose of this tutorial please limit the data size to a few hundred megabytes.
hdinsight-analyze-twitter-date-hive.md:	**Navigate** to HDInsight, **Select** the Cluster you have created. Click on **Connect RDP** icon at the bottom of the screen. Log into the RDP session by typing in your password. Once you have logged in, **open** an Explorer window and navigate to c:\.
hdinsight-analyze-twitter-date-hive.md:3. Now, **Ctrl-C** on the zip file on your local machine and then navigate to c:\ on the remote desktop session's C:\.  **Ctrl-V** after you clicked on the explorer Window (C:\) to upload the zip file through the RDP session.
hdinsight-analyze-twitter-date-hive.md:4. After the file has been uploaded, **right click**, **select** Extract All to c:\ to get the original text file back.  **Open** a Hadoop Command Line Window to start working with HIVE and Hadoop commands.
hdinsight-analyze-twitter-date-hive.md:5. The first step is, **type** C:\ and **Press Enter**.  This will take you to the c:\ folder where the twitter data resides.
hdinsight-analyze-twitter-date-hive.md:7. Now the raw twitter data has been copied into HDFS on your HDInsight Cluster.  The next step is to create a simple table structure for the data that we loaded.  This temporary Hive structured table that allows us to hold the data and do further ETL processing. type notepad and paste the following code into notepad and save it as: "c:\load_twitter_raw.hql"
hdinsight-analyze-twitter-date-hive.md:8. Once the file has been saved, run Hive in the command window:
hdinsight-analyze-twitter-date-hive.md:11. Before we can query against the twitter dataset using Hive, we need to run another ETL process. We will define a more detailed table schema for the data we have stored in the "twitter_raw" table.  This more complex ETL process takes longer. At the command prompt launch notepad again to paste the following Hive query code, then save it as "c:\twitter_etl.hql.txt".
hdinsight-analyze-twitter-date-hive.md:13. This complex Hive script will kick off a set of long Map Reduce jobs by the Hadoop cluster.  Depending on your dataset and the size of your cluster, this could take about 10 minutes.
hdinsight-analyze-twitter-date-hive.md:	![Monitoring and Tracking jobs](./media/hdinsight-analyze-twitter-date-hive/twitter_longjob_browser.PNG)
hdinsight-analyze-twitter-date-hive.md:In this tutorial we have seen how to transform unstructured Json dataset into structured Hive table to query, explore, and analyze data from Twitter using HDInsight on Windows Azure. For updates and support files, you may find them in the GitHub repository [here](https://github.com/wenming/BigDataSamples/tree/master/twittersample). 
hdinsight-analyze-twitter-date-hive.md:While this article demonstrates using the Hadoop command line, you can also perform tasks using the HDInsight Service Interactive Console. For more information, see [Guidance: HDInsight Interactive JavaScript and Hive Consoles][interactive-console].
hdinsight-avoid-blob-storage-throttling.md:TODO: provide a short description of the article and what skills will be gained by following the steps in the guide.
hdinsight-avoid-blob-storage-throttling.md:TODO: This section provides a conceptual overview of the feature or service.  A diagram is sometimes useful in visualizing how things are related to one another.  Thie information in this section is generally programming-language-neutral.  For third party services, it's fine to provide a brief overview and link to other documentation on the service provider's website for more information.
hdinsight-component-versioning.md:<properties linkid="manage-services-hdinsight-version" urlDisplayName="HDInsight Hadoop Version" pageTitle="What's new in the cluster versions provided by HDInsight?" metaKeywords="hdinsight, hadoop, hdinsight hadoop, hadoop azure" description="HDInsight supports multiple Hadoop cluster versions deployable at any time. See the Hadoop and HortonWorks Data Platform (HDP) distribution versions supported." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="What's new in the cluster versions provided by HDInsight?"/>
hdinsight-component-versioning.md:Windows Azure HDInsight now supports Hadoop 2.2 with HDinsight cluster version 3.0 and takes full advantage of this platform to provide a range of significant benefits to customers. These include, most notably:
hdinsight-component-versioning.md:- Hive: Order of magnitude improvements to Hive query response times (up to 40x) and to data compression (up to 80%) using the Optimized Row Columnar (ORC) format.
hdinsight-component-versioning.md:- YARN: A new, general-purpose, distributed, application management framework that has replaced the classic Apache Hadoop MapReduce framework for processing data in Hadoop clusters. It effectively serves as the Hadoop operating system, and takes Hadoop from a single-use data platform for batch processing to a multi-use platform that enables batch, interactive, online and stream processing. This new management framework improves scalability and cluster utilization according to criteria such as capacity guarantees, fairness, and service-level agreements.
hdinsight-component-versioning.md:Creation of HDInsight 3.0 clusters on Hadoop 2.2 is supported by the Windows Azure Portal, the HDinsight SDK, and by Windows Azure PowerShell.
hdinsight-component-versioning.md:With the release of Windows Azure HDInsight on Hadoop 2.2, Microsoft has make HDInsight available in all major Azure geographies with the exception of Greater China. Specifically, west Europe and southeast Asia data centers have been brought online. This enables customers to locate clusters in a data center that is close and potentially in a zone of similar compliance requirements. 
hdinsight-component-versioning.md:Only the "wasb://" syntax is supported in HDInsight 3.0 clusters. The older "asv://" syntax is supported in HDInsight 2.1 and 1.6 clusters, but it is not supported in HDInsight 3.0 clusters and it will not be supported in later versions. This means that any jobs submitted to an HDInsight 3.0 cluster that explicitly use the “asv://” syntax will fail. The wasb:// syntax should be used instead. Also, jobs submitted to any HDInsight 3.0 clusters that are created with an existing metastore that contains explicit references to resources using the asv:// syntax will fail. These metastores will need to be recreated using the wasb:// to address resources.
hdinsight-component-versioning.md:HDInsight supports multiple Hadoop cluster versions that can be deployed at any time. Each version choice provisions a specific version of the HortonWorks Data Platform (HDP) distribution and a set of components that are contained within that distribution.
hdinsight-component-versioning.md:Windows Azure HDInsight now supports Hadoop 2.2. It is based on the Hortonworks Data Platform version 2.0 and provides Hadoop services with the component versions itemized in the following table:
hdinsight-component-versioning.md:The default cluster version used by [Windows Azure HDInsight](http://go.microsoft.com/fwlink/?LinkID=285601) is 2.1. It is based on the Hortonworks Data Platform version 1.3.0 and provides Hadoop services with the component versions itemized in the following table:
hdinsight-component-versioning.md:[Windows Azure HDInsight](http://go.microsoft.com/fwlink/?LinkID=285601) cluster version 1.6 is also available. It is based on the Hortonworks Data Platform version 1.1.0 and provides Hadoop services with the component versions itemized in the following table:
hdinsight-component-versioning.md:If you use the **Quick Create** option, you will get the version 2.1 of HDInsight Hadoop cluster by default. If you use the **Custom Create** option from the Windows Azure Portal, you can choose the version of the cluster you will deploy from the **HDInsight Version** drop-down on the **Cluster Details** page. Version 3.0 of HDInsight Hadoop cluster is only available as an option on the **Custom Create** wizard.
hdinsight-component-versioning.md:The following table lists the versions of HDInsight currently available, the corresponding Hortonworks Data Platform (HDP) versions that they use, and their release dates. When known, their deprecation dates will also be provided.
hdinsight-component-versioning.md:**Additional notes and information on versioning**	
hdinsight-component-versioning.md:* The SQL Server JDBC Driver is used internally by HDInsight and is not used for external operations. If you wish to connect to HDInsight using ODBC, please use the Microsoft Hive ODBC driver. For more information on using Hive ODBC, [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][connect-excel-with-hive-ODBC].
hdinsight-component-versioning.md:* The component versions associated with HDInsight cluster versions may change in future updates to HDInsight. One way to determine the available components and to verify which versions are being used for a cluster to use the Ambari REST API. The GetComponentInformation command can be used to retrieve information about a service component. For details, see the [Ambari documentation][ambari-docs]. Another way to obtain this information is to login to a cluster using remote desktop and examine the contents of the "C:\apps\dist\" directory directly.
hdinsight-connect-excel-hive-ODBC-driver.md:<properties linkid="manage-services-hdinsight-connect-excel-with-hive-ODBC" urlDisplayName="Connect Excel to HDInsight" pageTitle="Connect Excel to HDInsight with the Hive ODBC Driver | Windows Azure" metaKeywords="" description="Learn how to set up and use the Microsoft Hive ODBC driver for Excel to query data in an HDInsight cluster." metaCanonical="" services="hdinsight" documentationCenter="" title="Connect Excel to HDInsight with the Microsoft Hive ODBC Driver" authors=""  solutions="" writer="bradsev" manager="paulettm" editor="mollybos"  />
hdinsight-connect-excel-hive-ODBC-driver.md:One key feature of Microsoft's Big Data Solution is the integration of  Microsoft Business Intelligence (BI) components with Apache Hadoop clusters that have been deployed by the Windows Azure HDInsight Service. An example of this integration is the ability to connect Excel to the Hive data warehouse of an HDInsight Hadoop cluster using the Microsoft Hive Open Database Connectivity (ODBC) Driver. 
hdinsight-connect-excel-hive-ODBC-driver.md:It is also possible to connect the data associated with an HDInsight cluster and other data sources, including other (non-HDInsight) Hadoop clusters, from Excel using the Microsoft Power Query add-in for Excel. For information on installing and using Power Query, see [Connect Excel to Windows Azure HDInsight with Power Query][connect-excel-power-query].
hdinsight-connect-excel-hive-ODBC-driver.md:This topic walks you through how to set up and use the Microsoft Hive ODBC driver from Excel to query data in an HDInsight cluster. There are three parts to this procedure:
hdinsight-connect-excel-hive-ODBC-driver.md:* You have established a Windows Azure Account and have enabled the HDInsight Service for your subscription. You have created an HDInsight cluster using the HDInsight Service. For instructions on how to do this, see [Getting Started with Windows Azure HDInsight Service][getting-started] 
hdinsight-connect-excel-hive-ODBC-driver.md:2. There are two versions of this installation package (32-bit **HiveODBC32.msi** and 64-bit **HiveODBC64.msi**). You should install the version that matches the version of the application from which you will be using the ODBC driver. Both packages can be installed on the same machine if you need both versions of the driver. 
hdinsight-connect-excel-hive-ODBC-driver.md:4. Click **Next**, accept the terms of the License Agreement and then **Next**, and then accept or change the default destination folder. Click **Next** once more, then **Install**, and then **Yes** on the **User Account Control** to allow installation.
hdinsight-connect-excel-hive-ODBC-driver.md:2. In the Control Panel, click **System and Security**->**Administrative Tools**. Then click **Data Sources (ODBC)** if you are using Windows 7 or **ODBC Data Sources (32 bit)** or **ODBC Data Sources (64 bit)**, as appropriate, if you are using Windows 8. This will launch the **ODBC Data Source Administrator** dialog. 
hdinsight-connect-excel-hive-ODBC-driver.md:5. Scroll down and select the ** Hive ODBC Driver** driver in the ODBC driver list.  
hdinsight-connect-excel-hive-ODBC-driver.md:7. Enter a data source a name in the **Data Source** box. In this example, *tutorial-cluster-21*. Select **Hive Server 2** from the **Hive Server Type** dropdown menu and **Windows Azure HDInsight Service** from the **Mechanism** dropdown menu in the **Authentication** box. The port should set to *443* and this value should not be changed.
hdinsight-connect-excel-hive-ODBC-driver.md:9. Click **Test** to make sure the connection works, then **OK** to close the **Microsoft Hive ODBC Hive Driver Setup** dialog. The new data source should now be listed on the **ODBC Data Source Administrator**. Confirm this and click **OK** to exit the wizard.
hdinsight-connect-excel-hive-ODBC-driver.md:4. In **Data Connection Wizard**, select **ODBC DSN** as the data source and click **Next**.
hdinsight-connect-excel-hive-ODBC-driver.md:5. In the **Connect to ODBC Data Source** dialog, select the Data Source name that you created in the previous step (in our example, *tutorialcluster*) and click **Next**.
hdinsight-connect-excel-hive-ODBC-driver.md:6. Re-enter the password for the cluster in the wizard and click **Test** to confirm the connection works and then click **OK**.
hdinsight-connect-excel-hive-ODBC-driver.md:7. When the **Select Database and Table** dialog in the **Data Connection Wizard** opens, select the table that you want to import (here, for example, we want the default "hivesamepletable") and click **Next**.
hdinsight-connect-excel-hive-ODBC-driver.md:	![Select Database and Table](./media/hdinsight-connect-excel-hive-ODBC-driver/HDI.SimbaHiveODBC.Excel.SelectDbAndTable.PNG) 
hdinsight-connect-excel-hive-ODBC-driver.md:8. When the **Save Data Connection File and Finish** dialog in the **Data Connection Wizard** opens, click the **Finish** button.
hdinsight-connect-excel-hive-ODBC-driver.md:	![Select Database and Table](./media/hdinsight-connect-excel-hive-ODBC-driver/HDI.SimbaHiveODBC.Excel.SaveDataConnection.PNG) 
hdinsight-connect-excel-hive-ODBC-driver.md:	![Select Database and Table](./media/hdinsight-connect-excel-hive-ODBC-driver/HDI.SimbaHiveODBC.Excel.ImportDataWizard.PNG) 
hdinsight-connect-excel-hive-ODBC-driver.md:10. Click on the **Definition** tab and add "Limit 200" at the end of the query in the **Command text** text box. (You can also replace this query text with another query as needed.) Then click **OK** to return to the **Import Data** dialog. 
hdinsight-connect-excel-hive-ODBC-driver.md:11. Click **OK** to close the **Import Data** dialog.  In the **DNS Configuration** dialog that opens, re-enter the password and click **OK**.
hdinsight-connect-excel-hive-ODBC-driver.md:12. The data from the hive table will open in the Excel workbook, where is can be inspected and analyzed using BI tools.
hdinsight-connect-excel-hive-ODBC-driver.md:The Microsoft Hive ODBC Driver makes it easy to import data from your HDInsight Service cluster into Excel where Business Intelligence tools may be used to inspect and analyze the data.
hdinsight-connect-excel-hive-ODBC-driver.md:* For information on using Sqoop to copy data from an HDInsight Service to SQL Azure, see [Using HDInsight to process Blob Storage data and write the results to a SQL Database][blob-hdi-sql]. 
hdinsight-connect-excel-hive-ODBC-driver.md:[blob-hdi-sql]: /en-us/manage/services/hdinsight/process-blob-data-and-write-to-sql/
hdinsight-connect-excel-power-query.md:<properties linkid="manage-services-hdinsight-connect-excel-with-power-query" urlDisplayName="HDInsight and Excel" pageTitle="Connect Excel to HDInsight with Power Query | Windows Azure" metaKeywords="hdinsight, excel, data explorer, hive excel, hdinsight excel, power query" description="Learn how to take advantage of business intelligence components and use Excel to access data stored in Windows Azure HDInsight using Power Query." metaCanonical="" services="hdinsight" documentationCenter="" title="Connect Excel to Windows Azure HDInsight with Power Query" authors=""  solutions="" writer="bradsev" manager="paulettm" editor="mollybos"  />
hdinsight-connect-excel-power-query.md:One key feature of Microsoft's big data solution is the integration of  Microsoft Business Intelligence (BI) components with HDInsight Hadoop clusters. A primary example of this integration is the ability to connect Excel to the Windows Azure storage account containing the data associated with your HDInsight cluster by using Microsoft Power Query for Excel. This article walks you through how to set up and use Power Query from Excel to query data associated with an HDInsight cluster. 
hdinsight-connect-excel-power-query.md:- Office 2013 Professional Plus, Office 365 Pro Plus, Excel 2013 Standalone, or Office 2010 Professional Plus.
hdinsight-connect-excel-power-query.md:Power Query can be used to import data from a variety of sources into Microsoft Excel, where it can power Business Intelligence (BI) tools like PowerPivot and Power View. In particular, Power Query can import data that has been output or that has been generated by a Hadoop job running on an HDInsight cluster. 
hdinsight-connect-excel-power-query.md:- Download Microsoft Power Query for Excel from the [Microsoft Download Center][powerquery-download] and install it.
hdinsight-connect-excel-power-query.md:The Power Query add-in for Excel makes it easy to import data from your HDInsight cluster into Excel where business intelligence tools such as PowerPivot and Power Map may be used to inspect, analyze, and present the data.
hdinsight-connect-excel-power-query.md:3. Click the **Power Query** menu, click **From Other Sources**, and then click **From Windows Azure HDInsight**. 
hdinsight-connect-excel-power-query.md:	Note: If you don't see the **Power Query** menu, go to **File** > **Options** > **Add-Ins**, and select **COM Addins** from the drop-down **Manager** box at the bottom of the page. Select the **Go...** button and verify that the box for the Microsoft Office Power Query for Excel Add-In has been checked.
hdinsight-connect-excel-power-query.md:3. Enter the **Account Name** of the Windows Azure Blob storage account associated with your cluster, and then click **OK**. 
hdinsight-connect-excel-power-query.md:4. Enter the **Account Key** for the Blob storage account, and then click **Save**. (You need to do this only the first time you access this store.)	
hdinsight-connect-excel-power-query.md:6. Locate **HiveSampleData.txt** in the **Name** column, and then click **Binary** in the same row.
hdinsight-connect-excel-power-query.md:6. Right-click **Column1**, point to **Split Column**, and then click **By Delimiter**.
hdinsight-connect-excel-power-query.md:7. Select **Tab** in **Select or enter delimiter**, and **At each occurrence of the delimiter**, and then click **OK**.
hdinsight-connect-excel-power-query.md:* [Use HDInsight to process Blob storage data and write the results to a SQL Database][blob-hdi-sql]. 
hdinsight-connect-excel-power-query.md:[blob-hdi-sql]: /en-us/manage/services/hdinsight/process-blob-data-and-write-to-sql/
hdinsight-debug-jobs.md:<properties linkid="manage-services-hdinsight-debug-error-messages" urlDisplayName="Debug HDInsight Errors" pageTitle="Debug HDInsight: Error messages | Windows Azure" metaKeywords="dinsight, hdinsight service, hdinsight azure, debug, error messages, errors" description="Learn about the error messages you might receive when administering HDInsight using PowerShell, and steps you can take to recover."  title="Debug HDInsight: Error messages" umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" />
hdinsight-debug-jobs.md:The error messages itemized in this topic are provided to help the users of Windows Azure HDInsight understand possible error conditions that they can encounter when administering the service using Windows Azure PowerShell and to advise them on the steps which can be taken to recover from the error. 
hdinsight-debug-jobs.md:The errors a user can encounter in Windows Azure PowerShell or in the Windows Azure Portal are listed alphabetically by name in the [HDInsight Errors](#hdinsight-error-messages) section where they are linked to an entry in the [Discription and Mitigation of Errors](#discription-mitigation-errors) section that provide the following infomation for the error:
hdinsight-debug-jobs.md:<h2><a id="discription-mitigation-errors"></a>Diagnosis and Mitigation of Errors</h2> 
hdinsight-debug-jobs.md:- **Description**: Please provide Azure SQL database details for at least one component in order to use custom settings for Hive and Oozie metastores.   
hdinsight-debug-jobs.md:- **Mitigation**: The user needs to supply a valid SQL Azure metastore and retry the request.  
hdinsight-debug-jobs.md:- **Description**: Could not create cluster in region *nameOfYourRegion*. Use a valid HDInsight region and retry request.   
hdinsight-debug-jobs.md:- **Description**: Cluster DNS name *yourDnsName* is invalid. Please ensure name starts and ends with alphanumeric and can only contain '-' special character  
hdinsight-debug-jobs.md:- **Mitigation**: Make sure that you have used a valid DNS name for your cluster that starts and ends with alphanumeric and contains no special characters other than the dash '-' and then retry the operation.
hdinsight-debug-jobs.md:- **Mitigation**: The user should specify a clustername that is unique and does not exist and retry. If the user is using the portal, the UI will notify them if a cluster name is already being used during the create steps. 
hdinsight-debug-jobs.md:- **Description**: Cluster password is invalid. Password must be at least 10 characters long and must contain at least one number, uppercase letter, lowercase letter and special character with no spaces and should not contain the username as part of it.  
hdinsight-debug-jobs.md:- **Mitigation**: Provie a valid cluster password and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Provide a valid cluster username and retry the operation.
hdinsight-debug-jobs.md:- **Description**: Cluster DNS name *yourDnsClusterName* is invalid. Please ensure name starts and ends with alphanumeric and can only contain '-' special character  
hdinsight-debug-jobs.md:- **Mitigation**: Provide a valid DNS cluster username and retry the operation.
hdinsight-debug-jobs.md:- **Description**: Container name in URI *yourcontainerURI* and DNS name *yourDnsName* in request body must be the same.  
hdinsight-debug-jobs.md:- **Mitigation**: Make sure that your container Name and your DNS name are the same and retry the operation.
hdinsight-debug-jobs.md:- **Description**: Deletion of deployment failed for the Cluster  
hdinsight-debug-jobs.md:- **Mitigation**: Delete cluster and create a new cluster.
hdinsight-debug-jobs.md:- **Mitigation**: Provide a unique name for the container and retry the create operation. 
hdinsight-debug-jobs.md:- **Description**: The server could not update the state of the cluster deployment.  
hdinsight-debug-jobs.md:- **Description**: Hosted Service *nameOfYourHostedService* already has a production deployment. A hosted service cannot contain multiple production deployments. Retry the request with a different cluster name.   
hdinsight-debug-jobs.md:- **Mitigation**: Use a different cluster name and retry the request.
hdinsight-debug-jobs.md:- **Mitigation**: If the cluster is in error state, delete it and then try again. 
hdinsight-debug-jobs.md:- **Description**: Hosted Service *nameOfYourHostedService* has no associated deployment.  
hdinsight-debug-jobs.md:- **Mitigation**: If the cluster is in error state, delete it and then try again. 
hdinsight-debug-jobs.md:- **Mitigation**: Free up resources in your subscription or increase the resources available to the subscription and try to create the cluster again.
hdinsight-debug-jobs.md:- **Mitigation**: Free up resources in your subscription or increase the resources available to the subscription and try to create the cluster again.
hdinsight-debug-jobs.md:- **Description**: Azure Storage location *dataRegionName* is not a valid location. Make sure the region is correct and retry request.   
hdinsight-debug-jobs.md:- **Mitigation**: Select a Storage location that supports HDInsight, check that your cluster is co-located and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Specify the supported node size for the data node and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Specify the supported node size for the head node and retry the operation
hdinsight-debug-jobs.md:- **Mitigation**: If the cluster is in error state, drop it and then try again.  
hdinsight-debug-jobs.md:- **Description**: External storage account blob container name *yourContainerName* is invalid. Make sure name starts with a letter and contains only lowercase letters, numbers and dash.  
hdinsight-debug-jobs.md:- **Mitigation**: Specify a valid storage account blob container name and retry the operation.
hdinsight-debug-jobs.md:- **Mitigation**: Specify a valid secret key for the storage account and retry the operation.
hdinsight-debug-jobs.md:- **Mitigation**: Specify a valid format for the version-header and retry the request. 
hdinsight-debug-jobs.md:- **Description**: One or more of the cluster creation request inputs is not valid. Make sure the input values are correct and retry request.  
hdinsight-debug-jobs.md:- **Mitigation**: Make sure the input values are correct and retry request. 
hdinsight-debug-jobs.md:- **Description**: Region capability not available for region *yourRegionName* and Subscription ID *yourSubscriptionId*.  
hdinsight-debug-jobs.md:- **Mitigation**: Check that your subscription ID is valid and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Supply a valid blob URL. The URL MUST be fully valid, including starting with *http://* and ending in *.com*. The fully qualified URL can usually be found in the storage tab of the manage.windowsazure.com portal.  
hdinsight-debug-jobs.md:- **Mitigation**: Supply a valid blob URL. The URL MUST be fully valid, including starting with *http://* and ending in *.com*. The fully qualified URL can usually be found in the storage tab of the manage.windowsazure.com portal. 
hdinsight-debug-jobs.md:- **Description**: Version capability not available for version *specifiedVersion* and Subscription ID *yourSubscriptionId*.  
hdinsight-debug-jobs.md:- **Mitigation**: Choose a version that is available and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Choose a version that is supported and retry the operation.
hdinsight-debug-jobs.md:- **Mitigation**: Choose a version that is supported in the region specified and retry the operation. 
hdinsight-debug-jobs.md:- **Mitigation**: Verify that the account exists and is properly specified in configuration and retry the operation. 
hdinsight-develop-deploy-java-mapreduce.md:<properties linkid="manage-services-hdinsight-develop-Java-MapReduce-programs-for-HDInsight" urlDisplayName="HDInsight Tutorials" pageTitle="Develop Java MapReduce programs for HDInsight | Windows Azure" metaKeywords="hdinsight, hdinsight development, hadoop development, hdinsight deployment, development, deployment, tutorial, MapReduce, Java" description="Learn how to develop Java MapReduce programs on HDInsight emulator, how to deploy them to HDInsight."  title="Develop Java MapReduce programs for HDInsight" umbracoNaviHide="0" disqusComments="1" writer="jgao" editor="cgronlun" manager="paulettm" />
hdinsight-develop-deploy-java-mapreduce.md:This tutorial walks you through an end-to-end scenario from developing and testing a word counting MapReduce job on HDInsight Emulator, to deploying and running it on Windows Azure HDInsight.
hdinsight-develop-deploy-java-mapreduce.md:- Install Windows Azure PowerShell on the emulator computer. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-develop-deploy-java-mapreduce.md:- [Develop a word counting MapReduce program in Java](#develop)
hdinsight-develop-deploy-java-mapreduce.md:- [Upload data files and the application to Windows Azure Blob storage](#upload)
hdinsight-develop-deploy-java-mapreduce.md:##<a name="develop"></a>Develop a word counting MapReduce program in Java
hdinsight-develop-deploy-java-mapreduce.md:2. Copy and paste the following program into notepad.
hdinsight-develop-deploy-java-mapreduce.md:	Notice the package name is **org.apache.hadoop.examples** and the class name is **WordCount**. You will use the names when you submit the MapReduce job.
hdinsight-develop-deploy-java-mapreduce.md:1. Open command prompt.
hdinsight-develop-deploy-java-mapreduce.md:3. Run the following command to check the existence of the two jar files:
hdinsight-develop-deploy-java-mapreduce.md:4. Run the following command to compile the program:
hdinsight-develop-deploy-java-mapreduce.md:5. Run the following command to create a jar file:
hdinsight-develop-deploy-java-mapreduce.md:	The command creates a WordCount.jar file in the current folder.
hdinsight-develop-deploy-java-mapreduce.md:In this tutorial, you will use the HDFS *copyFromLocal* command to upload the data files to HDFS. The next section shows you how to upload files using Windows Azure PowerShell to Windows Azure Blob storage. For other methods for uploading files to Windows Azure Blob storage, see [Upload data to HDInsight][hdinsight-upload-data].
hdinsight-develop-deploy-java-mapreduce.md:<tr><td>/WordCount/Apps</td><td>The folder for the mapper and reducer executables.</td></tr>
hdinsight-develop-deploy-java-mapreduce.md:<p>The Hadoop HDFS commands are case sensitive.</p> 
hdinsight-develop-deploy-java-mapreduce.md:0. Open Hadoop command line from your desktop. Hadoop command line is installed by the emulator installer.
hdinsight-develop-deploy-java-mapreduce.md:1. From the Hadoop command line window, run the following command to make a directory for the input files:
hdinsight-develop-deploy-java-mapreduce.md:2. Run the following command to copy some text files to the input folder on HDFS:
hdinsight-develop-deploy-java-mapreduce.md:3. Run the following command to list and verify the uploaded files:
hdinsight-develop-deploy-java-mapreduce.md:**To run the MapReduce job using Hadoop command line**
hdinsight-develop-deploy-java-mapreduce.md:1. Open Hadoop command line from your desktop.
hdinsight-develop-deploy-java-mapreduce.md:2. Run the following command to delete the /WordCount/Output folder structure from HDFS.  /WordCount/Output is the output folder of the word counting MapReduce job. The MapReduce job will fail if the folder already exists. This step is necessary if this is the second time you run the job.
hdinsight-develop-deploy-java-mapreduce.md:2. Run the following command:
hdinsight-develop-deploy-java-mapreduce.md:	From the screenshot, you can see both map and reduce completed 100%. It also lists the job ID, job_201312092021_0002. The same report can be retrieved by opening the **Hadoop MapReduce status** shortcut from your desktop, and looking for the job ID.
hdinsight-develop-deploy-java-mapreduce.md:1. Open Hadoop command line.
hdinsight-develop-deploy-java-mapreduce.md:2. Run the following commands to display the output:
hdinsight-develop-deploy-java-mapreduce.md:	You can append "|more" at the end of the command to get the page view. Or use the findstr command to find a string pattern:
hdinsight-develop-deploy-java-mapreduce.md:Until now, you have developed a word counting MapReduce job, and tested it successfully on the emulator.  The next step is to deploy and run it on Windows Azure HDInsight.
hdinsight-develop-deploy-java-mapreduce.md:In this tutorial, you will create a container on a separate storage account for the data files and the MapReduce application. The data files are the text files in the %hadoop_home% directory on your workstation.
hdinsight-develop-deploy-java-mapreduce.md:**To create a Blob storage and a container**
hdinsight-develop-deploy-java-mapreduce.md:2. Set the variables, and then run the commands:
hdinsight-develop-deploy-java-mapreduce.md:	The **$subscripionName** is associated with your Windows Azure subscription. You must name the **$storageAccountName_Data** and **$containerName_Data**. For the naming restrictions, see [Naming and Referencing Containers, Blobs, and Metadata](http://msdn.microsoft.com/en-us/library/windowsazure/dd135715.aspx). 
hdinsight-develop-deploy-java-mapreduce.md:3. Run the following command to create a storage account and a Blob storage container on the account
hdinsight-develop-deploy-java-mapreduce.md:4. Run the following commands to verify the storage account and the container:
hdinsight-develop-deploy-java-mapreduce.md:2. Set the first three variables, and then run the commands:
hdinsight-develop-deploy-java-mapreduce.md:	The **$storageAccountName_Data** and **$containerName_Data** are the same as you defined in the last procedure.
hdinsight-develop-deploy-java-mapreduce.md:	Notice the source file folder is **c:\Hadoop\hadoop-1.1.0-SNAPSHOT**, and the destination folder is **WordCount/Input**.
hdinsight-develop-deploy-java-mapreduce.md:3. Run the following commands to get a list of the txt files in the source file folder:
hdinsight-develop-deploy-java-mapreduce.md:4. Run the following commands to create a storage context object:
hdinsight-develop-deploy-java-mapreduce.md:5. Run the following commands to copy the files:
hdinsight-develop-deploy-java-mapreduce.md:6. Run the following command to list the uploaded files:
hdinsight-develop-deploy-java-mapreduce.md:2. Set the first three variables, and then run the commands:
hdinsight-develop-deploy-java-mapreduce.md:	The **$storageAccountName_Data** and **$containerName_Data** are the same as you defined in the last procedure, which means you will upload both the data file and the application to the same container on the same storage account.
hdinsight-develop-deploy-java-mapreduce.md:4. Run the following commands to create a storage context object:
hdinsight-develop-deploy-java-mapreduce.md:5. Run the following commands to copy the applications:
hdinsight-develop-deploy-java-mapreduce.md:6. Run the following command to list the uploaded files:
hdinsight-develop-deploy-java-mapreduce.md:	4. display standard error
hdinsight-develop-deploy-java-mapreduce.md:	5. display standard output
hdinsight-develop-deploy-java-mapreduce.md:2. Copy and paste the following code:
hdinsight-develop-deploy-java-mapreduce.md:		# The storage account and the HDInsight cluster variables
hdinsight-develop-deploy-java-mapreduce.md:		# Create a PSCredential object. The username and password are hardcoded here.  You can change them if you want.
hdinsight-develop-deploy-java-mapreduce.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardError 
hdinsight-develop-deploy-java-mapreduce.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $mrJob.JobId -StandardOutput
hdinsight-develop-deploy-java-mapreduce.md:3. Set the first six variables in the script. **$serviceNameToken** will be used for the HDInsight cluster name, the default storage account name, and the default Blob storage container name.  Because the service name must be between 3 and 24 characters, and the script append string with up to 10 character string to the names, you  must limit the string to 14 or less characters. And the $serviceNameToken must use lower case. **$storageAccountName_Data** and **$containerName_Data** are the storage account and container that are used for storing the data files and the application. **$location** must match the data storage account location.
hdinsight-develop-deploy-java-mapreduce.md:7. Run the following command to set the execution policy to remotesigned:
hdinsight-develop-deploy-java-mapreduce.md:8. When prompted, enter username and password for the HDInsight cluster. Because you will delete the cluster at the end of the script and you will not need the username and password anymore, the username and password can be any strings. If you don't want to get prompted for the credentials, see [Working with Passwords, Secure Strings and Credentials in Windows PowerShell][powershell-PSCredential]
hdinsight-develop-deploy-java-mapreduce.md:This section shows you how to download and display the output.  For the information on displaying the results on Excel, see [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-excel], and [Connect Excel to Windows Azure HDInsight with Power Query][hdinsight-powerquery].
hdinsight-develop-deploy-java-mapreduce.md:2. Run the following commands to set the values:
hdinsight-develop-deploy-java-mapreduce.md:3. Run the following commands to create an Azure storage contect object: 
hdinsight-develop-deploy-java-mapreduce.md:4. Run the following commands to download and display the output:
hdinsight-develop-deploy-java-mapreduce.md:In this tutorial, you have learned how to develop a Java MapReduce job, how to test the application on HDInsight emulator, and how to write a PowerShell script to provision an HDInsight cluster and run a MapReduce on the cluster. To learn more, see the following articles:
hdinsight-develop-deploy-java-mapreduce.md:- [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-streaming]
hdinsight-develop-deploy-java-mapreduce.md:[hdinsight-develop-streaming]: /en-us/documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs/
hdinsight-develop-deploy-java-mapreduce.md:[powershell-PSCredential]: http://social.technet.microsoft.com/wiki/contents/articles/4546.working-with-passwords-secure-strings-and-credentials-in-windows-powershell.aspx
hdinsight-develop-deploy-java-mapreduce.md:[image-emulator-wordcount-compile]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Compile-Java-MapReduce.png
hdinsight-develop-deploy-java-mapreduce.md:[image-emulator-wordcount-run]: ./media/hdinsight-develop-deploy-java-mapreduce/HDI-Emulator-Run-Java-MapReduce.png
hdinsight-get-started-30.md:<properties linkid="manage-services-hdinsight-get-started-hdinsight" urlDisplayName="Get Started" pageTitle="Get started using Hadoop 2.2 clusters with HDInsight (preview) | Windows Azure" metaKeywords="" description="Get started using Hadoop 2.2 clusters with HDInsight (preview), a big data solution. Learn how to provision clusters, run MapReduce jobs, and output data to Excel for analysis." metaCanonical="" services="hdinsight" documentationCenter="" title="Get started using Windows Azure HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-get-started-30.md:HDInsight makes [Apache Hadoop][apache-hadoop] available as a service in the cloud. It makes the MapReduce software framework available in a simpler, more scalable, and cost efficient Windows Azure environment. HDInsight also provides a cost efficient approach to the managing and storing of data using Windows Azure Blob storage. 
hdinsight-get-started-30.md:In this tutorial, you will provision an HDInsight cluster using the Windows Azure Management Portal, submit a Hadoop MapReduce job using PowerShell, and then import the MapReduce job output data into Excel for examination.
hdinsight-get-started-30.md:In conjunction with the general availability of Windows Azure HDInsight, Microsoft has also released HDInsight Emulator for Windows Azure, formerly known as Microsoft HDInsight Developer Preview. This product targets developer scenarios and as such only supports single-node deployments. For using HDInsight Emulator, see [Get Started with the HDInsight Emulator][hdinsight-emulator].
hdinsight-get-started-30.md:- Office 2013 Professional Plus, Office 365 Pro Plus, Excel 2013 Standalone, or Office 2010 Professional Plus.
hdinsight-get-started-30.md:There are several ways to submit MapReduce jobs to HDInsight. In this tutorial, you will use Windows Azure PowerShell. To install Windows Azure PowerShell, run the [Microsoft Web Platform Installer][powershell-download]. Click **Run** when prompted, click **Install**, and then follow the instructions. For more information, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started-30.md:2. Run the following command:
hdinsight-get-started-30.md:3. In the window, type the email address and password associated with your account. Windows Azure authenticates and saves the credential information, and then closes the window.
hdinsight-get-started-30.md:The other method to connect to  your subscription is using the certificate method. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started-30.md:2. Click **NEW** on the lower left corner, point to **DATA SERVICES**, point to **STORAGE**, and then click **QUICK CREATE**.
hdinsight-get-started-30.md:3. Enter **URL**, **LOCATION** and **REPLICATION**, and then click **CREATE STORAGE ACCOUNT**. Affinity groups are not supported. You will see the new storage account in the storage list. 
hdinsight-get-started-30.md:7. Make a note of the **STORAGE ACCOUNT NAME** and the **PRIMARY ACCESS KEY**.  You will need them later in the tutorial.
hdinsight-get-started-30.md:[How to Create a Storage Account][azure-create-storageaccount] and [Use Windows Azure Blob Storage with HDInsight][hdinsight-storage].
hdinsight-get-started-30.md:3. Click **NEW** on the lower left side, click **DATA SERVICES**, click **HDINSIGHT**, and then click **CUSTOM CREATE**.
hdinsight-get-started-30.md:	<tr><td><strong>Data Nodes</strong></td><td>Number of data nodes you want to deploy. For testing purposes, create a single node cluster. <br />The cluster size limit varies for Windows Azure subscriptions. Contact Azure billing support to increase the limit.</td></tr>
hdinsight-get-started-30.md:4. From the Configure Cluster user tab, enter **User Name** and **Password** for the HDInsight cluster user account. In addition to this account, you can create a RDP user account after the cluster is provisioned, so you can remote desktop into the cluster. For instructions, see [Administer HDInsight using Management portal][hdinsight-admin-portal]
hdinsight-get-started-30.md:* A MapReduce program. In this tutorial, you will use the WordCount sample that comes with the HDInsight cluster distribution so you don't need to write your own. It is located on */example/jars/hadoop-examples.jar*. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-get-started-30.md:The URI scheme provides both unencrypted access with the *WASB:* prefix, and SSL encrypted access with WASBS. We recommend using WASBS wherever possible, even when accessing data that lives inside the same Windows Azure data center.
hdinsight-get-started-30.md:Because HDInsight uses a Blob Storage container as the default file system, you can refer to files and directories inside the default file system using relative or absolute paths.
hdinsight-get-started-30.md:The use of the *wasb://* prefix in the paths of these files. This is needed to indicate Azure Blob Storage is being used for input and output files. The output directory assumes a default path relative to the *wasb:///user/&lt;username&gt;* folder. 
hdinsight-get-started-30.md:1. Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started-30.md:3. Run the following commands to set the variables.  
hdinsight-get-started-30.md:5. Run the following commands to create a MapReduce job definition:
hdinsight-get-started-30.md:	The hadoop-examples.jar file comes with the HDInsight cluster distribution. There are two arguments for the MapReduce job. The first one is the source file name, and the second is the output file path. The source file comes with the HDInsight cluster distribution, and the output file path will be created at the run-time.
hdinsight-get-started-30.md:6. Run the following command to submit the MapReduce job:
hdinsight-get-started-30.md:6. Run the following command to check the completion of the MapReduce job:
hdinsight-get-started-30.md:8. Run the following command to check any errors with running the MapReduce job:	
hdinsight-get-started-30.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $wordCountJob.JobId -StandardError
hdinsight-get-started-30.md:2. Run the following command to change directory to c:\ root.
hdinsight-get-started-30.md:2. Set the three variables in the following commands, and then run them:
hdinsight-get-started-30.md:3. Run the following commands to create a Windows Azure storage context object:
hdinsight-get-started-30.md:	The *Select-AzureSubscription* is used to set the current subscription in case you have multiple subscriptions, and the default subscription is not the one to use. 
hdinsight-get-started-30.md:4. Run the following command to download the MapReduce job output from the Blob container to the workstation:
hdinsight-get-started-30.md:5. Run the following command to print the MapReduce job output file:
hdinsight-get-started-30.md:	The MapReduce job produces a file named *part-r-00000* with the words and the counts.  The script uses the findstr command to list all of the words that contains *"there"*.
hdinsight-get-started-30.md:The Power Query add-in for Excel can be used to export output from HDInsight into Excel where Microsoft Business Intelligence (BI) tools can be used to further process or display the results. When you created an HDInsight cluster, a default container with the same name as the cluster was created in the storage account associated with it when it was created. This is automatically populated with a set of files. One of these files is a sample Hive table. In this section we will show how to import the data contained in this table into Excel for viewing and additional processing.
hdinsight-get-started-30.md:- Download Microsoft Power Query for Excel from the [Microsoft Download Center](http://www.microsoft.com/en-us/download/details.aspx?id=39379) and install it.
hdinsight-get-started-30.md:1. Open Excel, and create a new blank workbook.
hdinsight-get-started-30.md:3. Click the **Power Query** menu, click **From Other Sources**, and then click **From Windows Azure HDInsight**.
hdinsight-get-started-30.md:3. Enter the **Account Name** of the Azure Blob Storage Account associated with your cluster, and then click **OK**. This is the storage account you created earlier in the tutorial.
hdinsight-get-started-30.md:4. Enter the **Account Key** for the Azure Blob Storage Account, and then click **Save**. 
hdinsight-get-started-30.md:6. Locate **part-r-00000** in the **Name** column, and then click **Binary**.
hdinsight-get-started-30.md:6. Right-click **Column1**, point to **Split Column**, and then click **By Delimiter**.
hdinsight-get-started-30.md:7. Select **Tab** in **Select or enter delimiter**, and **At the right-most delimiter**, and then click **OK**.
hdinsight-get-started-30.md:8. Right-click **Column1.1**, and then select **Rename**.
hdinsight-get-started-30.md:In this tutorial, you have learned how to provision a cluster with HDInsight, run a MapReduce job on it, and import the results into Excel where they can be further processed and graphically displayed using BI tools. To learn more, see the following articles:
hdinsight-get-started-30.md:- [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-deploy-streaming]
hdinsight-get-started-30.md:- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]
hdinsight-get-started-30.md:[hdinsight-develop-deploy-streaming]: /en-us/documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs/
hdinsight-get-started-30.md:[hdinsight-develop-mapreduce]: /en-us/documentation/articles/hdinsight-develop-deploy-java-mapreduce/
hdinsight-get-started-emulator.md:This tutorial gets you started using the Microsoft HDInsight Emulator for Windows Azure (formerly HDInsight Server Developer Preview). The HDInsight Emulator comes with the same components from the Hadoop ecosystem as Windows Azure HDInsight. For details, including information on the versions deployed, see [What version of Hadoop is in Windows Azure HDInsight?](http://www.windowsazure.com/en-us/manage/services/hdinsight/howto-hadoop-version/ "HDInsight components and versions"). 
hdinsight-get-started-emulator.md:HDInsight Emulator provides a local development environment for the Windows Azure HDInsight. If you are familiar with Hadoop, you can get started with the Emulator using HDFS. But, in HDInsight, the default file system is Windows Azure Blob storage (WASB, aka Windows Azure Storage - Blobs), so eventually, you will want to develop your jobs using WASB. You can get started developing against WASB by using the Windows Azure Storage Emulator - probably only want to use a small subset of your data (no config changes required in the HDInsight Emulator, just a different storage account name). Then, you test your jobs locally against Windows  Azure Storage - again, only using a subset of your data (requires a config change in the HDInsight Emulator). Finally, you are ready to move the compute portion of your job to HDInsight and run a job against production data.
hdinsight-get-started-emulator.md:<p>The HDInsight Emulator can use only a single node deployment. </p>
hdinsight-get-started-emulator.md:- Install and configure Windows Azure PowerShell. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure]. 
hdinsight-get-started-emulator.md:<p>If you have had Microsoft HDInsight Developer Preview installed, you must uninstall the following two components from Control Panel/Program and Features first.</p>
hdinsight-get-started-emulator.md:1. Open Internet Explorer, and then browse to the [Microsoft HDInsight Emulator for Windows Azure installation page][hdinsight-emulator-install].
hdinsight-get-started-emulator.md:8. Verify the Web Platform Installer shows **the following products were successfully installed**, and then click **Finish**.
hdinsight-get-started-emulator.md:	- **Hadoop Command Line**: The Hadoop command prompt from which MapReduce, Pig and Hive jobs are run in the HDInsight Emulator.
hdinsight-get-started-emulator.md:	For known issues with installing and running HDInsight Server, see the [HDInsight Emulator Release Notes][hdinsight-emulator-release-notes]. The installation log is located at **C:\HadoopFeaturePackSetup\HadoopFeaturePackSetupTools\gettingStarted.winpkg.install.log**.
hdinsight-get-started-emulator.md:Now you have the HDInsight emulator configured on your workstation. You can run a MapReduce job to test the installation. You will first upload some text files to HDFS, and then run a word count MapReduce job to count the word frequencies of those files. 
hdinsight-get-started-emulator.md:1. From the desktop, double-click **Hadoop Command Line** to open the Hadoop command line window.  The current folder should be:
hdinsight-get-started-emulator.md:	If not, use the *cd* command to change directory to the folder.
hdinsight-get-started-emulator.md:2. Run the following Hadoop command to make a HDFS folder for storing the input and output files:
hdinsight-get-started-emulator.md:3. Run the following Hadoop command to copy some local files to HDFS:
hdinsight-get-started-emulator.md:4. Run the following command to list the files in the /user/HDIUser folder:
hdinsight-get-started-emulator.md:5. Run the following command to run the word count MapReduce job:
hdinsight-get-started-emulator.md:6. Run the following command to list the words with "windows" in them from the output file:
hdinsight-get-started-emulator.md:The HDInsight Emulator installation provides some samples to get new users started learning Apache Hadoop-based Services on Windows quickly. These samples covers some tasks that are typically needed when processing a big data set. Going through the samples can familiarize yourself with concepts associated with the MapReduce programming model and its ecosystem.
hdinsight-get-started-emulator.md:The samples are organized around the processing IIS W3C log data scenarios. A data generation tool is provided to create and import the data sets in various sizes to HDFS or WASB (Windows Azure Blob storage). See [Use Windows Azure Blob storage for HDInsight][hdinsight-blob-store] for more information). MapReduce, Pig or Hive jobs may then be run on the pages of data generated by the PowerShell script. Note that the Pig and Hive scripts used both compile to MapReduce programs. Users may run a series of jobs to observe, for themselves, the effects of using these different technologies and the effects of the size of the data on the execution of the processing tasks. 
hdinsight-get-started-emulator.md:The w3c scenario generates and imports IIS W3C log data in three sizes into HDFS or WASB: 1MB, 500MB, and 2GB. It provides three job types and implements each of them in C#, Java, Pig and Hive.
hdinsight-get-started-emulator.md:These samples and their documentation do not provide an in-depth study or full implementation of the key Hadoop technologies. The cluster used has only a single node and so the effect of adding more nodes cannot, with this release, be observed. 
hdinsight-get-started-emulator.md:Generating and importing the data to HDFS is done using the PowerShell script importdata.ps1.
hdinsight-get-started-emulator.md:1. Open Hadoop command line from desktop.
hdinsight-get-started-emulator.md:2. Run the following command to change directory to **C:\Hadoop\GettingStarted**:
hdinsight-get-started-emulator.md:3. Run the following command to generate and import data to HDFS:
hdinsight-get-started-emulator.md:4. Run the following command from Hadoop command line to list the imported files on the HDFS:
hdinsight-get-started-emulator.md:5. Run the following command to display one of the data files to the console window:
hdinsight-get-started-emulator.md:Now you have the data file created and imported to HDFS.  You can run different Hadoop jobs.
hdinsight-get-started-emulator.md:MapReduce is the basic compute engine for Hadoop. By default, it is implemented in Java, but there are also examples that leverage .NET and Hadoop Streaming that use C#. The syntax for running a MapReduce job is:
hdinsight-get-started-emulator.md:The jar file and the source files are located in the C:\Hadoop\GettingStarted\Java folder.
hdinsight-get-started-emulator.md:1. Open the Hadoop command line.
hdinsight-get-started-emulator.md:2. Run the following command to change directory to **C:\Hadoop\GettingStarted**:
hdinsight-get-started-emulator.md:3. Run the following command to remove the output directory in case the folder exists.  The MapReduce job will fail if the output folder already exists.
hdinsight-get-started-emulator.md:3. Run the following command:
hdinsight-get-started-emulator.md:	The following table describes the elements of the command:
hdinsight-get-started-emulator.md:4. Run the following command to display the output file:
hdinsight-get-started-emulator.md:	So, the Default.aspx page gets 3409 hits and so on. 
hdinsight-get-started-emulator.md:The Hive query engine will feel familiar to analysts with strong SQL skills. It provides a SQL-like interface and a relational data model for HDFS. Hive uses a language called HiveQL (or HQL), which is a dialect of SQL.
hdinsight-get-started-emulator.md:1. Open Hadoop command line.
hdinsight-get-started-emulator.md:3. Run the following command to remove the **/w3c/hive/input** folder in case the folder exists.  The hive job will fail if the folder exists.
hdinsight-get-started-emulator.md:4. Run the following command to create the **/w3c/hive/input** folder and copy the data file from the workstation to HDFS:
hdinsight-get-started-emulator.md:5. Run the following command to execute the **w3ccreate.hql** script file.  The script creates a Hive table, and loads data to the Hive table:
hdinsight-get-started-emulator.md:6. Run the following command to run the **w3ctotalhitsbypate.hql** HiveQL script file.  
hdinsight-get-started-emulator.md:	The following table describes the elements of the command:
hdinsight-get-started-emulator.md:	<tr><td>C:\Hadoop\hive-0.9.0\bin\hive.cmd</td><td>The Hive command script.</td></tr>
hdinsight-get-started-emulator.md:Note that as a first step in each of the jobs, a table will be created and data will be loaded into the table from the file created earlier. You can browse the file that was created by looking under the /Hive node in HDFS using the following command:
hdinsight-get-started-emulator.md:Pig processing uses a data flow language, called *Pig Latin*. Pig Latin abstractions provide richer data structures than MapReduce, and perform for Hadoop what SQL performs for RDBMS systems. 
hdinsight-get-started-emulator.md:1. Open Hadoop command line.
hdinsight-get-started-emulator.md:3. Run the following command to submit a Pig job:
hdinsight-get-started-emulator.md:	The following table shows the elements of the command:
hdinsight-get-started-emulator.md:	<tr><td>C:\Hadoop\pig-0.9.3-SNAPSHOT\bin\pig.cmd</td><td>The Pig command script.</td></tr>
hdinsight-get-started-emulator.md:Note that since Pig scripts compile to MapReduce jobs, and potentially to more than one such job, users may see multiple MapReduce jobs executing in the course of processing a Pig job.
hdinsight-get-started-emulator.md:1. Open Hadoop command line.
hdinsight-get-started-emulator.md:2. Run the following command:
hdinsight-get-started-emulator.md:The Windows Azure Storage emulator comes with [Windows Azure SDK for .NET][azure-sdk]. The storage emulator don't start automatically. You must manually start it.  The application name is *Windows Azure Storage Emulator*. To start/stop the emulators, right-click the blue Windows Azure icon in the Windows System Tray, and then click Show Storage Emulator UI.
hdinsight-get-started-emulator.md:<li>Stop the two Hadoop Hive services using services.msc: Apache Hadoop hiveserver and Apache Hadoop Hiveserver2.</li>
hdinsight-get-started-emulator.md:<p>It is because you are still using the Developer Preview version. Please follow the instructions found in the Install the HDInsight Emulator section of this article to uninstall the developer preview version, and then reinstall the application.</p> 
hdinsight-get-started-emulator.md:6. Enter **NAME** and select **ACCESS**. You can use any of the three access level.  The default is **Private**.
hdinsight-get-started-emulator.md:Before you can access a Windows Azure Storage account, you must add the account name and the account key to the configuration file.
hdinsight-get-started-emulator.md:	You must substitute &lt;StorageAccountName> and &lt;StorageAccountKey> with the values that match your storage account information.
hdinsight-get-started-emulator.md:	You must substitute &lt;StorageAccountName> and &lt;StorageAccountKey> with the values that match your storage account information.
hdinsight-get-started-emulator.md:5. Open the Hadoop command line on your desktop in elevated mode (Run as administrator)
hdinsight-get-started-emulator.md:6. Run the following commands to restart the Hadoop services:
hdinsight-get-started-emulator.md:7. Run the following command to test the connection to the default file system:
hdinsight-get-started-emulator.md:	The following commands list the contents in the same folder:
hdinsight-get-started-emulator.md:	To access HDFS, use the following command:
hdinsight-get-started-emulator.md:In this tutorial, you have an HDInsight Emulator installed, and have ran some Hadoop jobs. To learn more, see the following articles:
hdinsight-get-started-emulator.md:- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]
hdinsight-get-started-emulator.md:- [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-deploy-streaming]
hdinsight-get-started-emulator.md:[hdinsight-develop-mapreduce]: /en-us/documentation/articles/hdinsight-develop-deploy-java-mapreduce/
hdinsight-get-started-emulator.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/ 
hdinsight-get-started-emulator.md:[hdinsight-develop-deploy-streaming]: /en-us/manage/services/hdinsight/develop-deploy-hadoop-streaming-jobs/
hdinsight-get-started.md:<properties linkid="manage-services-hdinsight-get-started-hdinsight" urlDisplayName="Get Started" pageTitle="Get started using HDInsight | Windows Azure" metaKeywords="" description="Get started with HDInsight, a big data solution. Learn how to provision clusters, run MapReduce jobs, and output data to Excel for analysis." metaCanonical="" services="hdinsight" documentationCenter="" title="Get started using Windows Azure HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-get-started.md:HDInsight makes [Apache Hadoop][apache-hadoop] available as a service in the cloud. It makes the MapReduce software framework available in a simpler, more scalable, and cost efficient Windows Azure environment. HDInsight also provides a cost efficient approach to the managing and storing of data using Windows Azure Blob storage. 
hdinsight-get-started.md:In this tutorial, you will provision an HDInsight cluster using the Windows Azure Management Portal, submit a Hadoop MapReduce job using PowerShell, and then import the MapReduce job output data into Excel for examination.
hdinsight-get-started.md:In conjunction with the general availability of Windows Azure HDInsight, Microsoft has also released HDInsight Emulator for Windows Azure, formerly known as Microsoft HDInsight Developer Preview. This product targets developer scenarios and as such only supports single-node deployments. For using HDInsight Emulator, see [Get Started with the HDInsight Emulator][hdinsight-emulator].
hdinsight-get-started.md:- Office 2013 Professional Plus, Office 365 Pro Plus, Excel 2013 Standalone, or Office 2010 Professional Plus.
hdinsight-get-started.md:There are several ways to submit MapReduce jobs to HDInsight. In this tutorial, you will use Windows Azure PowerShell. To install Windows Azure PowerShell, run the [Microsoft Web Platform Installer][powershell-download]. Click **Run** when prompted, click **Install**, and then follow the instructions. For more information, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started.md:2. Run the following command:
hdinsight-get-started.md:3. In the window, type the email address and password associated with your account. Windows Azure authenticates and saves the credential information, and then closes the window.
hdinsight-get-started.md:The other method to connect to  your subscription is using the certificate method. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started.md:2. Click **NEW** on the lower left corner, point to **DATA SERVICES**, point to **STORAGE**, and then click **QUICK CREATE**.
hdinsight-get-started.md:3. Enter **URL**, **LOCATION** and **REPLICATION**, and then click **CREATE STORAGE ACCOUNT**. Affinity groups are not supported. You will see the new storage account in the storage list. 
hdinsight-get-started.md:7. Make a note of the **STORAGE ACCOUNT NAME** and the **PRIMARY ACCESS KEY**.  You will need them later in the tutorial.
hdinsight-get-started.md:[How to Create a Storage Account][azure-create-storageaccount] and [Use Windows Azure Blob Storage with HDInsight][hdinsight-storage].
hdinsight-get-started.md:3. Click **NEW** on the lower left side, click **Data Services**, click **HDInsight**, and then click **Quick Create**.
hdinsight-get-started.md:	<tr><td>Cluster Size</td><td>Number of data nodes you want to deploy. The default value is 4. But 8, 16 and 32 data node clusters are also available on the dropdown menu. Any number of data nodes may be specified when using the <strong>Custom Create</strong> option. Pricing details on the billing rates for various cluster sizes are available. Click the <strong>?</strong> symbol just above the dropdown box and follow the link on the pop up.</td></tr>
hdinsight-get-started.md:	<tr><td>Password (cluster admin)</td><td>The password for the account <i>admin</i>. The cluster user name is specified to be "admin" by default when using the Quick Create option. This can only be changed by using the <strong>Custom Create</strong> wizard. The password field must be at least 10 characters and must contain an uppercase letter, a lowercase letter, a number, and a special character.</td></tr>
hdinsight-get-started.md:* A MapReduce program. In this tutorial, you will use the WordCount sample that comes with the HDInsight cluster distribution so you don't need to write your own. It is located on */example/jars/hadoop-examples.jar*. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-get-started.md:The URI scheme provides both unencrypted access with the *WASB:* prefix, and SSL encrypted access with WASBS. We recommend using WASBS wherever possible, even when accessing data that lives inside the same Windows Azure data center.
hdinsight-get-started.md:Because HDInsight uses a Blob Storage container as the default file system, you can refer to files and directories inside the default file system using relative or absolute paths.
hdinsight-get-started.md:The use of the *wasb://* prefix in the paths of these files. This is needed to indicate Azure Blob Storage is being used for input and output files. The output directory assumes a default path relative to the *wasb:///user/&lt;username&gt;* folder. 
hdinsight-get-started.md:1. Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-get-started.md:3. Run the following commands to set the variables.  
hdinsight-get-started.md:5. Run the following commands to create a MapReduce job definition:
hdinsight-get-started.md:	The hadoop-examples.jar file comes with the HDInsight cluster distribution. There are two arguments for the MapReduce job. The first one is the source file name, and the second is the output file path. The source file comes with the HDInsight cluster distribution, and the output file path will be created at the run-time.
hdinsight-get-started.md:6. Run the following command to submit the MapReduce job:
hdinsight-get-started.md:6. Run the following command to check the completion of the MapReduce job:
hdinsight-get-started.md:8. Run the following command to check any errors with running the MapReduce job:	
hdinsight-get-started.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $wordCountJob.JobId -StandardError
hdinsight-get-started.md:2. Run the following command to change directory to c:\ root.
hdinsight-get-started.md:2. Set the three variables in the following commands, and then run them:
hdinsight-get-started.md:3. Run the following commands to create a Windows Azure storage context object:
hdinsight-get-started.md:	The *Select-AzureSubscription* is used to set the current subscription in case you have multiple subscriptions, and the default subscription is not the one to use. 
hdinsight-get-started.md:4. Run the following command to download the MapReduce job output from the Blob container to the workstation:
hdinsight-get-started.md:5. Run the following command to print the MapReduce job output file:
hdinsight-get-started.md:	The MapReduce job produces a file named *part-r-00000* with the words and the counts.  The script uses the findstr command to list all of the words that contains *"there"*.
hdinsight-get-started.md:The Power Query add-in for Excel can be used to export output from HDInsight into Excel where Microsoft Business Intelligence (BI) tools can be used to further process or display the results. When you created an HDInsight cluster, a default container with the same name as the cluster was created in the storage account associated with it when it was created. This is automatically populated with a set of files. One of these files is a sample Hive table. In this section we will show how to import the data contained in this table into Excel for viewing and additional processing.
hdinsight-get-started.md:- Download Microsoft Power Query for Excel from the [Microsoft Download Center](http://www.microsoft.com/en-us/download/details.aspx?id=39379) and install it.
hdinsight-get-started.md:1. Open Excel, and create a new blank workbook.
hdinsight-get-started.md:3. Click the **Power Query** menu, click **From Other Sources**, and then click **From Windows Azure HDInsight**.
hdinsight-get-started.md:3. Enter the **Account Name** of the Azure Blob Storage Account associated with your cluster, and then click **OK**. This is the storage account you created earlier in the tutorial.
hdinsight-get-started.md:4. Enter the **Account Key** for the Azure Blob Storage Account, and then click **Save**. 
hdinsight-get-started.md:6. Locate **part-r-00000** in the **Name** column, and then click **Binary**.
hdinsight-get-started.md:6. Right-click **Column1**, point to **Split Column**, and then click **By Delimiter**.
hdinsight-get-started.md:7. Select **Tab** in **Select or enter delimiter**, and **At the right-most delimiter**, and then click **OK**.
hdinsight-get-started.md:8. Right-click **Column1.1**, and then select **Rename**.
hdinsight-get-started.md:In this tutorial, you have learned how to provision a cluster with HDInsight, run a MapReduce job on it, and import the results into Excel where they can be further processed and graphically displayed using BI tools. To learn more, see the following articles:
hdinsight-get-started.md:- [Develop C# Hadoop streaming programs for HDInsight][hdinsight-develop-streaming]
hdinsight-get-started.md:- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]
hdinsight-get-started.md:[hdinsight-develop-streaming]: /en-us/documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs/
hdinsight-get-started.md:[hdinsight-develop-mapreduce]: /en-us/documentation/articles/hdinsight-develop-deploy-java-mapreduce/
hdinsight-hadoop-develop-deploy-streaming-jobs.md:<properties linkid="manage-services-hdinsight-develop-hadoop-streaming-programs-for-hdinsight" urlDisplayName="" pageTitle="Develop C# Hadoop streaming programs for HDInsight | Windows Azure" metaKeywords="hdinsight hdinsight development, hadoop development, dhinsight deployment, development, deployment, tutorial, MapReduce" description="Learn how to develop Hadoop streaming MapReduce programs in C#, and how to deploy them to Windows Azure HDInsight." metaCanonical="" services="" documentationCenter="" title="Develop C# Hadoop streaming programs for HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-hadoop-develop-deploy-streaming-jobs.md:Hadoop provides a streaming API to MapReduce that enables you to write map and reduce functions in languages other than Java. This tutorial walks you through an end-to-end scenario from developing/testing a Hadoop streaming MapReduce program using C# on an HDInsight emulator to running the MapReduce job on Windows Azure HDInsight and to retrieving the results.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:- Install Windows Azure PowerShell on the emulator computer. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure]
hdinsight-hadoop-develop-deploy-streaming-jobs.md:- [Develop a word count Hadoop streaming program in C#](#develop)
hdinsight-hadoop-develop-deploy-streaming-jobs.md:- [Upload data and the applications to Windows Azure Blob storage](#upload)
hdinsight-hadoop-develop-deploy-streaming-jobs.md:##<a name="develop"></a>Develop a word count Hadoop streaming program in C&#35;
hdinsight-hadoop-develop-deploy-streaming-jobs.md:The word count solution contains two console application projects: mapper and reducer.  The mapper application streams each word into the console and the reducer application counts the number of words that are streamed from a document. Both the mapper and reducer read characters, line by line, from the standard input stream (stdin) and write to the standard output stream (stdout).
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Click **FILE**, **New**, and then click **Project**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:5. In Solution Explorer, right-click **Program.cs**, and then click **Rename**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:6. Rename the file to **WordCountMapper.cs**, and then press **ENTER**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:11. Click **BUILD**, and then click **Build Solution** to compile the Mapper program.	
hdinsight-hadoop-develop-deploy-streaming-jobs.md:1. From Visual Studio 2013, click **FILE**, click **Add** and then click **New Project**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. From Solution Explorer, right-click **Program.cs**, and then click **Rename**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:5. Rename the file to **WordCountReducer.cs**, and then press **ENTER**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:11. Click **BUILD**, and then click **Build Solution** to compile the solution.	
hdinsight-hadoop-develop-deploy-streaming-jobs.md:The Mapper and the Reducer executables are located at:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. upload the Mapper and Reducer to the emulator HDFS
hdinsight-hadoop-develop-deploy-streaming-jobs.md:By default, HDInsight emulator uses HDFS as the default file system.  Optionally, you can configure the HDInsight emulator to use Windows Azure Blob storage. For details, see [Get started with HDInsight Emulator][hdinsight-emulator-wasb]. In this section, you will use the HDFS copyFromLocal command to upload the files. The next section shows you how to upload files using Windows Azure PowerShell. For other methods, see [Upload data to HDInsight][hdinsight-upload-data].
hdinsight-hadoop-develop-deploy-streaming-jobs.md:<tr><td>\WordCount\Apps</td><td>The folder for the mapper and reducer executables.</td></tr>
hdinsight-hadoop-develop-deploy-streaming-jobs.md:<p>The Hadoop HDFS commands are case sensitive.</p> 
hdinsight-hadoop-develop-deploy-streaming-jobs.md:1. From the Hadoop command line window, run the following command to make a directory for the input files:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Run the following command to copy some text files to the input folder on HDFS:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following command to list the uploaded files:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:**To deploy the Mapper and the Reducer to the emulator HDFS**
hdinsight-hadoop-develop-deploy-streaming-jobs.md:1. Open Hadoop command line from your desktop.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Run the following commands:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following command to list the uploaded files
hdinsight-hadoop-develop-deploy-streaming-jobs.md:1. Open Windows Azure PowerShell. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure]. 
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following commands to set variables:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. Run the following commands to define the streaming job:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:5. Run the following command to create a credential object:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:6. Run the following commands to submit the MapReduce job and wait for the job to complete:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. If a job failed, you can open the job details and find some helpful information for debugging.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:1. Open Hadoop command line.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Run the following commands to display the output:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:	You can append "|more" at the end of the command to get the page view.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:Windows Azure HDInsight uses Windows Azure Blob storage as the default file system. You can configure an HDInsight cluster to use additional Blob storage for the data files. In this section, you will create a storage account and upload the data files to the Blob storage.  The data files are the txt files in the %hadoop_home% directory.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:**To create a Blob storage and a container**
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Set the variables, and then run the commands:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following command to create a storage account and a Blob storage container on the account
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. Run the following commands to verify the storage account and the container:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Set the first three variables, and then run the commands:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:	Notice the source file folder is **c:\Hadoop\hadoop-1.1.0-SNAPSHOT**, and the destination folder is **WordCount/Input**.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following commands to get a list of the txt files in the source file folder:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. Run the following commands to create a storage context object:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:5. Run the following commands to copy the files:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:6. Run the following command to list the uploaded files:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Set the first three variables, and then run the commands:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. Run the following commands to create a storage context object:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:5. Run the following commands to copy the applications:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:6. Run the following command to list the uploaded files:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:	4. display standard error
hdinsight-hadoop-develop-deploy-streaming-jobs.md:	5. display standard output
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Copy and paste the following code:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:		# The storage account and the HDInsight cluster variables
hdinsight-hadoop-develop-deploy-streaming-jobs.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -Subscription $subscriptionName -JobId $mrJob.JobId -StandardError 
hdinsight-hadoop-develop-deploy-streaming-jobs.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -Subscription $subscriptionName -JobId $mrJob.JobId -StandardOutput
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Set the first four variables in the script. $serviceNameToken will be used for the HDInsight cluster name, the storage account name, and the Blob storage container name.  Because the service name must be between 3 and 24 characters, and the script append string with up to 10 character string to the names, you  must limit the string to 14 or less characters. You must use all lower case for the $serviceNameToken. $storageAccountName_Data and $containerName_Data are the storage account and container that are used for storing the data files and the applications. $location must match the data storage account location.
hdinsight-hadoop-develop-deploy-streaming-jobs.md:7. Run the following command to set the execution policy to remotesigned:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:8. When prompted, enter username and password for the HDInsight cluster. Because you will delete the cluster at the end of the script and you will not need the username and password anymore, the username and password can be any strings. If you don't want to get prompted for the credentials, see [Working with Passwords, Secure Strings and Credentials in Windows PowerShell][powershell-PSCredential]
hdinsight-hadoop-develop-deploy-streaming-jobs.md:This section shows you how to download and display the output.  For the information on displaying the results on Excel, see [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-excel], and [Connect Excel to Windows Azure HDInsight with Power Query][hdinsight-powerquery].
hdinsight-hadoop-develop-deploy-streaming-jobs.md:2. Set the values and then run the commands:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:3. Run the following commands to create an Azure storage contect object: 
hdinsight-hadoop-develop-deploy-streaming-jobs.md:4. Run the following commands to download and display the output:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:In this tutorial, you have learned how to develop a Hadoop streaming MapReduce job, how to test the application on HDInsight emulator, and how to write a PowerShell script to provision an HDInsight cluster and run a MapReduce on the cluster. To learn more, see the following articles:
hdinsight-hadoop-develop-deploy-streaming-jobs.md:- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]
hdinsight-hadoop-develop-deploy-streaming-jobs.md:[hdinsight-develop-mapreduce]: /en-us/documentation/articles/hdinsight-develop-deploy-java-mapreduce/
hdinsight-hadoop-develop-deploy-streaming-jobs.md:[hdinsight-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/
hdinsight-hadoop-develop-deploy-streaming-jobs.md:[powershell-PSCredential]: http://social.technet.microsoft.com/wiki/contents/articles/4546.working-with-passwords-secure-strings-and-credentials-in-windows-powershell.aspx
hdinsight-hadoop-recommendation-engine.md:Apache Mahout™ is a machine learning library built for use in scalable machine learning applications. Recommender engines are one of the most popular types of machine learning applications in use today and have many obvious marketing applications.
hdinsight-hadoop-recommendation-engine.md:Apache Mahout provides a built-in implementation for Item-based Collaborative Filtering. This approach is widely used to conduct recommendation data mining. Item-based collaborative filtering was developed by Amazon.com. The idea here is that data on user preferences that exhibit correlations between item preferences can be use to infer the tastes of future users from a similar group.
hdinsight-hadoop-recommendation-engine.md:In this tutorial you use the  [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/tasteprofile) site and download the [dataset](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip) to create song recommendations for users based on their past listening habits.
hdinsight-hadoop-recommendation-engine.md:1. [Setup and configuration](#setup)
hdinsight-hadoop-recommendation-engine.md:2. [Examining and formatting the data](#segment1)
hdinsight-hadoop-recommendation-engine.md:## <a name="setup"></a>Setup and configuration 
hdinsight-hadoop-recommendation-engine.md:This tutorial assumes that you have gotten setup with Windows Azure and the HDinsight preview and that you have created an HDInsight cluster on which you can run a sample. If you have not done this already, consult the [Get Started with the Windows Azure HDInsight](/en-us/manage/services/hdinsight/get-started-hdinsight/) tutorial for instructions on how to satisfy these prerequisites.
hdinsight-hadoop-recommendation-engine.md:## <a name="segment1"></a>Examining and formatting the data 
hdinsight-hadoop-recommendation-engine.md:1.	Convert the IDs of both the songs and users to integer values.
hdinsight-hadoop-recommendation-engine.md:If you do not have Visual Studio 2010 installed, please skip this step and go to Running Mahout Job Section to get a pre-generated version.
hdinsight-hadoop-recommendation-engine.md:Start by launching Visual Studio 2010. In Visual Studio, select **File -> New -> Project**. In the **Installed Templates** pane, inside the **Visual C#** node, select the **Window** category, and then select **Console Application** from the list. Name the project "ConvertToMahoutInput" and click the **OK** button.
hdinsight-hadoop-recommendation-engine.md:1. Once the application is created, open the **Program.cs** file and add the following static members to the **Program** class:
hdinsight-hadoop-recommendation-engine.md:2. Next, add the `using System.IO;` statment and fill the **Main** method with the following code:
hdinsight-hadoop-recommendation-engine.md:3. Now create the **GetUser** and **GetSong** functions, which convert the ids to integers:
hdinsight-hadoop-recommendation-engine.md:5. Download the sample data from [this link](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip). Once downloaded, open **train\_triplets.txt.zip** and extract **train\_triplets.txt**.
hdinsight-hadoop-recommendation-engine.md:	When running the utility, include a command line argument with the location of **train\_triplets.txt**. To do so, right-click the **ConvertToMahoutInput** project node in **Solution Explorer** and select **Properties**. On the project properties page, select the **Debug** tab on the left side, and add the path of &lt;localpath&gt;train\_triplets.txt to the **Command line arguments** text box:
hdinsight-hadoop-recommendation-engine.md:	![setting command line arguments][set-cmd-line-args]
hdinsight-hadoop-recommendation-engine.md:###Setting the Command line argument
hdinsight-hadoop-recommendation-engine.md:- Press **F5** to run the program. Once complete, open the **bin\Debug** folder from the location to which the project was saved, and view the utility's output.  You should find users.txt and mInput.txt
hdinsight-hadoop-recommendation-engine.md:- Open the HDInsight cluster portal, and click the **Remote Desktop** icon.
hdinsight-hadoop-recommendation-engine.md:2. Then Copy it onto the cluster by selecting the local zip file and press control-v to copy, then paste it in to your Hadoop Cluster.
hdinsight-hadoop-recommendation-engine.md:<p>You can generate recommendations for more users by putting their IDs on separate lines. If you have issues generating mInput.txt and users.txt you may download an pre-generated version at this github <a href="https://github.com/wenming/BigDataSamples/tree/master/mahout">repository</a>. 
hdinsight-hadoop-recommendation-engine.md:It is the most convenient to download everything as one <a href="https://github.com/wenming/BigDataSamples/archive/master.zip">zip file</a>. Find users.txt and mInput.txt and copy them to the remote cluster in folder c:\</p> 
hdinsight-hadoop-recommendation-engine.md:At this point you should open a Hadoop terminal window and navitate to the folder that contains users.txt and mInput.txt.  
hdinsight-hadoop-recommendation-engine.md:![Mahout command window][mahout-cmd-window]
hdinsight-hadoop-recommendation-engine.md:###Hadoop Command Window
hdinsight-hadoop-recommendation-engine.md:1. Next, copy both **mInput.txt** and **users.txt** to HDFS. To do so, open the **Hadoop Command Shell** and run the following commands:
hdinsight-hadoop-recommendation-engine.md:3. Now we can run the Mahout job using the following command:
hdinsight-hadoop-recommendation-engine.md:	There are many other "distance" functions that the recommendation engine could use to compare the feature fector for different users, you may experiment and change the Similarity class to SIMILARITY\_COOCCURRENCE, SIMILARITY\_LOGLIKELIHOOD, SIMILARITY\_TANIMOTO\_COEFFICIENT, SIMILARITY\_CITY\_BLOCK, SIMILARITY\_COSINE, SIMILARITY\_PEARSON\_CORRELATION, SIMILARITY\_EUCLIDEAN\_DISTANCE.  For the purpose of this tutorial we will not go into the detailed data science aspect of Mahout. 
hdinsight-hadoop-recommendation-engine.md:4. The Mahout job should run for several minutes, after which an output file will be created. Run the following command to create a local copy of the output file:
hdinsight-hadoop-recommendation-engine.md:5. Open the **output.txt** file from the **c:\\** root folder and inspect its contents. The structure of the file is as follows:
hdinsight-hadoop-recommendation-engine.md:Recommender engines provide important functionality for many modern social networking sites, online shopping, streaming media, and other internet sites. Mahout provides an out-of-the-box recommendation engine that is easy to use, contains many useful features, and is scalable on Hadoop.
hdinsight-hadoop-recommendation-engine.md:While this article demonstrates using the Hadoop command line, you can also perform tasks using the HDInsight Interactive Console. For more information, see [Guidance: HDInsight Interactive JavaScript and Hive Consoles][interactive-console].
hdinsight-hadoop-recommendation-engine.md:[set-cmd-line-args]: ./media/hdinsight-hadoop-recommendation-engine/setting-command-line-arguments.png
hdinsight-hadoop-recommendation-engine.md:[mahout-cmd-window]: ./media/hdinsight-hadoop-recommendation-engine/mahout-commandwindow.PNGhdinsight-interactive-console.md:<properties linkid="manage-services-hdinsight-howto-work-with-the-interactive-console" urlDisplayName="Interactive Console" pageTitle="How to work with the interactive console in HDInsight Service" metaKeywords="" description="In this guide, you'll learn how to perform common tasks such as file upload, run jobs, and visualize data using the interactive console for HDInsight Service." metaCanonical="" services="hdinsight" documentationCenter="" title="HDInsight Interactive JavaScript and Hive Consoles" authors=""  solutions="" writer="bradsev" manager="paulettm" editor="mollybos"  />
hdinsight-interactive-console.md:# HDInsight Interactive JavaScript and Hive Consoles
hdinsight-interactive-console.md:Microsoft Azure HDInsight Service comes with interactive consoles for both JavaScript and Hive. These consoles provide a simple, interactive read-evaluate-print loop (REPL) experience, where users can enter expressions, evaluated them, and then query and display the results of a MapReduce job immediately. The JavaScript console executes Pig Latin statements. The Hive console evaluates Hive Query Language (Hive QL) statements. Both types of statements get compiled into MapReduce programs. Managing Hadoop jobs on these consoles is much simpler than remote connecting into the head node of the Hadoop cluster and working with MapReduce programs directly.
hdinsight-interactive-console.md:**The JavaScript Console**: a command shell that provides a fluent interface to the Hadoop ecosystem. A fluent interface uses a method of chaining instructions that relays the context of one call in a sequence to the subsequent call in that sequence. The JavaScript console provides:
hdinsight-interactive-console.md:- Access to the Hadoop cluster, its resources, and the Hadoop Distributed File System (HDFS) commands.
hdinsight-interactive-console.md:- Management and manipulate of data coming into and out of the Hadoop cluster.
hdinsight-interactive-console.md:- A fluent interface that evaluates Pig Latin and JavaScript statements to define a series of MapReduce programs to create data processing workflows.
hdinsight-interactive-console.md:**The Hive Console**: Hive is a data warehouse framework, built on top of Hadoop, that provides data management, querying, and analysis. It uses HiveQL, an SQL dialect, to query data stored in a Hadoop cluster. The Hive console provides:
hdinsight-interactive-console.md:- Access to the Hadoop cluster, its resources, and the HDFS commands.		
hdinsight-interactive-console.md:The JavaScript console uses Pig Latin, a data flow language, and the Hive console uses HiveQL, a query language. 	
hdinsight-interactive-console.md:Pig (and the JavaScript console) will tend to be preferred by those who are more familiar with a scripting approach, where a sequence of chained (or fluent) transformations is used to define a data processing workflow. It is also a good choice if you have seriously unstructured data.	
hdinsight-interactive-console.md:Hive (and its console) will tend to be preferred by those who are more familiar with SQL and a relational database environment. The use of schema and a table abstraction in Hive means the experience is very close to that typically encountered in a RDBMS.
hdinsight-interactive-console.md:Pig and Hive provide higher level languages that are compiled into MapReduce programs that are written in Java and that run on the HDFS. If you need really precise control or high performance you will need to write the MapReduce programs directly.
hdinsight-interactive-console.md:2. Click **HDINSIGHT**. You shall see a list of deployed Hadoop clusters.
hdinsight-interactive-console.md:5. Enter your credential, and then click **Log On**.
hdinsight-interactive-console.md:8. Click **WordCount.js** from the upper right, and save the file  to a local directory, for example the ../downloads folder.
hdinsight-interactive-console.md:12. Run the following command:
hdinsight-interactive-console.md:15. Run the following commands to list the file and display the content:
hdinsight-interactive-console.md:16. Run the following command to list the data file that will be processed by the WordCount MapReduce job: 
hdinsight-interactive-console.md:17. Run the following command to execute the MapReduce program: 
hdinsight-interactive-console.md:	Note how the instructions are "chained" together using the dot operator, and the output file is called *DaVinciTop10Words*. In the next section, you will access the output file. 
hdinsight-interactive-console.md:18. Scroll to the right, and then click on **View Log** if you want to observe the job progress. This log will also provide diagnostics if the job fails to complete. When the job does complete, you will see the following message at the end of the log:
hdinsight-interactive-console.md:19. Run the following command to list the output file:
hdinsight-interactive-console.md:1. Run the following command to display the results in the DaVinciTop10Words directory:
hdinsight-interactive-console.md:		and	8428
hdinsight-interactive-console.md:2. Run the following command to parse the contents of the file into a data file:
hdinsight-interactive-console.md:3. Run the following command to plot the data
hdinsight-interactive-console.md:2. Enter the following command to create a two column table named _DaVinciWordCountTable_ from the WordCount sample output that was saved in the "DaVinciTop10Words" folder:
hdinsight-interactive-console.md:4. Enter the following commands to confirm that the two column table has been created:
hdinsight-interactive-console.md:1. Run the following command to query for the words with the top ten number of occurrences:
hdinsight-interactive-console.md:		and 8428
hdinsight-interactive-console.md:You have seen how to run a Hadoop job from the Interactive JavaScript console and how to inspect the results from a job using this console. You have also seen how the Interactive Hive console can be used to inspect and process the results of a Hadoop job by creating and querying a table that contains the output from a MapReduce program. You have seen examples of Pig Latin and Hive QL statements being used in the consoles. Finally, you have seen how the REPL interactive nature of the JavaScript and Hive consoles simplifies using a Hadoop cluster. To learn more, see the following articles:
hdinsight-introduction.md:<properties linkid="manage-services-hdinsight-introduction-hdinsight" urlDisplayName="HDInsight Introduction" pageTitle="Introduction to Windows Azure HDInsight | Windows Azure" metaKeywords="" description="Learn how Windows Azure HDInsight uses Apache Hadoop clusters in the cloud, to provide a software framework to manage, analyze, and report on big data." metaCanonical="" services="hdinsight" documentationCenter="" title="Introduction to Windows Azure HDInsight" authors=""  solutions="" writer="bradsev" manager="paulettm" editor="cgronlun"  />
hdinsight-introduction.md:Windows Azure HDInsight is a service that deploys and provisions Apache™ Hadoop® clusters in the cloud, providing a software framework designed to manage, analyze, and report on big data.
hdinsight-introduction.md:Data is described as "big data" to indicate that it is being collected in ever escalating volumes, at increasingly high velocities, and for a widening variety of unstructured formats and variable semantic contexts. Big data collection does not provide value to an enterprise on its own. For big data to provide value in the form of actionable intelligence or insight, not only must the right questions be asked and data relevant to the issues be collected, the data must be accessible, cleaned, analyzed, and then presented in a useful way, often in combination with data from various other sources that establish perspective and context in what is now referred to as a mashup.
hdinsight-introduction.md:Apache Hadoop is a software framework that facilitates big data management and analysis. Apache Hadoop core provides reliable data storage with the Hadoop Distributed File System (HDFS), and a simple MapReduce programming model to process and analyze, in parallel, the data stored in this distributed system. HDFS uses data replication to address hardware failure issues that arise when deploying such highly distributed systems.
hdinsight-introduction.md:To simplify the complexities of analyzing unstructured data from various sources, the MapReduce programming model provides a core abstraction that underwrites closure for map and reduce operations. The MapReduce programming model views all of its jobs as computations over datasets consisting of key-value pairs. So both input and output files must contain datasets that consist only of key-value pairs. The primary takeaway from this constraint is the MapReduce jobs are, as a result, composable.
hdinsight-introduction.md:Other Hadoop-related projects such as Pig and Hive are built on top of HDFS and the MapReduce framework. Projects such as these are used to provide a simpler way to manage a cluster than working with the MapReduce programs directly. Pig, for example, enables you to write programs using a procedural language called Pig Latin that are compiled to MapReduce programs on the cluster. It also provides fluent controls to manage data flow. Hive is a data warehouse infrastructure that provides a table abstraction for data in files stored in a cluster which can then be queried using SQL-like statements in a declarative language called HiveQL.
hdinsight-introduction.md:Windows Azure HDInsight makes Apache Hadoop available as a service in the cloud. It makes the HDFS/MapReduce software framework and related projects such as Pig and Hive available in a simpler, more scalable, and cost-efficient environment.
hdinsight-introduction.md:One of the primary efficiencies introduced by HDInsight is in how it manages and stores data. HDInsight uses Windows Azure Blob storage as the default file system. Blob storage and HDFS are distinct file systems that are optimized, respectively, for the storage of data and for computations on that data.
hdinsight-introduction.md:- Windows Azure Blob storage provides a highly scalable and available, low cost, long term, and shareable storage option for data that is to be processed using HDInsight.
hdinsight-introduction.md:- The Hadoop clusters deployed by HDInsight on HDFS are optimized for running MapReduce computational tasks on the data.
hdinsight-introduction.md:HDInsight clusters are deployed in Azure on compute nodes to execute MapReduce tasks and can be dropped by users once these tasks have been completed. Keeping the data in the HDFS clusters after computations have been completed would be an expensive way to store this data. Blob storage is a robust, general purpose Azure storage solution. So storing data in Blob storage enables the clusters used for computation to be safely deleted without losing user data. But Blob storage is not just a low cost solution: It provides a full-featured HDFS file system interface that provides a seamless experience to customers by enabling the full set of components in the Hadoop ecosystem to operate (by default) directly on the data that it manages.
hdinsight-introduction.md:HDInsight uses Windows Azure PowerShell to configure, run, and post-process Hadoop jobs. HDInsight also provides a Sqoop connector that can be used to import data from a Windows Azure SQL database to HDFS or to export data to a Windows Azure SQL database from HDFS.
hdinsight-introduction.md:Microsoft Power Query for Excel is available for importing data from Windows Azure HDInsight or any HDFS into Excel. This add-on enhances the self-service BI experience in Excel by simplifying data discovery and access to a broad range of data sources. In addition to Power Query, the Microsoft Hive ODBC Driver is available to integrate business intelligence (BI) tools such as Excel, SQL Server Analysis Services, and Reporting Services, facilitating and simplifying end-to-end data analysis.
hdinsight-introduction.md:This topic describes the Hadoop ecosystem supported by HDInsight, the main use scenarios for HDInsight, and a guide to further resources. It contains the following sections:
hdinsight-introduction.md: * <a href="#Ecosystem">The Hadoop Ecosystem on HDInsight</a>: HDInsight provides implementations of Pig, Hive and Sqoop, and supports other BI tools such as Excel, SQL Server Analysis Services and Reporting Services that are integrated with Blob storage/HDFS and the MapReduce framework using either the Power Query or the Microsoft Hive ODBC Driver. This section describes what jobs these programs in the Hadoop ecosystem are designed to handle.
hdinsight-introduction.md:HDInsight offers a framework implementing Microsoft's cloud-based solution for handling big data. This federated ecosystem manages and analyses large data amounts, exploiting the parallel processing capabilities of the MapReduce programming model. The Apache-compatible Hadoop technologies that can be used with HDInsight are itemized and briefly described in this section.
hdinsight-introduction.md:HDInsight provides implementations of Hive and Pig to integrate data processing and warehousing capabilities.  Microsoft's big data solution  integrates with Microsoft's BI tools, such as SQL Server Analysis Services, Reporting Services, PowerPivot, and Excel. This enables you to perform a straightforward BI on data stored and managed by HDInsight in Blob storage. 
hdinsight-introduction.md:Other Apache-compatible technologies and sister technologies that are part of the Hadoop ecosystem and have been built to run on top of Hadoop clusters can also be downloaded are used with HDInsight. These include open source technologies such as Sqoop which integrate HDFS with relational data stores. 
hdinsight-introduction.md:Pig is a high-level platform for processing big data on Hadoop clusters. Pig consists of a data flow language, called Pig Latin, supporting writing queries on large datasets and an execution environment running programs from a console. The Pig Latin programs consist of dataset transformation series converted under the covers, to a MapReduce program series. Pig Latin abstractions provide richer data structures than MapReduce, and perform for Hadoop what SQL performs for Relational Database Management Systems (RDBMS). Pig Latin is fully extensible. User Defined Functions (UDFs), written in Java, Python, Ruby, C#, or JavaScript, can be called to customize each processing path stage when composing the analysis. For additional information, see [Welcome to Apache Pig!](http://pig.apache.org/)
hdinsight-introduction.md:Hive is a distributed data warehouse managing data stored in an HDFS. It is the Hadoop query engine. Hive is for analysts with strong SQL skills providing an SQL-like interface and a relational data model. Hive uses a language called HiveQL; a dialect of SQL. Hive, like Pig, is an abstraction on top of MapReduce and when run, Hive translates queries into a series of MapReduce jobs. Scenarios for Hive are closer in concept to those for RDBMS, and so are appropriate for use with more structured data. For unstructured data, Pig is better choice. For additional information, see [Welcome to Apache Hive!](http://hive.apache.org/)
hdinsight-introduction.md:Sqoop is tool that transfers bulk data between Hadoop and relational databases such a SQL, or other structured data stores, as efficiently as possible. Use Sqoop to import data from external structured data stores into the HDFS or related systems like Hive. Sqoop can also extract data from Hadoop and export the extracted data to external relational databases, enterprise data warehouses, or any other structured data store type. For additional information, see the  [Apache Sqoop](http://sqoop.apache.org/) Web site.
hdinsight-introduction.md:###Business intelligence tools and connectors
hdinsight-introduction.md:Familiar business intelligence (BI) tools - such as Excel, PowerPivot, SQL Server Analysis Services and Reporting Services - retrieve, analyze, and report data integrated with HDInsight using either the Power Query add-in or the Microsoft Hive ODBC Driver.
hdinsight-introduction.md:These conditions apply to a wide variety of activities in business, science and governance. These might include, for example, monitoring supply chains in retail, suspicious trading patterns in finance, demand patterns for public utilities and services, air and water quality from arrays of environmental sensors, or crime patterns in metropolitan areas.
hdinsight-introduction.md:HDInsight (and Hadoop technologies in general) are most suitable for handling a large amount of logged or archived data that does not require frequent updating once it is written, and that is read often, typically to do a full analysis. This scenario is complementary to data more suitably handled by a RDBMS that requires lesser amounts of data (gigabytes instead of petabytes), and that must be continually updated or queried for specific data points within the full dataset. RDBMS work best with structured data organized and stored according to a fixed schema. MapReduce works well with unstructured data with no predefined schema because it is capable of interpreting that data when it is being processed.
hdinsight-introduction.md:* [HDInsight Documentation](http://go.microsoft.com/fwlink/?LinkID=285601): The documentation page for Windows Azure HDInsight with links to articles, videos, and more resources.
hdinsight-introduction.md:* [Big data and Windows Azure](http://www.windowsazure.com/en-us/home/scenarios/big-data/): Big data scenarios that explore what you can build with Windows Azure.	
hdinsight-introduction.md:**Microsoft: Windows and SQL Database**	
hdinsight-introduction.md:* [Windows Azure home page](https://www.windowsazure.com/en-us/): Scenarios, free trial sign up, development tools and documentation that you need get started building applications.
hdinsight-introduction.md:* [Management Portal for SQL Database](http://msdn.microsoft.com/en-us/library/windowsazure/gg442309.aspx): A lightweight and easy-to-use database management tool for managing SQL Database in the cloud.
hdinsight-introduction.md:* [Microsoft BI PowerPivot](http://www.microsoft.com/en-us/bi/PowerPivot.aspx): Download and get information about a powerful data mashup and exploration tool.
hdinsight-introduction.md:* [SQL Server 2012 Analysis Services](http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/SQL-Server-2012-analysis-services.aspx): Download an evaluation copy of SQL Server 2012 and learn how to build comprehensive, enterprise-scale analytic solutions that deliver actionable insights.	
hdinsight-introduction.md:* [SQL Server 2012 Reporting](http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/SQL-Server-2012-reporting-services.aspx): Download an evaluation copy of SQL Server 2012 and learn how to create comprehensive, highly scalable solutions that enables real-time decision making across the enterprise. 
hdinsight-introduction.md:* [HDFS](http://hadoop.apache.org/docs/r0.18.1/hdfs_design.html): Learn more about the architecture and design of the Hadoop Distributed File System (HDFS), the primary storage system used by Hadoop applications.		
hdinsight-monitor.md:<properties linkid="manage-services-hdinsight-howto-monitor-hdinsight" urlDisplayName="Monitor" pageTitle="Monitor HDInsight | Windows Azure" metaKeywords="" description="Learn how to monitor an HDInsight cluster and view Hadoop job history through the Windows Azure Management Portal." metaCanonical="" services="hdinsight" documentationCenter="" title="How to Monitor HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="mollybos"  />
hdinsight-monitor.md:To monitor the health of an HDInsight cluster and the Hadoop jobs running on the cluster, you can connect to the HDInsight Dashboard, and click the Monitor Cluster tile.
hdinsight-monitor.md:On the right, it shows both Namenode and job tracker are up running, and the 4 data nodes are running in the healthy state.
hdinsight-monitor.md:On the left, it shows the map reduce metrics for the past 30 minutes. You can change the monitor windows to 30 minutes, 1 hour, 3 hours, 12 hours, 1 day, 2 days, 1 week and 2 weeks.
hdinsight-monitor.md:To view Hadoop job history, connect to HDInsight Dashboard, and then click the Job History tile. 
hdinsight-monitor.md:* [How to: Deploy an HDInsight Cluster Programmatically](/en-us/manage/services/hdinsight/howto-deploy-cluster/)
hdinsight-provision-clusters.md:* [Using Cross-platform Command Line](#cli)
hdinsight-provision-clusters.md:<p>Currently, only the Southeast Asia, North Europe, West Europe, East US and the West US regions can host HDInsight clusters.</p>
hdinsight-provision-clusters.md:2. Click **+ NEW** on the bottom of the page, click **DATA SERVICES**, click **HDINSIGHT**, and then click **CUSTOM CREATE**.
hdinsight-provision-clusters.md:				<li>DNS name must start and end with alpha numeric, may contain dashes.</li>
hdinsight-provision-clusters.md:			<td>The default container on the storage account will be used as the default file system for the HDInsight cluster. When <strong>Use Existing Storage</strong> in the STORAGE ACCOUNT field and <strong>Create default container</strong> in the DEFAULT CONTAINER filed are chosen, the default container name has the same name as the cluster. If a container with the name fo the cluster already exists, a sequence number will be appended to the container name. For example, mycontainer1, mycontainer2, and so on.</td></tr>
hdinsight-provision-clusters.md:Windows Azure PowerShell is a powerful scripting environment that you can use to control and automate the deployment and management of your workloads in Windows Azure. For information on configuring a workstation to run HDInsight Powershell cmdlets, see [Install and configure Windows Azure PowerShell][powershell-install-configure]. For more information on using PowerShell with HDInsight, see [Administer HDInsight using PowerShell][hdinsight-admin-powershell]. For the list of the HDInsight PowerShell cmdlets, see [HDInsight cmdlet reference][hdinsight-powershell-reference].
hdinsight-provision-clusters.md:HDInsight uses a Windows Azure Blob Storage container as the default file system. A Windows Azure storage account and storage container are required before you can create an HDInsight cluster. The storage account must be located in the same data center as the HDInsight Cluster.
hdinsight-provision-clusters.md:- Run the following commands from a Windows Azure PowerShell console window:
hdinsight-provision-clusters.md:	If you have already had a storage account but do not know the account name and account key, you can use the following PowerShell commands to retrieve the information:
hdinsight-provision-clusters.md:- Run the following commands from a Windows Azure PowerShell window:
hdinsight-provision-clusters.md:Once you have the storage account and the blob container prepared, you are ready to create a cluster. 
hdinsight-provision-clusters.md:- Run the following commands from a Windows Azure PowerShell window:		
hdinsight-provision-clusters.md:You can also provision cluster and configure it to connect to more than one Azure Blob storage or custom Hive and Oozie metastores. This advanced feature allows you to separate lifetime of your data and metadata from the lifetime of the cluster. 
hdinsight-provision-clusters.md:- Run the following commands from a Windows PowerShell window:
hdinsight-provision-clusters.md:- Run the following commands from a Windows Azure PowerShell window:
hdinsight-provision-clusters.md:##<a id="cli"></a> Using Cross-platform command line
hdinsight-provision-clusters.md:Another option for provisioning an HDInsight cluster is the Cross-platform Command-line Interface. The command-line tool is implemented in Node.js. It can be used on any platform that supports Node.js including Windows, Mac and Linux. The command-line tool is open source.  The source code is managed in GitHub at <a href= "https://github.com/WindowsAzure/azure-sdk-tools-xplat">https://github.com/WindowsAzure/azure-sdk-tools-xplat</a>. For a general guide on how to use the command-line interface, see [How to use the Windows Azure Command-Line Tools for Mac and Linux][azure-command-line-tools]. For comprehensive reference documentation, see [Windows Azure command-line tool for Mac and Linux][azure-command-line-tool]. This article only covers using the command-line interface from Windows.
hdinsight-provision-clusters.md:The following procedures are needed to provision an HDInsight cluster using Cross-platform command line:
hdinsight-provision-clusters.md:- Install cross-platform command line
hdinsight-provision-clusters.md:- Download and import Windows Azure account publish settings
hdinsight-provision-clusters.md:The command-line interface can be installed using *Node.js Package Manager (NPM)* or Windows Installer.
hdinsight-provision-clusters.md:**To install the command-line interface using NPM**
hdinsight-provision-clusters.md:2.	Click **INSTALL** and following the instructions using the default settings.
hdinsight-provision-clusters.md:3.	Open **Command Prompt** (or *Windows Azure Command Prompt*, or *Developer Command Prompt for VS2012*) from your workstation.
hdinsight-provision-clusters.md:4.	Run the following command in the command prompt window.
hdinsight-provision-clusters.md:	<p>If you get an error saying the NPM command is not found, verify the following paths are in the PATH environment variable:
hdinsight-provision-clusters.md:5.	Run the following command to verify the installation:
hdinsight-provision-clusters.md:**To install the command-line interface using windows installer**
hdinsight-provision-clusters.md:2.	Scroll down to the **Command line tools** section, and then click **Cross-platform Command Line Interface** and follow the Web Platform Installer wizard.
hdinsight-provision-clusters.md:Before using the command-line interface, you must configure connectivity between your workstation and Windows Azure. Your Windows Azure subscription information is used by the command-line interface to connect to your account. This information can be obtained from Windows Azure in a publish settings file. The publish settings file can then be imported as a persistent local config setting that the command-line interface will use for subsequent operations. You only need to import your publish settings once.
hdinsight-provision-clusters.md:**To download and import publish settings**
hdinsight-provision-clusters.md:1.	Open a **Command Prompt**.
hdinsight-provision-clusters.md:2.	Run the following command to download the publish settings file.
hdinsight-provision-clusters.md:	The command shows the instructions for downloading the file, including an URL.
hdinsight-provision-clusters.md:3.	Open **Internet Explorer** and browse to the URL listed in the command prompt window.
hdinsight-provision-clusters.md:5.	From the command prompt window, run the following command to import the publish settings file:
hdinsight-provision-clusters.md:- From the command prompt window, run the following command:
hdinsight-provision-clusters.md:If you have already had a storage account but do not know the account name and account key, you can use the following commands to retrieve the information:
hdinsight-provision-clusters.md:For details on getting the information using the management portal, see the *How to: View, copy and regenerate storage access keys* section of [How to Manage Storage Accounts][azure-manage-storageaccount].
hdinsight-provision-clusters.md:The *azure hdinsight cluster create* command creates the container if it doesn't exist. If you choose to create the container beforehand, you can use the following command:
hdinsight-provision-clusters.md:Once you have the storage account and the blob container prepared, you are ready to create a cluster.
hdinsight-provision-clusters.md:- From the command prompt window, run the following command:
hdinsight-provision-clusters.md:Typically, you provision an HDInsight cluster, run their jobs, and then delete the cluster to cut down the cost. The command-line interface gives you the option to save the configurations into a file, so that you can reuse it every time you provision a cluster.  
hdinsight-provision-clusters.md:- From the command prompt window, run the following command:
hdinsight-provision-clusters.md:**To list and show cluster details**
hdinsight-provision-clusters.md:- Use the following commands to list and show cluster details:
hdinsight-provision-clusters.md:- Use the following command to delete a cluster:
hdinsight-provision-clusters.md:2. From the File menu, click **New**, and then click **Project**.
hdinsight-provision-clusters.md:6. Run the following commands in the console to install the packages.
hdinsight-provision-clusters.md:	This command adds .NET libraries and references to them to the current Visual Studio project.
hdinsight-provision-clusters.md:9. In the Main() function, copy and paste the following code:
hdinsight-provision-clusters.md:While the application is open in Visual Studio, press **F5** to run the application. A console window should open and display the status of the application. It can take several minutes to create a HDInsight cluster.
hdinsight-provision-clusters.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/ 
hdinsight-provision-clusters.md:[azure-command-line-tools]: /en-us/develop/nodejs/how-to-guides/command-line-tools/
hdinsight-provision-clusters.md:[azure-command-line-tool]: /en-us/manage/linux/other-resources/command-line-tools/
hdinsight-provision-clusters.md:[image-cli-clusterlisting]: ./media/hdinsight-provision-clusters/HDI.CLIListClusters.png "List and show clusters"
hdinsight-run-samples.md:Much additional documentation exists on the web for Hadoop-related technologies such as Java-based MapReduce programming and streaming, as well as documentation on the cmdlets using in PowerShell scripting. For more information on these resources, see the final **Resources for HDInsight** section of the [Introduction to Windows Azure HDInsight][hdinsight-resources] topic.
hdinsight-run-samples.md:<p>These samples are intended to get you up to speed quickly on how to deploy Hadoop jobs and to provide you an extensible testing bed to work with the concepts and scripting procedures used by the service. They provide you with examples of common tasks such as creating and importing data sets of various sizes, running jobs and composing jobs sequentially, and examining the results of your jobs. The data sets used can be varied in size, allowing you to observe the effects that data sets of various size has on job performance.</p>
hdinsight-run-samples.md:- You must have installed Windows Azure PowerShell, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-run-samples.md:- [**The 10-GB Graysort Sample**][10gb-graysort] This tutorial shows how to run a general purpose GraySort on a 10 GB file using HDInsight. There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm the data has been properly sorted.
hdinsight-run-samples.md:From this article and the articles on each of the samples, you learned how to run the samples included with the HDInsight clusters using Windows Azure PowerShell. For tutorials on using Pig, Hive, and MapReduce with HDInsight, see the following topics:
hdinsight-sample-10gb-graysort.md:This sample uses a modest 10 GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578 TB/min (100 TB in 173 minutes). For more information on this and other sorting benchmarks, see the [Sortbenchmark](http://sortbenchmark.org/)   site.
hdinsight-sample-10gb-graysort.md:2. **TeraSort** samples the input data and uses MapReduce to sort the data into a total order. TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] <= key < sample[i] are sent to reduce i. This guarantees that the output of reduce i are all less than the output of reduce i+1.
hdinsight-sample-10gb-graysort.md:3. **TeraValidate** is a MapReduce program that validates the output is globally sorted. It creates one map per a file in the output directory and each map ensures that each key is less than or equal to the previous one. The map function also generates records of the first and last keys of each file and the reduce function ensures that the first key of file i is greater than the last key of file i-1. Any problems are reported as output of the reduce with the keys that are out of order.
hdinsight-sample-10gb-graysort.md:The input and output format, used by all three applications, read and write the text files in the right format. The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require the output data be replicated on to multiple nodes.
hdinsight-sample-10gb-graysort.md:- You must have installed Windows Azure PowerShell, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-10gb-graysort.md:This topic shows you how to run the series of MapReduce programs that make up the Sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-10gb-graysort.md:1. Open Windows Azure PowerShell. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-10gb-graysort.md:2. Set the two variables in the following commands, and then run them:
hdinsight-sample-10gb-graysort.md:		# Provide the Windows Azure subscription name and the HDInsight cluster name.
hdinsight-sample-10gb-graysort.md:4. Run the following command to create a MapReduce job definition"
hdinsight-sample-10gb-graysort.md:5. Run the following commands to submit job, wait for job to complete and then print the standard error:
hdinsight-sample-10gb-graysort.md:		# Print output and standard error file of the MapReduce job
hdinsight-sample-10gb-graysort.md:		$teragen | Start-AzureHDInsightJob -Cluster $clustername | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | Get-AzureHDInsightJobOutput -Cluster $clustername -StandardError 
hdinsight-sample-10gb-graysort.md:2. Set the two variables in the following commands, and then run them:
hdinsight-sample-10gb-graysort.md:		# Provide the Windows Azure subscription name and the HDInsight cluster name.
hdinsight-sample-10gb-graysort.md:3. Run the following command to define the MapReduce job: 	 
hdinsight-sample-10gb-graysort.md:	The *"-Dmapred.map.tasks=50"* argument specifies that 50 maps will be created to execute the job. The *100000000* argument specifies the amount of data to generate. The final two arguments specify the input and output directories. 
hdinsight-sample-10gb-graysort.md:4. Run the following command to submit the job, wait for the job to complete, and print the standand error:
hdinsight-sample-10gb-graysort.md:		# Print output and standard error file of the MapReduce job 
hdinsight-sample-10gb-graysort.md:		$terasort | Start-AzureHDInsightJob -Cluster $clustername | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | Get-AzureHDInsightJobOutput -Cluster $clustername -StandardError 
hdinsight-sample-10gb-graysort.md:2. Set the two variables in the following commands, and then run them:
hdinsight-sample-10gb-graysort.md:		# Provide the Windows Azure subscription name and the HDInsight cluster name.
hdinsight-sample-10gb-graysort.md:3. Run the following command to define the MapReduce job: 
hdinsight-sample-10gb-graysort.md:	The *"-Dmapred.map.tasks=50"* argument specifies that 50 maps will be created to execute the job. he *"-Dmapred.reduce.tasks=25"* argument specifies that 25 reduce tasks will be created to execute the job. The final two arguments specify the input and output directories.  
hdinsight-sample-10gb-graysort.md:4. Run the following commands to submit the MapReduce job, wait for the job to complete and print the standard error:
hdinsight-sample-10gb-graysort.md:		# Print output and standard error file of the MapReduce job 
hdinsight-sample-10gb-graysort.md:		$teravalidate | Start-AzureHDInsightJob -Cluster $clustername | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | Get-AzureHDInsightJobOutput -Cluster $clustername -StandardError 
hdinsight-sample-10gb-graysort.md:	 * See the License for the specific language governing permissions and	
hdinsight-sample-10gb-graysort.md:	 * and waits for it to finish. 	
hdinsight-sample-10gb-graysort.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-csharp-streaming.md:Hadoop provides a streaming API to MapReduce that enables you to write map and reduce functions in languages other than Java. This tutorial shows how to write MapReduce progams in C# that uses the Hadoop streaming interface and how to run the programs on Windows Azure HDInsight using Windows Azure  PowerShell. 
hdinsight-sample-csharp-streaming.md:In the example, both the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line by line) and emit the output to [stdout][stdin-stdout-stderr]. The program counts all of the words in the text.
hdinsight-sample-csharp-streaming.md:When an executable is specified for **mappers**, each mapper task launches the executable as a separate process when the mapper is initialized. As the mapper task runs, it converts its inputs into lines and feeds the lines to the [stdin][stdin-stdout-stderr] of the process. In the meantime, the mapper collects the line-oriented outputs from the stdout of the process and converts each line into a key/value pair, which is collected as the output of the mapper. By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) is the value. If there is no tab character in the line, then entire line is considered as key and the value is null. 
hdinsight-sample-csharp-streaming.md:When an executable is specified for **reducers**, each reducer task launches the executable as a separate process when the reducer is initialized. As the reducer task runs, it converts its input key/values pairs into lines and feeds the lines to the [stdin][stdin-stdout-stderr] of the process. In the meantime, the reducer collects the line-oriented outputs from the [stdout][stdin-stdout-stderr] of the process, converts each line into a key/value pair, which is collected as the output of the reducer. By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) is the value. 
hdinsight-sample-csharp-streaming.md:- You must have installed Windows Azure PowerShell, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-csharp-streaming.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-csharp-streaming.md:1.	Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-csharp-streaming.md:3. Set the two variables in the following commands, and then run them:
hdinsight-sample-csharp-streaming.md:2. Run the following command to define the MapReduce job.
hdinsight-sample-csharp-streaming.md:	The parameters specify the mapper and reducer functions and the input file and output files.
hdinsight-sample-csharp-streaming.md:5. Run the following commands to run the MapReduce job, wait for the job to complete, and then print the standard error:
hdinsight-sample-csharp-streaming.md:		# Print output and standard error file of the MapReduce job
hdinsight-sample-csharp-streaming.md:		$streamingWC | Start-AzureHDInsightJob -Cluster $clustername | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | Get-AzureHDInsightJobOutput -Cluster $clustername -StandardError 
hdinsight-sample-csharp-streaming.md:6. Run the following commands to display the results of the word count.
hdinsight-sample-csharp-streaming.md:		# Blob storage container and account name
hdinsight-sample-csharp-streaming.md:The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and wc.exe application as the reduce interface to count the number of words that are streamed from a document. Both the mapper and reducer read characters, line by line, from the standard input stream (stdin) and write to the standard output stream (stdout). 
hdinsight-sample-csharp-streaming.md:The mapper code in the cat.cs file uses a StreamReader object to read the characters of the incoming stream into the console, which in turn writes the stream to the standard output stream with the static Console.Writeline method.
hdinsight-sample-csharp-streaming.md:The reducer code in the wc.cs file uses a [StreamReader][streamreader]   object to read characters from the standard input stream that have been output by the cat.exe mapper. As it reads the characters with the [Console.Writeline][console-writeline] method, it counts the words by counting space and end-of-line characters at the end of each word, and then it writes the total to the standard output stream with the [Console.Writeline][console-writeline] method. 
hdinsight-sample-csharp-streaming.md:In this tutorial, you saw how to deploy a MapReduce job on HDInsight using Hadoop Streaming.
hdinsight-sample-csharp-streaming.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-pi-estimator.md:The program uses a statistical (quasi-Monte Carlo) method to estimate the value of Pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, Pi/4. The value of Pi can be estimated from the value of 4R where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.
hdinsight-sample-pi-estimator.md:The PiEstimator Java code that contains the mapper and reducer functions is available for inspection below. The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle. The reducer program accumulates points counted by the mappers and then estimates the value of Pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.
hdinsight-sample-pi-estimator.md:The script provided for this sample submits a Hadoop JAR job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values. These parameter values can be changed to improve the estimated value of Pi. For reference, the first 10 decimal places of Pi are 3.1415926535.
hdinsight-sample-pi-estimator.md:The .jar file that contains the files needed by Hadoop on Azure to deploy the application is a .zip file and is available for download. You can unzip it with various compression utilities then explore the files at your convenience.
hdinsight-sample-pi-estimator.md:- You must have installed Windows Azure PowerShell, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-pi-estimator.md:This topic shows you how to run the sample, presents the Java code for the Pi Estimator MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-pi-estimator.md:1. Open Windows Azure PowerShell. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-pi-estimator.md:2. Set the two variables in the following commands, and then run them:
hdinsight-sample-pi-estimator.md:4. Run the following command to create a MapReduce definition:	
hdinsight-sample-pi-estimator.md:	The first argument indicates how many maps to create (default is 16). The second argument indicates how many samples are generated per map (10 million by default). So this program uses 10*10 million = 160 million random points to make its estimate of Pi. 
hdinsight-sample-pi-estimator.md:5. Run the following commands to submit the MapReduce job and wait for the job to complete:
hdinsight-sample-pi-estimator.md:6. Run the following command to retrieve the MapReduce job standard output:
hdinsight-sample-pi-estimator.md:		# Print output and standard error file of the MapReduce job
hdinsight-sample-pi-estimator.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $piJob.JobId -StandardOutput
hdinsight-sample-pi-estimator.md: 	* See the License for the specific language governing permissions and	
hdinsight-sample-pi-estimator.md:	//and then count points inside/outside of the inscribed circle of the square.	
hdinsight-sample-pi-estimator.md:	//and the area of unit square is 1.	
hdinsight-sample-pi-estimator.md:	//where H(i) is a 2-dimensional point and i >= 1 is the index.		
hdinsight-sample-pi-estimator.md:	//Generate points in a unit square and then		
hdinsight-sample-pi-estimator.md: 	// turn off speculative execution, because DFS doesn't handle		
hdinsight-sample-pi-estimator.md:	//Parse arguments and then runs a map/reduce job.	
hdinsight-sample-pi-estimator.md:	//Print output in standard out.		
hdinsight-sample-pi-estimator.md:	 ToolRunner.printGenericCommandUsage(System.err);
hdinsight-sample-pi-estimator.md:	 //main method for running it as a stand alone command. 		
hdinsight-sample-pi-estimator.md:In this tutorial, you saw how to run a MapReduce job on HDInsight and how to use Monte Carlo methods that require and generare large datasets that can be managed by this service.
hdinsight-sample-pi-estimator.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-sqoop-import-export.md:<properties linkid="manage-services-hdinsight-sample-sqoop-import-export" urlDisplayName="HDInsight Samples" pageTitle="Sqoop Import-Export sample | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure" description="Learn how to run Sqoop import and export on HDInsight." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="Sqoop Import-Export sample" />
hdinsight-sample-sqoop-import-export.md:While Hadoop is a natural choice for processing unstructured and semi-structured data, such as logs and files, there may also be a need to process structured data stored in relational databases.
hdinsight-sample-sqoop-import-export.md:Sqoop is a tool designed to transfer data between Hadoop clusters and relational databases. You can use it to import data from a relational database management system (RDBMS) such as SQL or MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop with MapReduce or Hive, and then export the data back into an RDBMS. In this tutorial, you are using a SQL Database for your relational database.
hdinsight-sample-sqoop-import-export.md:Sqoop is an open source software product of Cloudera, Inc. Software development for Sqoop moved in 2011 from gitHub to the [Apache Sqoop](http://sqoop.apache.org/) site.
hdinsight-sample-sqoop-import-export.md:In Windows Azure HDInsight, Sqoop is deployed from the Hadoop Command Shell on the head node of the HDFS cluster. You use the Remote Desktop feature available in the Hadoop on Azure portal to access the head node of the cluster for this deployment.
hdinsight-sample-sqoop-import-export.md:You have a Windows Azure Account and have enabled the HDInsight Service for your subscription. You have installed Windows Azure PowerShell and the Powershell tools for Windows Azure HDInsight, and have configured them for use with your account. For instructions on how to do this, see [Get started with Windows Azure HDInsight](/en-us/manage/services/hdinsight/get-started-hdinsight/)
hdinsight-sample-sqoop-import-export.md:You will also need your outward facing IP address for your current location when configuring your firewall on SQL Database. To obtain it, go to the site [WhatIsMyIP][what-is-my-ip] and make a note of it. Later in the procedure, you also need the outward facing IP address for the head of the Hadoop cluster. You can obtain this IP address in the same way.
hdinsight-sample-sqoop-import-export.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-sqoop-import-export.md:Log in into your Windows Azure account. To create a database server, click the **Database** icon in the lower left-hand corner on the page.
hdinsight-sample-sqoop-import-export.md:3. Select the type of subscription (such as **Pay-As-You-Go**) associated with you account in the **Create Server** window and press **Next**.
hdinsight-sample-sqoop-import-export.md:4. Select the appropriate **Region** in the **Create Server** window and click **Next**.
hdinsight-sample-sqoop-import-export.md:5. Specify the login and password of the server-level principal of your SQL Database server and then press **Next**.
hdinsight-sample-sqoop-import-export.md:8. Unzip the file, open an Administrator Command Prompt, and navigate to the AdventureWorks directory inside the AdventureWorks2012ForSQLAzure folder.
hdinsight-sample-sqoop-import-export.md:	For example, if the assigned SQL Database server is named b1gl33p, the administrator user name "Fred", and the password "Secret", you would type the following:
hdinsight-sample-sqoop-import-export.md:	The script creates the database, installs the schema, and populates the database with sample data.
hdinsight-sample-sqoop-import-export.md:10. Return to the **WindowsAzurePlatform** portal page, click your subscription on the left-hand side (**Pay-As-You-Go** in the example below) and select your database (here named wq6xlbyoq0). The AventureWorks2012 should be listed in the **Database Name** column. Select it and press the **Manage** icon at the top of the page.
hdinsight-sample-sqoop-import-export.md:11. Enter the credentials for the SQL database when prompted and press **Log on**.
hdinsight-sample-sqoop-import-export.md:14. Run the following query and review its result. 
hdinsight-sample-sqoop-import-export.md:1. From your Account page, scroll down to the Open Ports icon in the Your cluster section and click the icon to open the ODBC Server port on the head node in your cluster. 
hdinsight-sample-sqoop-import-export.md:2. Return to your Account page, scroll down to the Your cluster section and click the **Remote Desktop** icon this time to open the head node in your cluster. TBD: update to use with PS.
hdinsight-sample-sqoop-import-export.md:5. Enter your credentials for the Hadoop cluster (not your Hadoop on Azure account) into the **Windows Security** window and select **OK**.
hdinsight-sample-sqoop-import-export.md:6. Open Internet Explorer and go to the site WhatIsMyIP   to obtain the outward facing IP address for the head node of the cluster. Return the SQL Database management page and add a firewall rule that allows your Hadoop cluster access to SQL Database. The firewall grants access based on the originating IP address of each request. 
hdinsight-sample-sqoop-import-export.md:7. Double-click on the Hadoop Command Shell icon in the upper left hand of the Desktop to open it. Navigate to the "c:\Apps\dist\sqoop\bin" directory and run the following command:
hdinsight-sample-sqoop-import-export.md:	The sqoop command is:
hdinsight-sample-sqoop-import-export.md:8. Return to the **Accounts** page of the Hadoop on Azure portal and open the Interactive Console this time. Run the #lsr command from the JavaScript console to list the files and directories on your HDFS cluster. TBD Update for GA.
hdinsight-sample-sqoop-import-export.md:9. Run the #tail command to view selected results from the part-m-0000 file.
hdinsight-sample-sqoop-import-export.md:In this tutorial, you saw how to transfer data between a Hadoop cluster managed by Windows Azure HDInsight and a relational database using Apache Sqoop.
hdinsight-sample-sqoop-import-export.md:For tutorials runnng other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-wordcount.md:This sample topic shows how to run a MapReduce program that counts word occurrences in a text file with Windows Azure HDInsight using Windows Azure PowerShell. The WordCount MapReduce program is written in Java and runs on an HDInsight cluster. The text file analyzed here is the Project Gutenberg eBook edition of The Notebooks of Leonardo Da Vinci. 
hdinsight-sample-wordcount.md:The Hadoop MapReduce program reads the text file and counts how often each word occurs. The output is a new text file that consists of lines, each of which contains a word and the count (a key/value tab-separated pair) of how often that word occurred in the document. This process is done in two stages. The mapper takes each line from the input text as an input and breaks it into words. It emits a key/value pair each time a work occurs of the word followed by a 1. The reducer then sums these individual counts for each word and emits a single key/value pair that contains the word followed by the sum of its occurrences.
hdinsight-sample-wordcount.md:- You must have installed Windows Azure PowerShell, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure]
hdinsight-sample-wordcount.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-wordcount.md:1.	Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-sample-wordcount.md:3. Set the two variables in the following commands, and then run them:
hdinsight-sample-wordcount.md:5. Run the following command to create a MapReduce job definition:
hdinsight-sample-wordcount.md:	The hadoop-examples.jar file comes with the HDInsight cluster. There are two arguments for the MapReduce job. The first one is the source file name, and the second is the output file path. The source file comes with the HDInsight cluster, and the output file path will be created at the run-time.
hdinsight-sample-wordcount.md:6. Run the following command to submit the MapReduce job:
hdinsight-sample-wordcount.md:8. Run the following command to check any errors with running the MapReduce job:	
hdinsight-sample-wordcount.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $wordCountJob.JobId -StandardError 
hdinsight-sample-wordcount.md:2. Set the three variables in the following commands, and then run them:
hdinsight-sample-wordcount.md:3. Run the following commands to create a Windows Azure storage context object:
hdinsight-sample-wordcount.md:	The *Select-AzureSubscription* is used to set the current subscription in case you have multiple subscriptions, and the default subscription is not the one to use. 
hdinsight-sample-wordcount.md:4. Run the following command to download the MapReduce job output from the Blob container to the workstation:
hdinsight-sample-wordcount.md:5. Run the following command to print the MapReduce job output file:
hdinsight-sample-wordcount.md:	The MapReduce job produces a file named *part-r-00000* with the words and the counts.  The script uses the findstr command to list all of the words that contains *"there"*.
hdinsight-sample-wordcount.md:For tutorials runnng other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-wordcount.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/
hdinsight-submit-hadoop-jobs-programmatically.md:<properties linkid="manage-services-hdinsight-submit-hadoop-jobs-programmatically" urlDisplayName="HDInsight Administration" pageTitle="Submit Hadoop jobs programmatically | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure, hive, mapreduce, HDInsight .NET SDK, powershell, submit mapreduce jobs, submit hive jobs, development, hadoop, apache" description="Learn how to programmatically submit Hadoop jobs to Windows Azure HDInsight." umbracoNaviHide="0" disqusComments="1" writer="jgao" editor="cgronlun" manager="paulettm" title="Submit Hadoop jobs programmatically"/>
hdinsight-submit-hadoop-jobs-programmatically.md:In this article, you will learn how to submit MapReduce and Hive jobs using PowerShell and HDInsight .NET SDK.
hdinsight-submit-hadoop-jobs-programmatically.md:* Install and configure Windows Azure PowerShell. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-submit-hadoop-jobs-programmatically.md:Windows Azure PowerShell is a powerful scripting environment that you can use to control and automate the deployment and management of your workloads in Windows Azure. For more information on using PowerShell with HDInsight, see [Administer HDInsight using PowerShell][hdinsight-admin-powershell].
hdinsight-submit-hadoop-jobs-programmatically.md:Hadoop MapReduce is a software framework for writing applications which process vast amounts of data. HDInsight clusters come with a jar file, located at *\example\jars\hadoop-examples.far*, which contains several MapReduce examples. One of the examples is for counting word frequencies in source files. In this session, you will learn how to use PowerShell from a workstation to run the word count sample. For more information on developing and running MapReduce jobs, see [Use MapReduce with HDInsight][hdinsight-mapreduce].
hdinsight-submit-hadoop-jobs-programmatically.md:1.	Open **Windows Azure PowerShell**. For instructions on opening the Windows Azure PowerShell console window, see the [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-submit-hadoop-jobs-programmatically.md:3. Set these two variables by running the following PowerShell commands:
hdinsight-submit-hadoop-jobs-programmatically.md:5. Run the following commands to create a MapReduce job definition:
hdinsight-submit-hadoop-jobs-programmatically.md:	There are two arguments. The first one is the source file name, and the second is the output file path. For more information of the WASB prefix, see [Use Windows Azure Blob storage with HDInsight][hdinsight-storage].
hdinsight-submit-hadoop-jobs-programmatically.md:6. Run the following command to run the MapReduce job:
hdinsight-submit-hadoop-jobs-programmatically.md:7. Run the following command to check the completion of the MapReduce job:
hdinsight-submit-hadoop-jobs-programmatically.md:8. Run the following command to check any errors with running the MapReduce job:	
hdinsight-submit-hadoop-jobs-programmatically.md:		# Get the job standard error output
hdinsight-submit-hadoop-jobs-programmatically.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $wordCountJob.JobId -StandardError 
hdinsight-submit-hadoop-jobs-programmatically.md:2. Set these three variables by running teh following PowerShell commands:
hdinsight-submit-hadoop-jobs-programmatically.md:3. Run the following commands to create a Windows Azure storage context object:
hdinsight-submit-hadoop-jobs-programmatically.md:	The Select-AzureSubscription is used to set the current subscription in case you have multiple subscriptions, and the default subscription is not the one to use. 
hdinsight-submit-hadoop-jobs-programmatically.md:4. Run the following command to download the MapReduce job output from the Blob container to the workstation:
hdinsight-submit-hadoop-jobs-programmatically.md:5. Run the following command to print the MapReduce job output file:
hdinsight-submit-hadoop-jobs-programmatically.md:	The MapReduce job produces a file named *part-r-00000* with the words and the counts.  The script uses the findstr command to list all of the words that contains "there".
hdinsight-submit-hadoop-jobs-programmatically.md:Apache [Hive][apache-hive] provides a means of running MapReduce job through an SQL-like scripting language, called *HiveQL*, which can be applied towards summarization, querying, and analysis of large volumes of data. 
hdinsight-submit-hadoop-jobs-programmatically.md:1.	Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-submit-hadoop-jobs-programmatically.md:2. Set the first two variables in the following commands, and then run the commands:
hdinsight-submit-hadoop-jobs-programmatically.md:3. Run the following commands to select Azure subscription and the cluster to run the Hive job:
hdinsight-submit-hadoop-jobs-programmatically.md:The HDInsight .NET SDK provides .NET client libraries that makes it easier to work with HDInsight clusters from .NET. HDInsight clusters come with a jar file, located at *\example\jars\hadoop-examples.jar*, which contains several MapReduce examples. One of the examples is for counting word frequencies in source files. In this session, you will learn how to create a .NET application to run the word count sample. For more information on developing and running MapReduce jobs, see [Use MapReduce with HDInsight][hdinsight-mapreduce].
hdinsight-submit-hadoop-jobs-programmatically.md:2. From the File menu, click **New**, and then click **Project**.
hdinsight-submit-hadoop-jobs-programmatically.md:6. Run the following commands in the console to install the packages.
hdinsight-submit-hadoop-jobs-programmatically.md:	This command adds .NET libraries and references to them to the current Visual Studio project. The version shall be 0.11.0.1 or newer.
hdinsight-submit-hadoop-jobs-programmatically.md:9. In the Main() function, copy and paste the following code:
hdinsight-submit-hadoop-jobs-programmatically.md:	For information on certificate, see [Create and Upload a Management Certificate for Windows Azure][azure-certificate]. An easy way to configure the certificate is to run *Get-AzurePublishSettingsFile* and *Import-AzurePublishSettingsFile* PowerShell cmdlets. They will create and upload the management certificate automatically. After you run the PowerShell cmdlets, you can open *certmgr.msc* from the workstation, and find the certificate by expanding *Personal/Certificates*. The certificate created by the PowerShell cmdlets has *Windows Azure Tools* for both the *Issued To* and the *Issued By* fields.
hdinsight-submit-hadoop-jobs-programmatically.md:	There are two arguments. The first one is the source file name, and the second is the output file path. For more information of the WASB prefix, see [Use Windows Azure Blob storage with HDInsight][hdinsight-storage].
hdinsight-submit-hadoop-jobs-programmatically.md:10. In the Main() function, append the following code to run the job and wait the job to complete:
hdinsight-submit-hadoop-jobs-programmatically.md:While the application is open in Visual Studio, press **F5** to run the application. A console window should open and display the status of the application and the application output. 
hdinsight-submit-hadoop-jobs-programmatically.md:2. From the File menu, click **New**, and then click **Project**.
hdinsight-submit-hadoop-jobs-programmatically.md:6. Run the following commands in the console to install the packages.
hdinsight-submit-hadoop-jobs-programmatically.md:	This command adds .NET libraries and references to them to the current Visual Studio project.
hdinsight-submit-hadoop-jobs-programmatically.md:9. In the Main() function, copy and paste the following code:
hdinsight-submit-hadoop-jobs-programmatically.md:	For information on certificate, see [Create and Upload a Management Certificate for Windows Azure][azure-certificate]. An easy way to configure the certificate is to run *Get-AzurePublishSettingsFile* and *Import-AzurePublishSettingsFile* PowerShell cmdlets. They will create and upload the management certificate automatically. After you run the PowerShell cmdlets, you can open *certmgr.msc* from the workstation, and find the certificate by expanding *Personal/Certificates*. The certificate created by the PowerShell cmdlets has *Windows Azure Tools* for both the *Issued To* and the *Issued By* fields.
hdinsight-submit-hadoop-jobs-programmatically.md:	There are two arguments. The first one is the source file name, and the second is the output file path. For more information of the WASB prefix, see [Use Windows Azure Blob storage with HDInsight][hdinsight-storage].
hdinsight-submit-hadoop-jobs-programmatically.md:10. In the Main() function, append the following code to run the job and wait the job to complete:
hdinsight-submit-hadoop-jobs-programmatically.md:While the application is open in Visual Studio, press **F5** to run the application. A console window should open and display the status of the application. The output shall be:
hdinsight-submit-hadoop-jobs-programmatically.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/ 
hdinsight-understand-nosql.md:<properties linkid="manage-hdinsight-understand-nosql" urlDisplayName="Understand NoSQL technologies" pageTitle="Understanding NoSQL Technologies on Windows Azure | Windows Azure" metaKeywords="" description="Learn how NoSQL technologies on HDInsight can help you manage data not suited to relational databases, such as big data sets and JSON documents or graphs." umbracoNaviHide="0" disqusComments="1" writer="dchappell" editor="cgronlun" manager="paulettm" title="Data management: Understanding NoSQL technologies on Windows Azure" />
hdinsight-understand-nosql.md:# Data management: Understanding NoSQL technologies on Windows Azure
hdinsight-understand-nosql.md:**Summary:** Windows Azure offers a broad array of options for your NoSQL workloads, but it can be challenging to determine which one is right for you.  This document explains NoSQL technologies on Windows Azure and how to choose the right service to manage your non-relational data, which includes unstructured or particularly large data sets often referred to as "big data." 
hdinsight-understand-nosql.md:Relational technology has long been the dominant approach for working with data. However, when you work with very large amounts of different types of data, scaling across many servers is challenging. Also, relational technology is not the best fit for managing some kinds of data, such as JSON documents or graphs. This guide walks through the options, explaining what each one provides and why you might want to use it.
hdinsight-understand-nosql.md:**Download:** <a href="http://go.microsoft.com/fwlink/p/?LinkId=330292" target="_blank">Understanding NoSQL technologies on Windows Azure PDF file</a>
hdinsight-upload-data.md:<properties linkid="manage-services-hdinsight-howto-upload-data-to-hdinsight" urlDisplayName="Upload Data" pageTitle="Upload data to HDInsight | Windows Azure" metaKeywords="" description="Learn how to upload and access data in HDInsight using Azure Storage Explorer, the interactive console, the Hadoop command line, or Sqoop." metaCanonical="" services="" documentationCenter="" title="Upload data to HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-upload-data.md:Windows Azure HDInsight provides a full-featured Hadoop Distributed File System (HDFS) over Windows Azure Blob storage. It has been designed as an HDFS extension to provide a seamless experience to customers by enabling the full set of components in the Hadoop ecosystem to operate directly on the data it manages. Both Windows Azure Blob storage and HDFS are distinct file systems that are optimized for storage of data and computations on that data. For the benefits of using Windows Azure Blob storage, see [Use Windows Azure Blob storage with HDInsight][hdinsight-storage]. 
hdinsight-upload-data.md:Windows Azure HDInsight clusters are typically deployed to execute MapReduce jobs and are dropped once these jobs have been completed. Keeping the data in the HDFS clusters after computations have been completed would be an expensive way to store this data. Windows Azure Blob storage is a highly available, highly scalable, high capacity, low cost, and shareable storage option for data that is to be processed using HDInsight. Storing data in a Blob enables the HDInsight clusters used for computation to be safely released without losing data. 
hdinsight-upload-data.md:* [Upload data to Blob storage using Hadoop command line](#commandline)
hdinsight-upload-data.md:AzCopy is a command line utility which is designed to simplify the task of transferring data in to and out of a Windows Azure Storage account. You can use this as a standalone tool or incorporate this utility in an existing application. [Download AzCopy][azure-azcopy-download].
hdinsight-upload-data.md:Windows Azure PowerShell is a powerful scripting environment that you can use to control and automate the deployment and management of your workloads in Windows Azure. You can use Windows Azure PowerShell to upload data to Blob storage, so the data can be processed by MapReduce jobs. For information on configuring your workstation to run Windows Azure PowerShell, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-upload-data.md:1. Open Windows Azure PowerShell console window as instructed in [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-upload-data.md:Blob storage containers store data as key/value pairs, and there is no directory hierarchy. However the "/" character can be used within the key name to make it appear as if a file is stored within a directory structure. For example, a blob's key may be *input/log1.txt*. No actual "input" directory exists, but due to the presence of the "/" character in the key name, it has the appearance of a file path. In the previous script, you can give the file a folder structure by setting the $blobname variable. For example *$blobname="myfolder/myfile.txt"*.
hdinsight-upload-data.md:- They hold some special metadata needed by the Hadoop file system, notably the permissions and owners for the folders.
hdinsight-upload-data.md:*Azure Storage Explorer* is a useful tool for inspecting and altering the data in your Windows Azure Storage. It is a free tool that can be downloaded from [http://azurestorageexplorer.codeplex.com/](http://azurestorageexplorer.codeplex.com/ "Azure Storage Explorer").
hdinsight-upload-data.md:Before using the tool, you must know your Windows Azure storage account name and account key. For instructions on getting this information, see the "How to: View, copy and regenerate storage access keys" section of [Manage storage accounts][azure-storage-account].  
hdinsight-upload-data.md:3. Enter **Storage account name** and **Storage account key**, and then click **Add Storage Account**. You can add multiple storage accounts, and each account will be displayed on a tab. 
hdinsight-upload-data.md:7. Specify a file to upload, and then click **Open**.
hdinsight-upload-data.md:##<a id="commandline"></a> Upload data to Blob storage using Hadoop Command Line
hdinsight-upload-data.md:To use Hadoop command line, you must first connect to the cluster using remote desktop. 
hdinsight-upload-data.md:2. Click **HDINSIGHT**. You will see a list of deployed Hadoop clusters.
hdinsight-upload-data.md:5. Click **ENABLE REMOTE** if you haven't enabled remote desktop, and follow the instructions.  Otherwise, skip to the next step.
hdinsight-upload-data.md:8. Enter your credentials, and then click **OK**.
hdinsight-upload-data.md:10. From the desktop, click **Hadoop Command Line**.
hdinsight-upload-data.md:13. Use the following command to list the uploaded files:
hdinsight-upload-data.md:Sqoop is a tool designed to transfer data between Hadoop and relational databases. You can use it to import data from a relational database management system (RDBMS) such as SQL or MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop with MapReduce or Hive, and then export the data back into a RDBMS. For more information, see [Sqoop User Guide][apache-sqoop-guide].
hdinsight-upload-data.md:Before importing data, you must know the Windows Azure SQL Database server name, database account name, account password, and database name. You must also configure a firewall rule for the database server to allow connections from your HDInsight cluster head node. For instruction on creating SQL database and configuring firewall rules, see [How to use Windows Azure SQL Database in .NET applications][sqldatabase-howto]. To obtain the outward facing IP Address for your HDInsight cluster head node, you can use Remote Desktop to connect to the head node, and then browse to [www.whatismyip.com][whatismyip].
hdinsight-upload-data.md:The following procedure uses PowerShell to submit a Sqoop job. This is only supported by version 1.2 or later HDInsight clusters.  Optionally, you can use Hadoop command line to run Sqoop commands.
hdinsight-upload-data.md:**To import data to HDInsight using Sqoop and PowerShell**
hdinsight-upload-data.md:1. Open Windows Azure PowerShell console window as instructed in [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-upload-data.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "import --connect jdbc:sqlserver://$sqlDatabaseServerName.database.windows.net;user=$sqlDatabaseUserName@$sqlDatabaseServerName;password=$sqlDatabasePassword;database=$sqlDatabaseDatabaseName --table $tableName --target-dir $hdfsOutputDir -m 1" 
hdinsight-upload-data.md:		Write-Host "Standard Error" -BackgroundColor Green
hdinsight-upload-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-upload-data.md:		Write-Host "Standard Output" -BackgroundColor Green
hdinsight-upload-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-upload-data.md:Note: When specifying an escape character as delimiter with the arguments *--input-fields-terminated-by* and *--input-fields-terminated-by*, do not put quotes around the escape character.  For example. 
hdinsight-upload-data.md:	# Run the job and show the standard error 
hdinsight-upload-data.md:	$wordCountJobDefinition | Start-AzureHDInsightJob -Cluster $clusterName  | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 | %{ Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $_.JobId -StandardError}
hdinsight-upload-data.md:Now that you understand how to get data into HDInsight, use the following articles to learn how to perform analysis:
hdinsight-upload-data.md:[azure-storage-client-library]: /en-us/develop/net/how-to-guides/blob-storage/
hdinsight-upload-data.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/
hdinsight-upload-data.md:[sqldatabase-howto]: http://www.windowsazure.com/en-us/develop/net/how-to-guides/sql-database/
hdinsight-use-blob-storage.md:<properties linkid="manage-services-hdinsight-howto-blob-store" urlDisplayName="Blob Storage with HDInsight" pageTitle="Use Blob storage with HDInsight | Windows Azure" metaKeywords="" description="Learn how HDInsight uses Blob storage as the underlying data store for HDFS and how you can query data from the store." metaCanonical="" services="storage,hdinsight" documentationCenter="" title="Use Windows Azure Blob storage with HDInsight" authors=""  solutions="" writer="sburgess" manager="paulettm" editor="mollybos"  />
hdinsight-use-blob-storage.md:Windows Azure HDInsight supports both Hadoop Distributed Files System (HDFS) and Windows Azure Blob storage for storing data. Blob storage is a robust, general purpose Windows Azure storage solution. Blob storage provides a full-featured HDFS interface for a seamless experience by enabling the full set of components in the Hadoop ecosystem to operate (by default) directly on the data. Blob storage is not just a low-cost solution; storing data in Blob storage enables the HDInsight clusters used for computation to be safely deleted without losing user data. 
hdinsight-use-blob-storage.md:> Most HDFS commands such as <b>ls</b>, <b>copyFromLocal</b>, <b>mkdir</b>, and so on, still work as expected. Only the commands that are specific to the native HDFS implementation (which is referred to as DFS) such as <b>fschk</b> and <b>dfsadmin</b> will show different behavior on Windows Azure Blob storage.
hdinsight-use-blob-storage.md:Hadoop supports a notion of default file system. The default file system implies a default scheme and authority; it can also be used to resolve relative paths. During the HDInsight provision process, user must specify a Blob storage container used as the default file system. 
hdinsight-use-blob-storage.md:* **Container in the same storage account:** Because the account name and key are stored in the *core-site.xml*, you have full access to the files in the container.
hdinsight-use-blob-storage.md:        > Public container allows you to get a list of all blobs available in that container and get container metadata. Public blob allows  you to access the blobs only if you know the exact URL. For more information, see <a href="http://msdn.microsoft.com/en-us/library/windowsazure/dd179354.aspx">Restrict access to containers and blobs</a>.
hdinsight-use-blob-storage.md:Blob storage containers store data as key/value pairs, and there is no directory hierarchy. However the "/" character can be used within the key name to make it appear as if a file is stored within a directory structure. For example, a blob's key may be *input/log1.txt*. No actual *input* directory exists, but due to the presence of the "/" character in the key name, it has the appearance of a file path.
hdinsight-use-blob-storage.md:The implied performance cost of not having compute and storage co-located is mitigated by the way the compute clusters are provisioned close to the storage account resources inside the Windows Azure data center, where the high speed network makes it very efficient for the compute nodes to access the data inside Blob storage.
hdinsight-use-blob-storage.md:* **Data reuse and sharing:** The data in HDFS is located inside the compute cluster. Only the applications that have access to the compute cluster can use the data using HDFS API. The data in Blob storage can be accessed either through the HDFS APIs or through the [Blob Storage REST APIs](http://msdn.microsoft.com/en-us/library/windowsazure/dd135733.aspx). Thus, a larger set of applications (including other HDInsight clusters) and tools can be used to produce and consume the data.
hdinsight-use-blob-storage.md:* **Geo-replication:** Your Blob storage containers can be geo-replicated through the Azure Portal. While this gives you geographic recovery and data redundancy, a fail-over to the geo-replicated location will severely impact your performance and may incur additional costs. So our recommendation is to choose the geo-replication wisely and only if the value of the data is worth the additional cost.
hdinsight-use-blob-storage.md:Certain MapReduce jobs and packages may create intermediate results that you don't really want to store in the Blob storage container. In that case, you can still elect to store the data in the local HDFS. In fact, HDInsight uses DFS for several of these intermediate results in Hive jobs and other processes. 
hdinsight-use-blob-storage.md:When provisioning an HDInsight cluster from Windows Azure Management Portal, there are two options: *quick create* and *custom create*. Using either of the options, a Windows Azure Storage account must be created beforehand.  For instructions, see [How to Create a Storage Account]( /en-us/manage/services/storage/how-to-create-a-storage-account/). 
hdinsight-use-blob-storage.md:Creating an HDInsight default file system can be done by creating a new Blob storage container through the commonly-used APIs in a storage account for which core-site.xml contains the storage key. In addition, you can also create a new container by referring to it in an HDFS command from Hadoop command line. For example:
hdinsight-use-blob-storage.md:The example command will not only create the new directory *newdirectory* but, if it doesn't exist, will also create a new container called *newcontainer*.
hdinsight-use-blob-storage.md:The URI scheme provides both unencrypted access with the *WASB:* prefix, and SSL encrypted access with *WASBS*. We recommend using *WASBS* wherever possible, even when accessing data that lives inside the same Windows Azure data center.
hdinsight-use-blob-storage.md:refers to the file *result.txt* on the read-only WASB file system in the root container at the location *myaccount.blob.core.windows.net* that gets accessed through SSL. Note that *wasb://myaccount.blob.core.windows.net/output/result.txt* results in an exception, because Blob storage does not allow "/" inside path names in the root container to avoid ambiguities between paths and folder names. 
hdinsight-use-blob-storage.md:Because HDInsight uses a Blob storage container as the default file system, you can refer to files and directories inside the default file system using relative or absolute paths. For example, the following statement lists all top-level directories and files of the default file system from Hadoop command line:
hdinsight-use-blob-storage.md:In this article, you learned how to use Blob storage with HDInsight and that Blob storage is a fundamental component of HDInsight. This allows you to build scalable, long-term archiving data acquisition solutions with Windows Azure Blob storage and use HDInsight to unlock the information inside the stored data.
hdinsight-use-hadoop-dotnet-sdk.md:<properties linkid="manage-services-hdinsight-howto-sdk" urlDisplayName="HDInsight SDK" pageTitle="How to use the HDInsight .NET libraries - Windows Azure Services" metaKeywords="" description="Learn how to get the HDInsight NuGet packages and use them from your .NET application." metaCanonical="" services="hdinsight" documentationCenter="" title="Use the Hadoop .NET SDK with HDInsight" authors=""  solutions="" writer="sburgess" manager="paulettm" editor="mollybos"  />
hdinsight-use-hadoop-dotnet-sdk.md:The Hadoop .NET SDK provides .NET client libraries that makes it easier to work with Hadoop from .NET. In this tutorial you will learn how to get the Hadoop .NET SDK and use it to build a simple .NET based application that runs Hive queries using the Windows Azure HDInsight Service. Given an actors.txt file, you will write an application to find the actor/actress who gets the most awards. 
hdinsight-use-hadoop-dotnet-sdk.md:* [Download and install the Hadoop .NET SDK](#install)
hdinsight-use-hadoop-dotnet-sdk.md:##<a id="install"></a> Download and Install the Hadoop .NET SDK##
hdinsight-use-hadoop-dotnet-sdk.md:* **LINQ to Hive client library** - translates C# or F# LINQ queries into HiveQL queries and executes them on the Hadoop cluster. This library can also execute arbitrary HiveQL queries from a .NET application.
hdinsight-use-hadoop-dotnet-sdk.md:* **WebClient library** - contains client libraries for *WebHDFS* and *WebHCat*.
hdinsight-use-hadoop-dotnet-sdk.md:	* **WebHDFS client library** - works with files in HDFS and Windows Azure Blog Storage.
hdinsight-use-hadoop-dotnet-sdk.md:	* **WebHCat client library** - manages the scheduling and execution of jobs in HDInsight cluster.
hdinsight-use-hadoop-dotnet-sdk.md:These commands add the libraries and references to the current Visual Studio project.
hdinsight-use-hadoop-dotnet-sdk.md:You must have a [Windows Azure subscription][free-trial], and a [Windows Azure Storage Account][create-storage-account] before you can proceed. You must also know your Windows Azure storage account name and account key. For the instructions on how to  get this information, see the *How to: View, copy and regenerate storage access keys* section of [How to Manage Storage Accounts](/en-us/manage/services/storage/how-to-manage-a-storage-account/).
hdinsight-use-hadoop-dotnet-sdk.md:You must also download the Actors.txt file used in this tutorial. Perform the following steps to download this file to your development environment:
hdinsight-use-hadoop-dotnet-sdk.md:2. Download [Actors.txt](http://www.microsoft.com/en-us/download/details.aspx?id=37003), and save the file to the C:\Tutorials folder.
hdinsight-use-hadoop-dotnet-sdk.md:In this section you will learn how to upload files to Hadoop cluster programmatically and how to execute Hive jobs using LINQ to Hive.
hdinsight-use-hadoop-dotnet-sdk.md:2. From the File menu, click **New**, and then click **Project**.
hdinsight-use-hadoop-dotnet-sdk.md:6. Run the following commands in the console to install the packages.
hdinsight-use-hadoop-dotnet-sdk.md:	These commands add .NET libraries and references to them to the current Visual Studio project.
hdinsight-use-hadoop-dotnet-sdk.md:9. In the Main() function, copy and paste the following code:
hdinsight-use-hadoop-dotnet-sdk.md:		Console.WriteLine("Creating MovieData directory and uploading actors.txt...");
hdinsight-use-hadoop-dotnet-sdk.md:		// and drop a previous Actors table.
hdinsight-use-hadoop-dotnet-sdk.md:		string command =
hdinsight-use-hadoop-dotnet-sdk.md:		hiveConnection.ExecuteHiveQuery(command).Wait();
hdinsight-use-hadoop-dotnet-sdk.md:		command =
hdinsight-use-hadoop-dotnet-sdk.md:		hiveConnection.ExecuteHiveQuery(command).Wait();
hdinsight-use-hadoop-dotnet-sdk.md:	If you choose to use the default file system container, you can find the storage account name, the storage key, and the container name from the *c:\apps\dist\hadoop-1.1.0-SNAPSHOT\conf>core-site.xml* configuration file by remoting to the cluster. The container used as the default file system can be found by search *fs.default.name*; the storage account name and the account key can be found by searching *fs.azure.account.key*.
hdinsight-use-hadoop-dotnet-sdk.md:While the application is open in Visual Studio, press **F5** to run the application. A console window should open and display the steps executed by the application as data is uploaded, stored into a Hive table, and finally queried. Once the application is complete and the query results have been returned, press any key to terminate the application.
hdinsight-use-hadoop-dotnet-sdk.md:Now you understand how to create a .NET application using Hadoop .NET SDK. To learn more, see the following articles:
hdinsight-use-hive.md:<properties linkid="manage-services-hdinsight-howto-hive" urlDisplayName="Use Hive with HDInsight" pageTitle="Use Hive with HDInsight | Windows Azure" metaKeywords="" description="Learn how to use Hive with HDInsight. You'll use a log file as input into an HDInsight table, and use HiveQL to query the data and report basic statistics." metaCanonical="" services="hdinsight" documentationCenter="" title="Use Hive with HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="mollybos"  />
hdinsight-use-hive.md:[Apache Hive][apache-hive] provides a means of running MapReduce job through an SQL-like scripting language, called *HiveQL*, which can be applied towards summarization, querying, and analysis of large volumes of data. In this article, you will use HiveQL to query the data in an Apache log4j log file and report basic statistics. 
hdinsight-use-hive.md:- You must have installed **Windows Azure PowerShell**, and have configured them for use with your account. For instructions on how to do this, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-hive.md:Databases are appropriate when managing smaller sets of data for which low latency queries are possible. However, when it comes to big data sets that contain terabytes of data, traditional SQL databases are often not an ideal solution. Database administrators have habitually scaling up to deal with these larger data sets, buying bigger hardware as database load increased and performance degraded.  
hdinsight-use-hive.md:Hive and HiveQL also offer an alternative to writing MapReduce jobs in Java when querying data. It provides a simple SQL-like wrapper that allows queries to be written in HiveQL that are then compiled to MapReduce for you by HDInsight and run on the cluster.
hdinsight-use-hive.md:Hive also allows programmers who are familiar with the MapReduce framework to plug in custom mappers and reducers to perform more sophisticated analysis that may not be supported by the built-in capabilities of the HiveQL language.  
hdinsight-use-hive.md:Hive is best suited for the batch processing of large amounts of immutable data (such as web logs). It is not appropriate for transaction applications that need very fast response times, such as database management systems. Hive is optimized for scalability (more machines can be added dynamically to the Hadoop cluster), extensibility (within the MapReduce framework and with other programming interfaces), and fault-tolerance. Latency is not a key design consideration.   
hdinsight-use-hive.md:Generally, applications save errors, exceptions and other coded issues in a log file, so administrators can use the data in the log files to review problems that may arise and to generate metrics that are relevant to errors or other issues like performance. These log files usually get quite large in size and contain a wealth of data that must be processed and mined for intelligence on the application. 
hdinsight-use-hive.md:Log files are therefore a paradigmatic example of big data. HDInsight provides a Hive data warehouse system that facilitates easy data summarization, ad-hoc queries, and the analysis of these big datasets stored in Hadoop compatible file systems such as Windows Azure Blob Storage.
hdinsight-use-hive.md:In this article, you use a log4j sample file distributed with the HDInsight cluster that is stored in *\example\data\sample.log*. Each log inside the file consists of a line of fields that contains a `[LOG LEVEL]` field to show the type and the severity. For example:
hdinsight-use-hive.md:replace *mycontainer* with the container name, and *mystorage* with the Blob storage account name. 
hdinsight-use-hive.md:In the last section, you uploaded a log4j file called sample.log to the default file system container.  In this section, you will run HiveQL to create a hive table, load data to the hive table, and then query the data to find out how many error logs there were.
hdinsight-use-hive.md:This article provides the instructions for using Windows Azure PowerShell cmdlets to run a Hive query. Before you go through this section, you must first setup the local environment, and configure the connection to Windows Azure as explained in the **Prerequisites** section at the beginning of this topic.
hdinsight-use-hive.md:1. Open a Windows Azure PowerShell console windows. The instructions can be found in [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-hive.md:2. Set the variables in the following script and run it:
hdinsight-use-hive.md:		# Provide Windows Azure subscription name, and the Azure Storage account and container that is used for the default HDInsight file system.
hdinsight-use-hive.md:	The LOAD DATA HiveQL command will result in moving the data file to the \hive\warehouse\ folder.  The DROP TABLE command will delete the table and the data file.  If you use the internal table option and want to run the script again, you must upload the sample.log file again. If you want to keep the data file, you must use the CREATE EXTERNAL TABLE command as shown in the script.
hdinsight-use-hive.md:	Use the DROP TABLE first in case you run the script again and the log4jlogs table already exists.
hdinsight-use-hive.md:7. Run the following script to print the standard output:
hdinsight-use-hive.md:		# Print the standard error and the standard output of the Hive job.
hdinsight-use-hive.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $hiveJob.JobId -StandardOutput
hdinsight-use-hive.md:2. Set the variables for the following script and run it:
hdinsight-use-hive.md:You can use the same command to run a HiveQL file:
hdinsight-use-hive.md:While Hive makes it easy to query data using a SQL-like query language, other components available with HDInsight provide complementary functionality such as data movement and transformation. To learn more, see the following articles:
hdinsight-use-hive.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/ 
hdinsight-use-mapreduce.md:Hadoop MapReduce is a software framework for writing applications which process vast amounts of data. In this tutorial, you will use Windows Azure PowerShell from your workstation to submit a MapReduce program that counts word occurrences in a text to an HDInsight cluster. The word counting program is written in Java and the program comes with the HDInsight cluster.
hdinsight-use-mapreduce.md:- A workstation with Windows Azure PowerShell installed and configured. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-mapreduce.md:1. [Understand the scenario](#scenario)
hdinsight-use-mapreduce.md:##<a id="scenario"></a>Understand the scenario
hdinsight-use-mapreduce.md:The output of the MapReduce job is a set of key-value pairs. The key is a string that specifies a word and the value is an integer that specifies the total number of occurrences of that word in the text. This is done in two stages: 
hdinsight-use-mapreduce.md:* The mapper takes each line from the input text as an input and breaks it into words. It emits a key/value pair each time a work occurs of the word followed by a 1. The output will be sorted before sending to reducer. 
hdinsight-use-mapreduce.md:* The reducer then sums these individual counts for each word and emits a single key/value pair containing the word followed by the sum of its occurrences.
hdinsight-use-mapreduce.md:* A MapReduce program. In this tutorial, you will use the word counting sample that comes with HDInsight clusters so you don't need to write your own. It is located on */example/jars/hadoop-examples.jar*. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-use-mapreduce.md:1.	Open **Windows Azure PowerShell**. For instructions of opening Windows Azure PowerShell console window, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-mapreduce.md:3. Set the two variables in the following commands, and then run them:
hdinsight-use-mapreduce.md:4. Run the following command and provide your Windows azure account information:
hdinsight-use-mapreduce.md:5. Run the following commands to create a MapReduce job definition:
hdinsight-use-mapreduce.md:	The hadoop-examples.jar file comes with the HDInsight cluster distribution. There are two arguments for the MapReduce job. The first one is the source file name, and the second is the output file path. The source file comes with the HDInsight cluster distribution, and the output file path will be created at the run-time.
hdinsight-use-mapreduce.md:6. Run the following command to submit the MapReduce job:
hdinsight-use-mapreduce.md:	In addition to the MapReduce job definition, you also provide the HDInsight cluster name where you want to run the MapReduce job, and the credentials. The Start-AzureHDInsightJob is an asynchronized call. To check the completion of the job, use the *Wait-AzureHDInsightJob* cmdlet.
hdinsight-use-mapreduce.md:7. Run the following command to check the completion of the MapReduce job:
hdinsight-use-mapreduce.md:8. Run the following command to check any errors with running the MapReduce job:	
hdinsight-use-mapreduce.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $wordCountJob.JobId -StandardError 
hdinsight-use-mapreduce.md:2. Run the following command to change directory to c:\ root:
hdinsight-use-mapreduce.md:2. Set the three variables in the following commands, and then run them:
hdinsight-use-mapreduce.md:3. Run the following commands to create a Windows Azure storage context object:
hdinsight-use-mapreduce.md:	The *Select-AzureSubscription* is used to set the current subscription in case you have multiple subscriptions, and the default subscription is not the one to use. 
hdinsight-use-mapreduce.md:4. Run the following command to download the MapReduce job output from the Blob container to the workstation:
hdinsight-use-mapreduce.md:5. Run the following command to print the MapReduce job output file:
hdinsight-use-mapreduce.md:	The MapReduce job produces a file named *part-r-00000* with the words and the counts.  The script uses the findstr command to list all of the words that contains *"there"*.
hdinsight-use-mapreduce.md:While MapReduce provides powerful diagnostic abilities, it can be a bit challenging to master. Other languages such as Pig and Hive provide an easier way to work with data stored in HDInsight. To learn more, see the following articles:
hdinsight-use-mapreduce.md:* [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce]
hdinsight-use-mapreduce.md:* [Develop C# Hadoop streaming MapReduce programs for HDInsight][hdinsight-develop-streaming]
hdinsight-use-mapreduce.md:[hdinsight-develop-mapreduce]: /en-us/documentation/articles/hdinsight-develop-deploy-java-mapreduce/
hdinsight-use-mapreduce.md:[hdinsight-develop-streaming]: /en-us/documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs/
hdinsight-use-mapreduce.md:[powershell-install-configure]: /en-us/manage/install-and-configure-windows-powershell/
hdinsight-use-pig.md:<properties linkid="manage-services-hdinsight-howto-pig" urlDisplayName="Use Pig with HDInsight" pageTitle="Use Pig with HDInsight | Windows Azure" metaKeywords="" description="Learn how to use Pig with HDInsight. Write Pig Latin statements to analyze an application log file, and run queries on the data to generate output for analysis." metaCanonical="" services="hdinsight" documentationCenter="" title="Use Pig with HDInsight" authors=""  solutions="" writer="jgao" manager="paulettm" editor="cgronlun"  />
hdinsight-use-pig.md:[Apache *Pig*][apachepig-home] provides a scripting language to execute *MapReduce* jobs as an alternative to writing Java code. In this tutorial, you will use PowerShell to run some Pig Latin statements to analyze an Apache log4j log file, and run various queries on the data to generate output. This tutorial demonstrates the advantages of Pig, and how it can be used to simplify MapReduce jobs. 
hdinsight-use-pig.md:For more information on Pig Latin, see [Pig Latin Reference Manual 1][piglatin-manual-1] and [Pig Latin Reference Manual 2][piglatin-manual-2].
hdinsight-use-pig.md:* Install and configure Windows Azure PowerShell. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-pig.md:* [Understand Pig Latin](#understand)
hdinsight-use-pig.md:Databases are great for small sets of data and low latency queries. However, when it comes to big data and large data sets in terabytes, traditional SQL databases are not the ideal solution. As database load increases and performance degrades, historically, database administrators have had to buy bigger hardware. 
hdinsight-use-pig.md:Generally, all applications save errors, exceptions and other coded issues in a log file, so administrators can review the problems, or generate certain metrics from the log file data. These log files usually get quite large in size, containing a wealth of data that must be processed and mined. 
hdinsight-use-pig.md:Log files are a good example of big data. Working with big data is difficult using relational databases and statistics/visualization packages. Due to the large amounts of data and the computation of this data, parallel software running on tens, hundreds, or even thousands of servers is often required to compute this data in a reasonable time. Hadoop provides a MapReduce framework for writing applications that processes large amounts of structured and unstructured data in parallel across large clusters of machines in a very reliable and fault-tolerant manner.
hdinsight-use-pig.md:Using Pig reduces the time needed to write mapper and reducer programs. This means that no Java is required, and there is no need for boilerplate code. You also have the flexibility to combine Java code with Pig. Many complex algorithms can be written in less than five lines of human-readable Pig code.
hdinsight-use-pig.md:The visual representation of what you will accomplish in this article is shown in the following two figures. These figures show a representative sample of the dataset to illustrate the flow and transformation of the data as you run through the lines of Pig code in the script. The first figure shows a sample of the log4j file:
hdinsight-use-pig.md:replace *mycontainer* with the container name, and *mystorage* with the Blob storage account name. 
hdinsight-use-pig.md:##<a id="understand"></a> Understand Pig Latin
hdinsight-use-pig.md:In this session, you will review some Pig Latin statements, and the results after running the statements. In the next session, you will run PowerShell to execute the Pig statements.
hdinsight-use-pig.md:1. Load data from the file system, and then display the results 
hdinsight-use-pig.md:This section provides instructions for using PowerShell cmdlets. Before you go through this section, you must first setup the local environment, and configure the connection to Windows Azure. For details, see [Get started with Windows Azure HDInsight][hdinsight-getting-started] and [Administer HDInsight using PowerShell][hdinsight-admin-powershell].
hdinsight-use-pig.md:1. Open a Windows Azure PowerShell console windows. For instructions, see [Install and configure Windows Azure PowerShell][powershell-install-configure].
hdinsight-use-pig.md:2. Set the variable in the following script, and run it:
hdinsight-use-pig.md:		# Print the standard error and the standard output of the Pig job.
hdinsight-use-pig.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $pigJob.JobId -StandardOutput
hdinsight-use-pig.md:[hdinsight-configure-powershell]: /en-us/manage/services/hdinsight/install-and-configure-powershell-for-hdinsight/ 
