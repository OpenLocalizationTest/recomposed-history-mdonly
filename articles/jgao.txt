hdinsight-administer-use-powershell.md:The HDInsight cluster distribution comes with some MapReduce samples. One of the samples is for counting word frequencies in source files.
hdinsight-administer-use-powershell.md:The following PowerShell script submits the word count sample job: 
hdinsight-administer-use-powershell.md:The HDInsight cluster distribution comes with a sample Hive table called *hivesampletable*. You can use a HiveQL "show tables;" to list the Hive tables on a cluster.
hdinsight-administer-use-powershell.md:		SELECT * FROM hivesampletable 
hdinsight-administer-use-powershell.md:The Hive job will first show the Hive tables created on the cluster, and the data returned from the hivesampletable.
hdinsight-analyze-flight-delay-data.md:- The CREATE EXTERNAL TABLE command doesn't allow any folders in the LOCATION. This is the reason why the tutorial makes a copy of the sample.log file.
hdinsight-analyze-flight-delay-data.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "export --connect $sqlDatabaseConnectionString --table $sqlDatabaseTableName --export-dir $exportDir --fields-terminated-by \001 "
hdinsight-analyze-flight-delay-data.md:		$sqoopJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $sqoopDef #-Debug -Verbose
hdinsight-analyze-flight-delay-data.md:		Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob
hdinsight-analyze-flight-delay-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-analyze-flight-delay-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-analyze-flight-delay-data.md:* [Use Sqoop with HDInsight][hdinsight-sqoop]
hdinsight-analyze-flight-delay-data.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-analyze-twitter-data.md:3. Enter **Name**, **Description**, **Website**. You can make up a URL for the Website field. The following table shows some sample values to use:
hdinsight-analyze-twitter-data.md:As a validation procedure, you can check the output file, **/tutorials/twitter/data/tweets.txt**, on your Windows Azure Blob storage using an Azure storage explorer, or Windows Azure PowerShell.  For a sample PowerShell script for listing files, see [Use Blob storage with HDInsight][hdinsight-storage-powershell]. 
hdinsight-analyze-twitter-data.md:As a validation procedure, you can check the output file, **/tutorials/twitter/twitter.hql**, on your Windows Azure Blob storage using an Azure storage explorer, or Windows Azure PowerShell.  For a sample PowerShell script for listing files, see [Use Blob storage with HDInsight][hdinsight-storage-powershell].  
hdinsight-analyze-twitter-data.md:After the analysis results has been placed on WASB, you can export the data to Windows Azure SQL database/SQL server, export the data to Excel using Power Query, or connect your application to the data using Hive ODBC driver.  For more information, see [Use Sqoop with HDInsight][hdinsight-sqoop] ,[Analyze flight delay data using HDInsight][hdinsight-analyze-flight-delay-data], [Connect Excel to HDInsight with Power Query][hdinsight-power-query], and [Connect Excel to HDInsight with the Microsoft Hive ODBC Driver][hdinsight-hive-odbc].
hdinsight-analyze-twitter-data.md:- [Use Sqoop with HDInsight][hdinsight-sqoop]
hdinsight-analyze-twitter-data.md:[hdinsight-sqoop]: ../hdinsight-sqoop/
hdinsight-connect-excel-hive-ODBC-driver.md:8. Select the table that you want to import, and then click **Next**. The *hivesampletable* is a sample hive table that comes with HDInsight clusters.  You can choose it if you haven't created one. For more information on run Hive queries and create Hive tables, see [Use Hive with HDInsight][hdinsight-hive].
hdinsight-connect-excel-hive-ODBC-driver.md:- [Use Sqoop with HDInsight][hdinsight-sqoop]
hdinsight-connect-excel-hive-ODBC-driver.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-connect-excel-power-query.md:6. Locate **HiveSampleData.txt** in the **Name** column (the folder path is **../hive/warehouse/hivesampletable/**), and then click **Binary** on the left of HiveSampleData.txt.
hdinsight-develop-deploy-java-mapreduce.md:After the job is completed, you have the options to export the data to SQL Server or Windows Azure SQL database using [Sqoop][hdinsight-sqoop], or to export the data to Excel.  
hdinsight-develop-deploy-java-mapreduce.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-emulator-release-notes.md:* Bug fix in the command used to run Pig samples in the RunSamples.ps1 script installed in the GettingStarted folder. 
hdinsight-emulator-release-notes.md:		c:\Hadoop\sqoop-1.4.2\bin>sqoop version 
hdinsight-get-started-30.md:* [Run a WordCount MapReduce program](#sample)
hdinsight-get-started-30.md:##<a name="sample"></a>Run a WordCount MapReduce job
hdinsight-get-started-30.md:* A MapReduce program. In this tutorial, you will use the WordCount sample that comes with the HDInsight cluster distribution so you don't need to write your own. It is located on */example/jars/hadoop-mapreduce-examples.jar*. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-get-started-30.md:**To run the WordCount sample**
hdinsight-get-started-30.md:The Power Query add-in for Excel can be used to export output from HDInsight into Excel where Microsoft Business Intelligence (BI) tools can be used to further process or display the results. When you created an HDInsight cluster, a default container with the same name as the cluster was created in the storage account associated with it when it was created. This is automatically populated with a set of files. One of these files is a sample Hive table. In this section we will show how to import the data contained in this table into Excel for viewing and additional processing.
hdinsight-get-started-emulator.md:* [Run the word count sample](#runwordcount)
hdinsight-get-started-emulator.md:* [Run the getting started samples](#rungetstartedsamples)
hdinsight-get-started-emulator.md:##<a name="rungetstartedsamples"></a> Run the get started samples
hdinsight-get-started-emulator.md:The HDInsight Emulator installation provides some samples to get new users started learning Apache Hadoop-based Services on Windows quickly. These samples covers some tasks that are typically needed when processing a big data set. Going through the samples can familiarize yourself with concepts associated with the MapReduce programming model and its ecosystem.
hdinsight-get-started-emulator.md:The samples are organized around the processing IIS W3C log data scenarios. A data generation tool is provided to create and import the data sets in various sizes to HDFS or WASB (Windows Azure Blob storage). See [Use Windows Azure Blob storage for HDInsight][hdinsight-blob-store] for more information). MapReduce, Pig or Hive jobs may then be run on the pages of data generated by the PowerShell script. Note that the Pig and Hive scripts used both compile to MapReduce programs. Users may run a series of jobs to observe, for themselves, the effects of using these different technologies and the effects of the size of the data on the execution of the processing tasks. 
hdinsight-get-started-emulator.md:- [Load sample w3c log data](#loaddata)
hdinsight-get-started-emulator.md:- [Rebuild the samples](#rebuild)
hdinsight-get-started-emulator.md:These samples and their documentation do not provide an in-depth study or full implementation of the key Hadoop technologies. The cluster used has only a single node and so the effect of adding more nodes cannot, with this release, be observed. 
hdinsight-get-started-emulator.md:###<a name="loaddata"></a>Load sample W3c log data
hdinsight-get-started-emulator.md:**To import sample w3c log data:**
hdinsight-get-started-emulator.md:### <a name="rebuild"></a>Rebuild the samples
hdinsight-get-started-emulator.md:The samples currently contain all of the required binaries, so building is not required. If you'd like to make changes to the Java or .NET samples, you can rebuild them using either msbuild, or the included PowerShell script.
hdinsight-get-started-emulator.md:**To rebuilt the samples**
hdinsight-get-started-emulator.md:		powershell -F buildsamples.ps1
hdinsight-get-started-emulator.md:Here is a sample for submitting a Hadoop job:
hdinsight-get-started.md:* [Run a WordCount MapReduce program](#sample)
hdinsight-get-started.md:##<a name="sample"></a>Run a WordCount MapReduce job
hdinsight-get-started.md:* A MapReduce program. In this tutorial, you will use the WordCount sample that comes with the HDInsight cluster distribution so you don't need to write your own. It is located on */example/jars/hadoop-examples.jar*. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-get-started.md:**To run the WordCount sample**
hdinsight-get-started.md:The Power Query add-in for Excel can be used to export output from HDInsight into Excel where Microsoft Business Intelligence (BI) tools can be used to further process or display the results. When you created an HDInsight cluster, a default container with the same name as the cluster was created in the storage account associated with it when it was created. This is automatically populated with a set of files. One of these files is a sample Hive table. In this section we will show how to import the data contained in this table into Excel for viewing and additional processing.
hdinsight-hadoop-recommendation-engine.md:This tutorial assumes that you have gotten setup with Windows Azure and the HDinsight preview and that you have created an HDInsight cluster on which you can run a sample. If you have not done this already, consult the [Get Started with the Windows Azure HDInsight](/en-us/manage/services/hdinsight/get-started-hdinsight/) tutorial for instructions on how to satisfy these prerequisites.
hdinsight-hadoop-recommendation-engine.md:This example deals with the way in which users express a preference for certain songs. The assumption is that the number of times a user listens to a song provides a measure of that user's preference for that song. Patterns detected in the preference data can be used to predict future user preferences based on some of their expressed musical preferences. You can view a sample of this dataset in the **Description** section of the [Echo Nest Taste Profile Subset](http://labrosa.ee.columbia.edu/millionsong/tasteprofile) web page:
hdinsight-hadoop-recommendation-engine.md:5. Download the sample data from [this link](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip). Once downloaded, open **train\_triplets.txt.zip** and extract **train\_triplets.txt**.
hdinsight-interactive-console.md:In this section, you use the JavaScript console to run the WordCount sample that ships with the HDInsight Service. The JavaScript query run here uses the fluent API layered on Pig that is provided by the Interactive Console.  The text file analyzed here is the Project Gutenberg eBook edition of *The Notebooks of Leonardo Da Vinci*. A filter is specified so that the results of the MapReduce job contains only the ten most frequently occurring words. 
hdinsight-interactive-console.md:	![HDI.Tiles.Samples][hdi-tiles-samples]
hdinsight-interactive-console.md:2. Enter the following command to create a two column table named _DaVinciWordCountTable_ from the WordCount sample output that was saved in the "DaVinciTop10Words" folder:
hdinsight-interactive-console.md:[hdi-tiles-samples]: ./media/hdinsight-interactive-console/HDI.TileSamples.PNG
hdinsight-introduction.md:Sqoop is tool that transfers bulk data between Hadoop and relational databases such a SQL, or other structured data stores, as efficiently as possible. Use Sqoop to import data from external structured data stores into the HDFS or related systems like Hive. Sqoop can also extract data from Hadoop and export the extracted data to external relational databases, enterprise data warehouses, or any other structured data store type. For additional information, see the  [Apache Sqoop](http://sqoop.apache.org/) Web site.
hdinsight-introduction.md:* [Run the HDInsight samples](/en-us/manage/services/hdinsight/howto-run-samples/): A tutorial on how the run the samples that ship with HDInsight.
hdinsight-introduction.md:* [Adventure Works for SQL Database](http://msftdbprodsamples.codeplex.com/releases/view/37304): Download page for SQL Database sample database.	
hdinsight-run-samples.md:<properties linkid="manage-services-hdinsight-howto-run-samples" urlDisplayName="Run HDInsight Samples" pageTitle="Run the HDInsight Samples | Windows Azure" metaKeywords="" description="Get started using the Windows Azure HDInsight service with the samples provided. Use PowerShell scripts that run MapReduce programs on data clusters." metaCanonical="" services="hdinsight" documentationCenter="" title="Run the HDInsight samples" authors=""  solutions="" writer="sburgess" manager="paulettm" editor="mollybos"  />
hdinsight-run-samples.md:#Run the HDInsight samples
hdinsight-run-samples.md:A set of samples are provided to help you get started with Windows Azure HDInsight. These samples are made available on each of the HDInsight clusters that you create. Running these samples will familiarize you with Windows Azure PowerShell HDInsight cmdlets.
hdinsight-run-samples.md:**What these samples are**
hdinsight-run-samples.md:<p>These samples are intended to get you up to speed quickly on how to deploy Hadoop jobs and to provide you an extensible testing bed to work with the concepts and scripting procedures used by the service. They provide you with examples of common tasks such as creating and importing data sets of various sizes, running jobs and composing jobs sequentially, and examining the results of your jobs. The data sets used can be varied in size, allowing you to observe the effects that data sets of various size has on job performance.</p>
hdinsight-run-samples.md:## The samples ##
hdinsight-run-samples.md:HDInsight ships with the following samples.
hdinsight-run-samples.md:## How to run the samples ##
hdinsight-run-samples.md:The samples can be run using Windows Azure PowerShell. Instructions on how to do this are provided for each of the samples on the pages linked above.
hdinsight-run-samples.md:From this article and the articles on each of the samples, you learned how to run the samples included with the HDInsight clusters using Windows Azure PowerShell. For tutorials on using Pig, Hive, and MapReduce with HDInsight, see the following topics:
hdinsight-run-samples.md:[pi-estimator]: /en-us/manage/services/hdinsight/howto-run-samples/sample-pi-estimator/
hdinsight-run-samples.md:[10gb-graysort]: /en-us/manage/services/hdinsight/howto-run-samples/sample-10gb-graysort/
hdinsight-run-samples.md:[wordcount]: /en-us/manage/services/hdinsight/howto-run-samples/sample-wordcount/
hdinsight-run-samples.md:[cs-streaming]: /en-us/manage/services/hdinsight/howto-run-samples/sample-csharp-streaming/
hdinsight-sample-10gb-graysort.md:<properties linkid="manage-services-hdinsight-sample-10gb-graysort" urlDisplayName="HDInsight Samples" pageTitle="The 10GB GraySort sample | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure" description="Learn how to run a general purpose GraySort with Windows Azure HDInsight using Windows Azure PowerShell.." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="The 10GB GraySort sample" />
hdinsight-sample-10gb-graysort.md:# The 10GB GraySort sample
hdinsight-sample-10gb-graysort.md:This sample topic shows how to run a general purpose GraySort with Windows Azure HDInsight using Windows Azure PowerShell. A GraySort is a benchmark sort whose metric is the sort rate (TB/minute) that is achieved while sorting very large amounts of data, usually a 100 TB minimum. 
hdinsight-sample-10gb-graysort.md:This sample uses a modest 10 GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578 TB/min (100 TB in 173 minutes). For more information on this and other sorting benchmarks, see the [Sortbenchmark](http://sortbenchmark.org/)   site.
hdinsight-sample-10gb-graysort.md:This sample uses three sets of MapReduce programs:	
hdinsight-sample-10gb-graysort.md:2. **TeraSort** samples the input data and uses MapReduce to sort the data into a total order. TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] <= key < sample[i] are sent to reduce i. This guarantees that the output of reduce i are all less than the output of reduce i+1.
hdinsight-sample-10gb-graysort.md:1. [Run the sample with Windows Azure PowerShell](#run-sample)	
hdinsight-sample-10gb-graysort.md:<h2><a id="run-sample"></a>Run the sample with Windows Azure PowerShell</h2>
hdinsight-sample-10gb-graysort.md:Three tasks are required by the sample, each corresponding to one of the MapReduce programs decribed in the introduction:	
hdinsight-sample-10gb-graysort.md:	 * Generates the sampled split points, launches the job, 	
hdinsight-sample-10gb-graysort.md:This sample has demonstrated how to run a series of MapReduce jobs using Windows Azure HDInsight, where the data output for one job becomes the input for the next job in the series.
hdinsight-sample-10gb-graysort.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-10gb-graysort.md:[pi-estimator]: /en-us/manage/services/hdinsight/howto-run-samples/sample-pi-estimator/
hdinsight-sample-10gb-graysort.md:[wordcount]: /en-us/manage/services/hdinsight/howto-run-samples/sample-wordcount/
hdinsight-sample-10gb-graysort.md:[cs-streaming]: /en-us/manage/services/hdinsight/howto-run-samples/sample-csharp-streaming/
hdinsight-sample-csharp-streaming.md:<properties linkid="manage-services-hdinsight-sample-csharp-streaming" urlDisplayName="HDInsight Samples" pageTitle="The HDInsight C# streaming wordcount sample | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure" description="Learn how to run a sample TBD." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="The HDInsight C# streaming wordcount sample" />
hdinsight-sample-csharp-streaming.md:# The HDInsight C# streaming wordcount sample
hdinsight-sample-csharp-streaming.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-csharp-streaming.md:1. [Run the sample with Windows Azure PowerShell](#run-sample)	
hdinsight-sample-csharp-streaming.md:<h2><a id="run-sample"></a>Run the sample with Windows Azure PowerShell</h2>
hdinsight-sample-csharp-streaming.md:	Note that the output files of a MapReduce job are immutable. So if you rerun this sample you will need to change the name of the output file.
hdinsight-sample-csharp-streaming.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-csharp-streaming.md:[pi-estimator]: /en-us/manage/services/hdinsight/howto-run-samples/sample-pi-estimator/
hdinsight-sample-csharp-streaming.md:[wordcount]: /en-us/manage/services/hdinsight/howto-run-samples/sample-wordcount/
hdinsight-sample-csharp-streaming.md:[10gb-graysort]: /en-us/manage/services/hdinsight/howto-run-samples/sample-10gb-graysort/
hdinsight-sample-pi-estimator.md:<properties linkid="manage-services-hdinsight-sample-pi-estimator" urlDisplayName="HDInsight Samples" pageTitle="The HDInsight Pi estimator sample | Windows Azure" metaKeywords="hdinsight, hdinsight sample, mapreduce" description="Learn how to run a simple MapReduce sample on HDInsight." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="The HDInsight Pi estimator sample" />
hdinsight-sample-pi-estimator.md:# The HDInsight Pi estimator sample
hdinsight-sample-pi-estimator.md:The program uses a statistical (quasi-Monte Carlo) method to estimate the value of Pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, Pi/4. The value of Pi can be estimated from the value of 4R where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.
hdinsight-sample-pi-estimator.md:The script provided for this sample submits a Hadoop JAR job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values. These parameter values can be changed to improve the estimated value of Pi. For reference, the first 10 decimal places of Pi are 3.1415926535.
hdinsight-sample-pi-estimator.md:The other samples that are available to help you get up to speed using HDInsight to run MapReduce jobs are listed on [Running the HDInsight Samples][run-samples] along with links to instructions on how to run them.
hdinsight-sample-pi-estimator.md:This topic shows you how to run the sample, presents the Java code for the Pi Estimator MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-pi-estimator.md:1. [Run the sample with Windows Azure PowerShell](#run-sample)	
hdinsight-sample-pi-estimator.md:<h2><a id="run-sample"></a>Run the sample with Windows Azure PowerShell</h2>
hdinsight-sample-pi-estimator.md:	The first argument indicates how many maps to create (default is 16). The second argument indicates how many samples are generated per map (10 million by default). So this program uses 10*10 million = 160 million random points to make its estimate of Pi. 
hdinsight-sample-pi-estimator.md:	//Halton sequence is used to generate sample points for Pi estimation.	 
hdinsight-sample-pi-estimator.md:	//@param offset samples starting from the (offset+1)th sample.	
hdinsight-sample-pi-estimator.md:	//@param size the number of samples for this map	
hdinsight-sample-pi-estimator.md: 	reporter.setStatus("Generated " + i + " samples.");
hdinsight-sample-pi-estimator.md:For tutorials running other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-pi-estimator.md:[run-samples]: /en-us/manage/services/hdinsight/howto-run-samples
hdinsight-sample-pi-estimator.md:[10gb-graysort]: /en-us/manage/services/hdinsight/howto-run-samples/sample-10gb-graysort/
hdinsight-sample-pi-estimator.md:[wordcount]: /en-us/manage/services/hdinsight/howto-run-samples/sample-wordcount/
hdinsight-sample-pi-estimator.md:[cs-streaming]: /en-us/manage/services/hdinsight/howto-run-samples/sample-csharp-streaming/
hdinsight-sample-sqoop-import-export.md:<properties linkid="manage-services-hdinsight-sample-sqoop-import-export" urlDisplayName="HDInsight Samples" pageTitle="Sqoop Import-Export sample | Windows Azure" metaKeywords="hdinsight, hdinsight administration, hdinsight administration azure" description="Learn how to run Sqoop import and export on HDInsight." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="Sqoop Import-Export sample" />
hdinsight-sample-sqoop-import-export.md:# Sqoop Import-Export sample
hdinsight-sample-sqoop-import-export.md:This sample topic shows how to use Apache Sqoop to import data from a SQL database on Windows Azure to an Hadoop on Azure HDFS cluster.
hdinsight-sample-sqoop-import-export.md:Sqoop is an open source software product of Cloudera, Inc. Software development for Sqoop moved in 2011 from gitHub to the [Apache Sqoop](http://sqoop.apache.org/) site.
hdinsight-sample-sqoop-import-export.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-sqoop-import-export.md:	The script creates the database, installs the schema, and populates the database with sample data.
hdinsight-sample-sqoop-import-export.md:7. Double-click on the Hadoop Command Shell icon in the upper left hand of the Desktop to open it. Navigate to the "c:\Apps\dist\sqoop\bin" directory and run the following command:
hdinsight-sample-sqoop-import-export.md:	`sqoop import --connect "jdbc:sqlserver://[serverName].database.windows.net;username=[userName]@[serverName];password=[password];database=AdventureWorks2012" --table Sales.SalesOrderDetail --target-dir /data/lineitemData -m 1`
hdinsight-sample-sqoop-import-export.md:	The sqoop command is:
hdinsight-sample-sqoop-import-export.md:	`sqoop import --connect "jdbc:sqlserver://wq6xlbyoq0.database.windows.net;username=HadoopOnAzureSqoopAdmin@wq6xlbyoq0;password=Pa$$w0rd;;database=AdventureWorks2012" --table Sales.SalesOrderDetail --target-dir /data/lineitemData -m 1`
hdinsight-sample-sqoop-import-export.md:For tutorials runnng other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-sqoop-import-export.md:[wordcount]: /en-us/manage/services/hdinsight/howto-run-samples/sample-wordcount/
hdinsight-sample-sqoop-import-export.md:[pi-estimator]: /en-us/manage/services/hdinsight/howto-run-samples/sample-pi-estimator/
hdinsight-sample-sqoop-import-export.md:[cs-streaming]: /en-us/manage/services/hdinsight/howto-run-samples/sample-csharp-streaming/
hdinsight-sample-sqoop-import-export.md:[10gb-graysort]: /en-us/manage/services/hdinsight/howto-run-samples/sample-10gb-graysort/
hdinsight-sample-wordcount.md:<properties linkid="manage-services-hdinsight-sample-wordcount" urlDisplayName="HDInsight Samples" pageTitle="The HDInsight WordCount sample | Windows Azure" metaKeywords="hdinsight, hdinsight sample, mapreduce" description="Learn how to run a simple MapReduce sample on HDInsight." umbracoNaviHide="0" disqusComments="1" writer="bradsev" editor="cgronlun" manager="paulettm" title="The HDInsight WordCount sample"/>
hdinsight-sample-wordcount.md:#The HDInsight WordCount sample
hdinsight-sample-wordcount.md:This sample topic shows how to run a MapReduce program that counts word occurrences in a text file with Windows Azure HDInsight using Windows Azure PowerShell. The WordCount MapReduce program is written in Java and runs on an HDInsight cluster. The text file analyzed here is the Project Gutenberg eBook edition of The Notebooks of Leonardo Da Vinci. 
hdinsight-sample-wordcount.md:This topic shows you how to run the sample, presents the Java code for the MapReduce program, summarizes what you have learned, and outlines some next steps. It has the following sections.
hdinsight-sample-wordcount.md:1. [Run the sample using Windows Azure PowerShell](#run-sample)	
hdinsight-sample-wordcount.md:<h2><a id="run-sample"></a>Run the sample using Windows Azure PowerShell</h2> 
hdinsight-sample-wordcount.md:![HDI.Sample.WordCount.Output][image-hdi-sample-wordcount-output]
hdinsight-sample-wordcount.md:Note that the output files of a MapReduce job are immutable. So if you rerun this sample you will need to change the name of the output file.
hdinsight-sample-wordcount.md:For tutorials runnng other samples and providing instructions on using Pig, Hive, and MapReduce jobs on Windows Azure HDInsight with Windows Azure PowerShell, see the following topics:
hdinsight-sample-wordcount.md:[10gb-graysort]: /en-us/manage/services/hdinsight/howto-run-samples/sample-10gb-graysort/
hdinsight-sample-wordcount.md:[pi-estimator]: /en-us/manage/services/hdinsight/howto-run-samples/sample-pi-estimator/
hdinsight-sample-wordcount.md:[cs-streaming]: /en-us/manage/services/hdinsight/howto-run-samples/sample-csharp-streaming/
hdinsight-sample-wordcount.md:[image-hdi-sample-wordcount-output]: ./media/hdinsight-sample-wordcount/HDI.Sample.WordCount.Output.png
hdinsight-submit-hadoop-jobs-programmatically.md:* [Submit Sqoop jobs using PowerShell](#sqoop-powershell)
hdinsight-submit-hadoop-jobs-programmatically.md:Hadoop MapReduce is a software framework for writing applications which process vast amounts of data. HDInsight clusters come with a jar file, located at *\example\jars\hadoop-examples.jar*, which contains several MapReduce examples. This file has been renamed to hadoop-mapreduce-examples.jar on version 3.0 HDInsight clusters. One of the examples is for counting word frequencies in source files. In this session, you will learn how to use PowerShell from a workstation to run the word count sample. For more information on developing and running MapReduce jobs, see [Use MapReduce with HDInsight][hdinsight-mapreduce].
hdinsight-submit-hadoop-jobs-programmatically.md:HDInsight clusters come with a sample Hive table called *hivesampletable*. In this session, you will use PowerShell to run a Hive job for listing some data from the Hive table. 
hdinsight-submit-hadoop-jobs-programmatically.md:		$querystring = "SELECT * FROM hivesampletable WHERE Country='United Kingdom';"
hdinsight-submit-hadoop-jobs-programmatically.md:See [Use Sqoop with HDInsight][hdinsight-sqoop].
hdinsight-submit-hadoop-jobs-programmatically.md:The HDInsight .NET SDK provides .NET client libraries that makes it easier to work with HDInsight clusters from .NET. HDInsight clusters come with a jar file, located at *\example\jars\hadoop-examples.jar*, which contains several MapReduce examples. One of the examples is for counting word frequencies in source files. In this session, you will learn how to create a .NET application to run the word count sample. For more information on developing and running MapReduce jobs, see [Use MapReduce with HDInsight][hdinsight-mapreduce].
hdinsight-submit-hadoop-jobs-programmatically.md:HDInsight clusters come with a sample Hive table called *hivesampletable*. In this session, you will create a .NET application to run a Hive job for listing the Hive tables created on HDInsight cluster. For a more information on using Hive, see [Use Hive with HDInsight][hdinsight-hive].
hdinsight-submit-hadoop-jobs-programmatically.md:	hivesampletable
hdinsight-submit-hadoop-jobs-programmatically.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-upload-data.md:* [Import data from Windows Azure SQL Database to Blob storage using Sqoop](#sqoop)
hdinsight-upload-data.md:12. The following sample demonstrates how to copy the davinci.txt file from the local file system on the HDInsight head node to the /example/data directory.
hdinsight-upload-data.md:##<a id="sqoop"></a> Import data to HDFS from SQL Database/SQL Server using Sqoop
hdinsight-upload-data.md:Sqoop is a tool designed to transfer data between Hadoop and relational databases. You can use it to import data from a relational database management system (RDBMS) such as SQL or MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop with MapReduce or Hive, and then export the data back into a RDBMS. For more information, see [Sqoop User Guide][apache-sqoop-guide].
hdinsight-upload-data.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "import --connect jdbc:sqlserver://$sqlDatabaseServerName.database.windows.net;user=$sqlDatabaseUserName@$sqlDatabaseServerName;password=$sqlDatabasePassword;database=$sqlDatabaseDatabaseName --table $tableName --target-dir $hdfsOutputDir -m 1" 
hdinsight-upload-data.md:		$sqoopJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $sqoopDef #-Debug -Verbose
hdinsight-upload-data.md:		Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob
hdinsight-upload-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-upload-data.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-upload-data.md:3. Paste the script into the Windows Azure PowerShell console window to run it. See [Get started with HDInsight][hdinsight-getting-started] for a PowerShell sample for retrieving the data file from Windows Azure Blob storage.
hdinsight-upload-data.md:For more information on using Sqoop, see [Use Sqoop with HDInsight][hdinsight-sqoop].
hdinsight-upload-data.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-upload-data.md:[apache-sqoop-guide]: http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html
hdinsight-use-blob-storage.md:[Windows Azure PowerShell][powershell-install] can be used to create Blob containers. The following is a sample PowerShell script:
hdinsight-use-blob-storage.md:**PowerShell sample for uploading a file**
hdinsight-use-blob-storage.md:**PowerShell sample for downloading a file**
hdinsight-use-blob-storage.md:	$blob = "example/data/sample.log" # The name of the blob to be downloaded.
hdinsight-use-blob-storage.md:**PowerShell sample for deleting a file**
hdinsight-use-blob-storage.md:	$blob = "example/data/sample.log" # The name of the blob to be downloaded.
hdinsight-use-blob-storage.md:**PowerShell sample for listing files in a folder**
hdinsight-use-hive.md:In this article, you use a log4j sample file distributed with the HDInsight cluster that is stored in *\example\data\sample.log*. Each log inside the file consists of a line of fields that contains a `[LOG LEVEL]` field to show the type and the severity. For example:
hdinsight-use-hive.md:	wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log
hdinsight-use-hive.md:	wasb:///example/data/sample.log
hdinsight-use-hive.md:	/example/data/sample.log
hdinsight-use-hive.md:In the last section, you uploaded a log4j file called sample.log to the default file system container.  In this section, you will run HiveQL to create a hive table, load data to the hive table, and then query the data to find out how many error logs there were.
hdinsight-use-hive.md:		               "LOAD DATA INPATH 'wasb://$containerName@$storageAccountName.blob.core.windows.net/example/data/sample.log' OVERWRITE INTO TABLE log4jLogs;" +
hdinsight-use-hive.md:	The LOAD DATA HiveQL command will result in moving the data file to the \hive\warehouse\ folder.  The DROP TABLE command will delete the table and the data file.  If you use the internal table option and want to run the script again, you must upload the sample.log file again. If you want to keep the data file, you must use the CREATE EXTERNAL TABLE command as shown in the script.
hdinsight-use-hive.md:		    SELECT * FROM hivesampletable
hdinsight-use-hive.md:	For longer HiveQL queries, it is recommended to use PowerShell Here-Strings or HiveQL script file. The following samples shows how to use the Invoke-Hive cmdlet to run a HiveQL script file.  The HiveQL script file must be uploaded to WASB.
hdinsight-use-mapreduce.md:2. [Run the Sample with Windows Azure PowerShell](#run-sample)	
hdinsight-use-mapreduce.md:* A MapReduce program. In this tutorial, you will use the word counting sample that comes with HDInsight clusters so you don't need to write your own. It is located on */example/jars/hadoop-examples.jar*. The file name is *hadoop-mapreduce-examples.jar* on version 3.0 HDInsight clusters. For instructions on writing your own MapReduce job, see [Develop Java MapReduce programs for HDInsight][hdinsight-develop-MapReduce].
hdinsight-use-mapreduce.md:##<a id="run-sample"></a>Run the Sample with Windows Azure PowerShell
hdinsight-use-mapreduce.md:Note that the output files of a MapReduce job are immutable. So if you rerun this sample you will need to change the name of the output file.
hdinsight-use-mapreduce.md:* [Run the HDInsight Samples][hdinsight-samples]
hdinsight-use-mapreduce.md:[hdinsight-samples]: /en-us/documentation/articles/hdinsight-run-samples/
hdinsight-use-oozie.md:2.  A Sqoop action exports the HiveQL action output to a table on Windows Azure SQL database. For more information on Sqoop, see [Use Sqoop with HDInsight][hdinsight-sqoop].
hdinsight-use-oozie.md:		        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
hdinsight-use-oozie.md:		        </sqoop>
hdinsight-use-oozie.md:- Copy the data file (/example/data/sample.log) to wasb:///tutorials/useoozie/data/sample.log. 
hdinsight-use-oozie.md:- The CREATE EXTERNAL TABLE command doesn't allow any sub-folders under the folder specified in the LOCATION clause. This is the reason why the tutorial makes a copy of the sample.log file.
hdinsight-use-oozie.md:			Write-Host "Make a copy of the sample.log file ... " -ForegroundColor Green
hdinsight-use-oozie.md:			Start-CopyAzureStorageBlob -SrcContainer $containerName -SrcBlob "example/data/sample.log" -Context $destContext -DestContainer $containerName -destBlob "$destFolder/data/sample.log" -DestContext $destContext
hdinsight-use-oozie.md:		# make a copy of example/data/sample.log to example/data/log4j/sample.log
hdinsight-use-oozie.md:Here is a sample PowerShell script that you can use:
hdinsight-use-oozie.md:- [Use Sqoop with HDInsight][hdinsight-sqoop]
hdinsight-use-oozie.md:[hdinsight-sqoop]: ../hdinsight-use-sqoop/
hdinsight-use-pig.md:The visual representation of what you will accomplish in this article is shown in the following two figures. These figures show a representative sample of the dataset to illustrate the flow and transformation of the data as you run through the lines of Pig code in the script. The first figure shows a sample of the log4j file:
hdinsight-use-pig.md:![Whole File Sample][image-hdi-log4j-sample]
hdinsight-use-pig.md:HDInsight uses a Windows Azure Blob storage container as the default file system.  For more information, see [Use Windows Azure Blob Storage with HDInsight][hdinsight-storage]. In this article you will use a log4j sample file distributed with the HDInsight cluster stored in *\example\data\sample.log*. For information on uploading data, see [Upload data to HDInsight][hdinsight-upload-data].
hdinsight-use-pig.md:		wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log
hdinsight-use-pig.md:		wasb:///example/data/sample.log
hdinsight-use-pig.md:		/example/data/sample.log
hdinsight-use-pig.md:		LOGS = LOAD 'wasb:///example/data/sample.log';
hdinsight-use-pig.md:		$QueryString =  "LOGS = LOAD 'wasb:///example/data/sample.log';" +
hdinsight-use-pig.md:[image-hdi-log4j-sample]: ./media/hdinsight-use-pig/HDI.wholesamplefile.png
hdinsight-use-sqoop.md:<properties linkid="manage-services-hdinsight-use-sqoop" urlDisplayName="Use Sqoop with HDInsight Samples" pageTitle="Use Sqoop with HDInsight | Windows Azure" metaKeywords="" description="Learn how to use Windows Azure PowerShell from a workstation to run Sqoop import and export between an HDInsight cluster and a Windows Azure SQL database." umbracoNaviHide="0" disqusComments="1" writer="jgao" editor="cgronlun" manager="paulettm" title="Use Sqoop with HDInsight" />
hdinsight-use-sqoop.md:- [What is Sqoop?](#whatissqoop)
hdinsight-use-sqoop.md:## <a id="whatissqoop"></a> What is Sqoop?
hdinsight-use-sqoop.md:[Sqoop][sqoop-user-guide-1.4.4] is a tool designed to transfer data between Hadoop clusters and relational databases. You can use it to import data from a relational database management system (RDBMS) such as SQL or MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop with MapReduce or Hive, and then export the data back into an RDBMS. In this tutorial, you are using a SQL Database for your relational database.
hdinsight-use-sqoop.md:HDInsight cluster comes with some sample data. You will use the following two:
hdinsight-use-sqoop.md:- A log4j log file located at */example/data/sample.log*. The following logs are extracted from the file:
hdinsight-use-sqoop.md:- A Hive table named *hivesampletable* referencing the data file located at */hive/warehouse/hivesampletable*. The table contains some mobile device data. The Hive table schema is:
hdinsight-use-sqoop.md:You will first export both sample.log and hivesampletable to SQL database, and then import the table containing the mobile device data back to HDInsight using the following path:
hdinsight-use-sqoop.md:	/tutorials/usesqoop/importeddata
hdinsight-use-sqoop.md:When you provision an HDInsight cluster, a Windows Azure Storage account and a specific Blob storage container from that account is designated as the default file system, just like in HDFS. In addition to this storage account, you can add additional storage accounts from either the same Windows Azure subscription or different Windows Azure subscriptions during the provision process. For instructions on adding additional storage accounts, see [Provision HDInsight clusters][hdinsight-provision]. To simply the PowerShell script used in this tutorial, all of the files are stored in the default file system container, located at */tutorials/usesqoop*. By default this container has the same name as the HDInsight cluster name. 
hdinsight-use-sqoop.md:A file stored in the default file system container can be accessed from HDInsight using any of the following URIs (use sample.log as an example):
hdinsight-use-sqoop.md:	wasb://mycontainer@mystorageaccount.blob.core.windows.net/example/data/sample.log
hdinsight-use-sqoop.md:	wasb:///example/data/sample.log
hdinsight-use-sqoop.md:	/example/data/sample.log
hdinsight-use-sqoop.md:	example/data/sample.log
hdinsight-use-sqoop.md:You will create two SQL database tables used by Sqoop export later in the tutorial. You will also need to pre-process the sample.log files before it can be processed by Sqoop.
hdinsight-use-sqoop.md:In this tutorial, you will export a log4j log file (a delimited file) and a Hive table to SQL Database.  The delimited file is */example/data/sample.log*. Earlier in the tutorial, you see a few samples of log4j logs. In the log file, there are some empty lines and some other lines similar to:
hdinsight-use-sqoop.md:**To pre-process the sample.log file**
hdinsight-use-sqoop.md:		$sourceBlobName = "example/data/sample.log"
hdinsight-use-sqoop.md:		$destBlobName = "tutorials/usesqoop/data/sample.log"
hdinsight-use-sqoop.md:6. To examine the modified data file, you can use Windows Azure Management portal, or a Windows Azure Storage explorer tool, or Windows Azure PowerShell.  [Get started with HDInsight][hdinsight-get-started] has a code sample on using PowerShell to download a file and display the file content.
hdinsight-use-sqoop.md:		$exportDir_log4j = "/tutorials/usesqoop/data"
hdinsight-use-sqoop.md:	Notice $exportDir_log4j doesn't have the sample.log file file name specified. Sqoop will export the data from all of the files under that folder.
hdinsight-use-sqoop.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "export --connect $connectionString --table $tableName_log4j --export-dir $exportDir_log4j --input-fields-terminated-by \0x20 -m 1"
hdinsight-use-sqoop.md:		$sqoopJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $sqoopDef #-Debug -Verbose
hdinsight-use-sqoop.md:		Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-use-sqoop.md:	Notice the field delimiter is **\0x20**, which is space. The delimiter is defined in  the sample.log file pre-process PowerShell script. To find out about **-m 1**, see [Sqoop user guide][sqoop-user-guide-1.4.4].
hdinsight-use-sqoop.md:**To export the hivesampletable Hive table**
hdinsight-use-sqoop.md:		$exportDir_mobile = "/hive/warehouse/hivesampletable"
hdinsight-use-sqoop.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "export --connect $connectionString --table $tableName_mobile --export-dir $exportDir_mobile --fields-terminated-by \t -m 1"
hdinsight-use-sqoop.md:		$sqoopJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $sqoopDef #-Debug -Verbose
hdinsight-use-sqoop.md:		Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-use-sqoop.md:		$targetDir_mobile = "/tutorials/usesqoop/importeddata/"
hdinsight-use-sqoop.md:		$sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command "import --connect $connectionString --table $tableName_mobile --target-dir $targetDir_mobile --fields-terminated-by \t --lines-terminated-by \n -m 1"
hdinsight-use-sqoop.md:		$sqoopJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $sqoopDef #-Debug -Verbose
hdinsight-use-sqoop.md:		Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardError
hdinsight-use-sqoop.md:		Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $sqoopJob.JobId -StandardOutput
hdinsight-use-sqoop.md:6. To examine the modified data file, you can use Windows Azure Management portal, or a Windows Azure Storage explorer tool, or Windows Azure PowerShell.  [Get started with HDInsight][hdinsight-get-started] has a code sample on using PowerShell to download a file and display the file content.
hdinsight-use-sqoop.md:[sqoop-user-guide-1.4.4]: https://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html