stream-analytics-developer-guide.md:Each Stream Analytics job definition must contain at least one data stream input source to be consumed and transformed by the job.  [Azure Blob Storage][azure.blob.storage] and [Azure Service Bus Event Hubs][azure.event.hubs] are supported as data stream input sources.  Event Hub input sources are used to collect event streams from multiple different devices and services, while Blob Storage can be used an input source for ingesting large amounts of data.  Because Blobs do not stream data, Stream Analytics jobs over Blobs will not be temporal in nature unless the records in the Blob contain timestamps.
stream-analytics-developer-guide.md:In the preview release of Stream Analytics, stopping a job does not preserve any state about the last events consumed by the job.  As a result, restarting a stopped job can result in dropped events or duplicate data.  If a job must be stopped temporarily, the best practice is to inspect the output and use the insert time of the last record to approximate when the job stopped.  Then specify this time as the Start Output value when the job is restarted. This is a temporary limitation and enabling job start and stop without data loss is a high priority to fix in future releases.  
stream-analytics-developer-guide.md:- Out of order policy: Settings for handling events that do not arrive to the Stream Analytics job sequentially. You can designate a time threshold to reorder events within by specifying a Tolerance Window and also determine an action to take on events outside this window: Drop or Adjust.  Drop will drop all events received out of order and Adjust will change the System.Timestamp of out of order events to the timestamp of the most recently received ordered event.  
stream-analytics-developer-guide.md:- [Get started using Azure Stream Analytics][stream.analytics.get.started]
stream-analytics-developer-guide.md:[stream.analytics.get.started]: ../stream-analytics-get-started/
stream-analytics-get-started.md:	pageTitle="Get started using Azure Stream Analytics | Azure" 
stream-analytics-get-started.md:	description="Get started using Azure Stream Analytics to process and transform events in Azure Service Bus Event Hub and store the results to Azure SQL Database." 
stream-analytics-get-started.md:# Get started using Azure Stream Analytics
stream-analytics-get-started.md:To get you started quickly using Stream Analytics, this tutorial will show you how to consume device temperature reading data from an [Azure Service Bus Event Hub][azure.event.hubs.documentation] and process the data, outputting the results to an [Azure SQL Database][azure.sql.database.documentation].  The following diagram shows the flow of events from input to processing to output:
stream-analytics-get-started.md:![Azure Stream Analytics get started flow][img.get.started.flowchart]
stream-analytics-get-started.md:	- **REGION**: Select the region where you want to run the job. Azure Stream Analytics is currently only available in 2 regions during preview. For more information, see [Azure Stream Analytics limitations and known issues][stream.analytics.limitations]. Consider placing the job and the Event Hub in the same region to ensure better performance and that you will not be paying to transfer data between regions.
stream-analytics-get-started.md:		If your Event Hub is in a different subscription, select **Use Event Hub from Another Subscription** and manually enter the **SERVICE BUS NAMESPACE**, **EVENT HUB NAME**, **EVENT HUB POLICY NAME**, **EVENT HUB POLICY KEY**, and **EVENT HUB PARTITION COUNT**.  
stream-analytics-get-started.md:	- **SQL DATABASE**: Choose the SQL Database you created earlier in the tutorial.  If it is in -the same subscription, select the database from the dropdown menu.   If not, manually enter the Server Name and Database fields. 
stream-analytics-get-started.md:[img.stream.analytics.event.hub.client.output]: .\media\stream-analytics-get-started\AzureStreamAnalyticsEHClientOuput.png
stream-analytics-get-started.md:[img.stream.analytics.event.hub.shared.access.policy.config]: .\media\stream-analytics-get-started\AzureStreamAnalyticsEHSharedAccessPolicyConfig.png
stream-analytics-get-started.md:[img.stream.analytics.job.output2]: .\media\stream-analytics-get-started\AzureStreamAnalyticsSQLOutput2.png
stream-analytics-get-started.md:[img.stream.analytics.job.output1]: .\media\stream-analytics-get-started\AzureStreamAnalyticsSQLOutput1.png
stream-analytics-get-started.md:[img.stream.analytics.config.output]: .\media\stream-analytics-get-started\AzureStreamAnalyticsConfigureOutput.png
stream-analytics-get-started.md:[img.stream.analytics.config.input]: .\media\stream-analytics-get-started\AzureStreamAnalyticsConfigureInput.png
stream-analytics-get-started.md:[img.get.started.flowchart]: ./media/stream-analytics-get-started/StreamAnalytics.get.started.flowchart.png
stream-analytics-get-started.md:[img.job.quick.create]: ./media/stream-analytics-get-started/StreamAnalytics.quick.create.png
stream-analytics-get-started.md:[img.stream.analytics.portal.button]: ./media/stream-analytics-get-started/StreamAnalyticsPortalButton.png
stream-analytics-get-started.md:[img.event.hub.policy.configure]: ./media/stream-analytics-get-started/StreamAnalytics.Event.Hub.policy.png
stream-analytics-get-started.md:[img.create.table]: ./media/stream-analytics-get-started/StreamAnalytics.create.table.png
stream-analytics-get-started.md:[img.stream.analytics.job.output]: ./media/stream-analytics-get-started/StreamAnalytics.job.output.png
stream-analytics-get-started.md:[img.stream.analytics.operation.logs]: ./media/stream-analytics-get-started/StreamAnalytics.operation.log.png
stream-analytics-get-started.md:[img.stream.analytics.operation.log.details]: ./media/stream-analytics-get-started/StreamAnalytics.operation.log.details.png
stream-analytics-introduction.md:Building stream-processing systems is a complex task.  Streaming operations such as correlations and aggregations not only need to be implemented efficiently, but they also need to be scalable and fault-tolerant.  Additional operational challenges layer on this including deployment, debugging and monitoring.  The costs associated with building and maintaining such a solution quickly rise.  Larger enterprises have resigned themselves to paying such a high cost with custom built solutions, while smaller companies often miss the opportunity due to the high barrier to entry and the prohibitively expensive costs associated with it.  Azure Stream Analytics tackles these challenges.
stream-analytics-introduction.md:As devices become smarter and more devices are built with communication capabilities, the expectation of what can be done with the data generated and collected from these devices continues to evolve both in the commercial and consumer spaces. It is expected that with so much data available, we can quickly combine and process the data, gaining more insight into the environment around us, and the devices we use regularly. A canonical IoT Scenario can be described using the Vending Machine example. The Vending machines regularly send data such as product stock, status, temperature etc to either a field gateway (if the vending machine is Non-IP capable) or to a cloud gateway (if IP capable) for ingestion into the system. The incoming data stream is processed and transformed such that the computed output can be immediately fed back through the gateways to the device to take a corresponding action.  For example, if the machine is overheating, the device might need to reboot itself or automatically do a firmware upgrade without any human intervention.  The processed output can also trigger other alerts and notifications for a technician to be scheduled automatically based on the events. 
stream-analytics-introduction.md:As the volume of devices, machines and applications grow, a common enterprise use case to run businesses is the need to monitor and respond to changing business needs by creating rich analytics near real-time. The canonical Telemetry/Log Analysis scenario can be described using the Online Service or Application example, however the pattern is commonly seen across businesses that collect and report on application or device telemetry. The application or service regularly collects health data.  Data representing the current status of the application or infrastructure at a point in time, user request logs and other data representing actions or activities performed within the application are collected. The data is historically saved to a blob or other types of data store for further processing. With the recent trend towards real-time dashboarding, in addition to saving the data to a blob or other type of store for historical analysis, customers are looking to process and transform the stream of incoming data directly such that it can be immediately provided to end users in the form of Dashboards and/or Notifications when action needs to be taken. For example if a service site goes down, operations personnel can be notified to begin investigation and resolve the issue quickly. In several of these use cases, a human typically monitors a real-time dashboard built on top of the data set updated after processing of the ingested data via Stream Analytics. 
stream-analytics-introduction.md:- [Get started using Azure Stream Analytics][stream.analytics.get.started]
stream-analytics-introduction.md:[stream.analytics.get.started]: ../stream-analytics-get-started/
stream-analytics-limitations.md:+ [Release notes and known issues](#knownissues)
stream-analytics-limitations.md:In this preview release, the number of Streaming Units provided to a job may sometimes be higher than the amount selected or billed.  Additionally, Streaming Units will not be throttled down, meaning that observed performance may be higher than guaranteed depending on the availability of computational resources.
stream-analytics-limitations.md:At this time Stream Analytics does not support live edits to the definition or configuration of a running job.  In order to change the input, output, query, scale or configuration of a running job, you must first stop the job.
stream-analytics-limitations.md:Stopping a job does not preserve any state about job progress, meaning that there is currently no way to configure a restarted job to resume from where it was last stopped.  This is a limitation that will be addressed in a future release.  For best practices on starting and stopping jobs, see [Azure Stream Analytics developer guide][stream.analytics.developer.guide]. 
stream-analytics-limitations.md:Some metrics related to job usage and performance, such as latency, are not available in the preview release.  The preview release also only surfaces job throughput in terms of event count, not size.
stream-analytics-limitations.md:##<a name="knownissues"></a>Release notes / known issues
stream-analytics-limitations.md:When creating a Stream Analytics job in a region for the first time, you will be prompted to create a new storage account or specify an existing account for monitoring Stream Analytics jobs in that region.  Due to latency in configuring monitoring, creating another Stream Analytics job in the same region within 30 minutes will prompt for the specifying of a second storage account instead of showing the recently configured one in the Monitoring Storage Account Dropdown.  To avoid creating an unnecessary storage account, wait 30 minutes after creating a job in a region for the first time before provisioning additional jobs in that region. 
stream-analytics-limitations.md:The third page of the Add Input and Add Output dialogs for Event Hub sources has a dropdown titled Event Hub which contains both a list of Service Bus namespaces in the current subscription and an option to connect to an Event Hub in a different subscription.  If you wish to connect to an Event Hub in the same subscription, select its Service Bus namespace here.  If you wish to connect to an Event Hub outside of the subscription, select “Use Event Hub from Another Subscription”.  
stream-analytics-limitations.md:###Cannot reference the same query step more than once
stream-analytics-limitations.md:In this preview release a given sub-query step defined using the WITH keyword cannot be referenced more than once.  A common scenario that this may impact is a self-join using aliases of the same step.  To workaround this behavior, please create two separate steps with the same sub-query and different names.
stream-analytics-limitations.md:Any event vales with type conversions not supported in the Data Types section of [Azure Stream Analytics Query Language Reference][stream.analytics.query.language.reference] will result in a NULL value.  In this preview release no error logging is in place for these conversion exceptions. 
stream-analytics-limitations.md:When running a partitioned query with a non-partitioned sub-query as the second step, if one of the Event Hub partitions on the input is completely empty, the query will not generate results. An error for this will be reflected in the Operation Logs for the job.  Please make sure all Event Hub partitions have incoming events at all times to avoid this problem.
stream-analytics-limitations.md:###Large blob inputs not supported
stream-analytics-limitations.md:Stream Analytics does not trim whitespace on column headers. Including whitespace at the beginning or end or a column name will result in null entries in the job output.   
stream-analytics-limitations.md:- [Get started using Azure Stream Analytics][stream.analytics.get.started]
stream-analytics-limitations.md:[Release notes and known issues]: #Release-notes-and-known-issues
stream-analytics-limitations.md:[stream.analytics.get.started]: ../stream-analytics-get-started/
stream-analytics-limitations.md:[Link 1 to another azure.microsoft.com documentation topic]: ../virtual-machines-windows-tutorial/
stream-analytics-limitations.md:[Link 2 to another azure.microsoft.com documentation topic]: ../web-sites-custom-domain-name/
stream-analytics-limitations.md:[Link 3 to another azure.microsoft.com documentation topic]: ../storage-whatis-account/
stream-analytics-monitor-and-manage-jobs-use-powershell.md:>WACOM.NOTE The following error messages indicates that Azure Stream Analytics is not enabled on the subscription:
stream-analytics-monitor-and-manage-jobs-use-powershell.md:	Error Code: InvalidResourceType.  Error Message: The resource type 'streamingjobs' could not be found in the namespace 'Microsoft.StreamAnalytics'.  
stream-analytics-monitor-and-manage-jobs-use-powershell.md:>[AZURE.NOTE] There is a temporary limitation where Stream Analytics jobs created via Azure PowerShell do not have monitoring enabled.  To workaround this issue, navigate to the job’s Monitor page in the Azure Portal and click the “Enable” button.  
stream-analytics-monitor-and-manage-jobs-use-powershell.md:If you specify an input that already exists and do not specify –Force parameter, the cmdlet will ask whether or not to replace the existing input.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:This command creates a new input from the file Input.json.  If an existing input with the name specified in the input definition file is already defined, the cmdlet will ask whether or not to replace it.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:This command creates a new input on the job called EntryStream.  If an existing input with this name is already defined, the cmdlet will ask whether or not to replace it.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:If you specify a job name that already exists and do not specify –Force parameter, the cmdlet will ask whether or not to replace the existing job.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:This command creates a new job from the definition in JobDefinition.json.  If an existing job with the name specified in the job definition file is already defined, the cmdlet will ask whether or not to replace it.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:If you specify an output that already exists and do not specify –Force parameter, the cmdlet will ask whether or not to replace the existing output.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:This command creates a new output called "output" in the job StreamingJob.  If an existing output with this name is already defined, the cmdlet will ask whether or not to replace it.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:If you specify a transformation that already exists and do not specify –Force parameter, the cmdlet will ask whether or not to replace the existing transformation.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:This command creates a new transformation called StreamingJobTransform in the job StreamingJob.  If an existing transformation is already defined with this name, the cmdlet will ask whether or not to replace it.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:Asynchronously stops a Stream Analytics job from running in Microsoft Azure and de-allocates resources that were that were being used. The job definition and meta-data will remain available within your subscription through both the Azure Portal and Management APIs, such that the job can be edited and restarted. You will not be charged for a job in the Stopped state.
stream-analytics-monitor-and-manage-jobs-use-powershell.md:- [Get started with Stream Analytics][stream.analytics.get.started]
stream-analytics-monitor-and-manage-jobs-use-powershell.md:[stream-analytics-get-started]
stream-analytics-monitor-and-manage-jobs-use-powershell.md:[stream.analytics.get.started]: ../stream-analytics-get-started/
stream-analytics-real-time-event-processing-reference-architecture.md:Traditionally, analytics solutions have been based on capabilities such as ETL and Data Warehousing, where data is stored prior to analysis. Changing requirements, including more rapidly arriving data are pushing this existing model to the limit. The ability to analyze data within moving streams prior to storage is one solution, and whilst it is not a new capability, the approach has not been widely adopted across all industry verticals. 
stream-analytics-scale-jobs.md:An Azure Stream Analytics job definition includes Inputs, Query and Output. Inputs are where the job reads data stream from, the output is where the job send the job results to, and the query is used to transform the input stream.  A job requires at least one data stream input source. The data stream input source can be either an Azure Service Bus Event Hub or an Azure Blob storage. For more information, see [Introduction to Azure Stream Analytics][stream.analytics.introduction], [Get started using Azure Stream Analytics][stream.analytics.get.started], and [Azure Stream Analytics developer guide][stream.analytics.developer.guide]. 
stream-analytics-scale-jobs.md:<li>The step is not partitioned.</li>
stream-analytics-scale-jobs.md:When a query is partitioned, the input events are processed and aggregated in separate partition groups, and outputs events are also generated for each of the groups. Partitioning can cause some unexpected results when the group-by field is not the Partition Key in the data stream input. For example, the TollBoothId field in the previous sample query is not the Partition Key of Input1. The data from the tollbooth #1 can be spread in multiple partitions. Each of the Input1 partitions will be processed separately by Stream Analytics, and multiple records of the car-pass-through count of the same tollbooth in the same tumbling window will be created. In case the input partition key can't be changed, this problem can be fixed fixed by adding an additional non-partition step. For example:
stream-analytics-scale-jobs.md:- [Get started using Azure Stream Analytics][stream.analytics.get.started]
stream-analytics-scale-jobs.md:[stream.analytics.get.started]: ../stream-analytics-get-started/
