#Use Azure Data Lake Store from Apache Storm with HDInsight

Azure Data Lake Store is an HDFS compatible cloud storage service that provides high throughput, availability, durability, and reliability for your data. In this document, you will learn how to use a Java-based Storm topology to write data to Azure Data Lake Store using the [HdfsBolt](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html) component, which is provided as part of Apache Storm.

> [AZURE.IMPORTANT] [TBD note about needing some pre-compiled bits or building from trunk]

##Prerequisites

* Java SDK 1.7 or higher
* Maven 3.x
* An Azure subscription

###Configure environment variables

The following environment variables may be set when you install Java and the JDK on your development workstation. However, you should check that they exist and that they contain the correct values for your system.

* __JAVA_HOME__ - should point to the directory where the Java runtime environment (JRE) is installed. For example, in a Unix or Linux distribution, it should have a value similar to `/usr/lib/jvm/java-7-oracle`. In Windows, it would have a value similar to `c:\Program Files (x86)\Java\jre1.7`.

* __PATH__ - should contain the following paths:

    * __JAVA\_HOME__ (or the equivalent path)
    
    * __JAVA\_HOME\bin__ (or the equivalent path)
    
    * The directory where Maven is installed

##Topology implementation

The example used in this document is written in Java, and uses the following components:

* __TickSpout__: Generates the event data used by other components in the topology.

* __PartialCount__: Counts events generated by TickSpout.

* __FinalCount__: Aggregates count data from PartialCount.

* __ADLStoreBolt__: Writes data to Azure Data Lake Store using the [HdfsBolt](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html) component.

The project containing this topology is available as a download from [TBD].

###Understanding ADLStoreBolt

The ADLStoreBolt component internally uses HdfsBolt, which writes to HDFS. Microsoft has contributed code to the Apache Hadoop and Storm projects that enables it to also use Azure Data Lake Store and Azure Blob storage.

> [AZURE.NOTE] While support for using Azure Data Lake and Azure Blob storage is built into HDInsight, you must also manually add support libraries for this functionality to your Storm project. This is because the version of HdfsBolt and other common Hadoop components that it relies on do not include this functionality yet. Components that include this functionality will be available as part of Hadoop version 2.7.


##Create an HDInsight cluster and Data Lake Store

Create a new Storm on HDInsight cluster using the steps in the [Use HDInsight with Data Lake Store using Azure](data-lake-store-hdinsight-hadoop-use-portal.md) document. The steps in this document will walk you through creating a new HDInsight cluster and Azure Data Lake Store.

> [AZURE.IMPORTANT] When you create the HDInsight cluster, you must select __Storm__ as the cluster type. The OS can be either Windows or Linux.

##