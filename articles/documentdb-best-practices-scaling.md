<properties title="Scaling with Microsoft Azure DocumentDB" pageTitle="Scaling with Microsoft Azure DocumentDB | Azure" description="required" metaKeywords="" services="" solutions="" documentationCenter="" authors="" videoId="" scriptId="" />

<tags ms.service="documentdb" ms.workload="big-data" ms.tgt_pltfrm="na" ms.devlang="na" ms.topic="article" ms.date="01/01/1900" ms.author="" />


#Scaling with Microsoft Azure DocumentDB

Microsoft Azure DocumentDB allows you to scale elastically as the demands of your application change throughout its lifecycle. Scaling DocumentDB is accomplished by increasing the capacity of your DocumentDB Database Account through the Microsoft Azure portal. When you create a Database Account, it is provisioned with database storage and reserved throughput. At any time you can change the provisioned database storage and throughput for your Account by adding or removing service capacity units expressed as database storage through the Azure portal.  

DocumentDB operates on a reserved capacity model with granular resource increments to express your capacity needs. If the capacity requirements of your application change, you can scale out or scale back the amount of provisioned capacity in your Account.      Capacity provisioned under a Database Account is available for all databases and collections that exist or are created within the Account.  

DocumentDB will allocate your provisioned capacity to collections within your Database Account based on the data stored in each collection. This provides you with control and flexibility over how capacity is allocated within an Account. There is no practical limit on the size of a Database Account or Database; however, operational limits are in place to govern consumption patterns of the service based on available capacity per region. Refer to the service limits and defaults for more information on DocumentDB’s operational limits and default values. Provisioned capacity is allocated to a DocumentDB collection upon creation and collections can grow elastically based on the data stored in the collection. Capacity is expressed as size in GB and includes a corresponding allocation of reserved throughput. By default, DocumentDB will pre-allocate the minimum (todo: hyperlink to defaults and limits) ‘collection size’ for each new collection and allocate additional storage in GB increments as a collection grows. For each additional allocation of storage a proportionate amount of throughput is allocated to the collection. With this capability, collections can dynamically scale in size and throughput.  

##Managing Capacity
Provisioned capacity can be added or removed through the Azure Portal or through the Azure Management APIs. Capacity will be available for use within minutes of adjusting the provisioned capacity level for your Account. Capacity is expressed in discrete units through the Microsoft Azure portal and Azure management APIs. Each unit includes an allocation of 10GB of database storage and associated throughput. You can reduce the capacity provisioned for your Database Account through the Azure portal and Azure management APIs. Capacity that has not been allocated to a collection can be freed from the Account by scaling the capacity slider back in the Azure portal in 10GB increments. To free up additional capacity, you must delete existing collections or data stored in those collections.  
 
##Partitioning Document Data 
Each JSON document in DocumentDB belongs to a single collection within a database. Based on your application scenarios and design, you may choose to create multiple collections for your documents. When deciding to spread data across collections, the following factors should be considered:  

1.	Collection size limits – collections can expand and will consume provisioned capacity up to the maximum size limit of a single collection. If you expect your document data to grow beyond the maximum size of a single collection, you will need to spread your data across multiple collections. 
2.	Query and transaction boundaries – collections provide the boundary for both queries and transactions. Transactional execution of JavaScript as a script or trigger is always scoped to a single collection. Queries that span collections require application logic to fan out queries and aggregate result sets. Scripts that are executed across multiple collections require application logic to coordinate execution.
3.	Frequently accessed data – as collections expand in size they are allocated proportionate throughput capacity to serve read and write requests. If a dataset includes a set of frequently accessed (or hot) documents, consideration should be given to spreading these documents across one or more collections to ensure that request throttling is localized to a hot collection and does not impact read and write availability of other documents. Request rate throttling is applied on a per collection basis.
4.	Document use case heterogeneity – by default DocumentDB will index all properties contained in a document. An index is scoped to a collection and consumes storage from provisioned database storage. Indexing policies (todo: hyperlink to indexing policies) can be used to tune how and when documents should be indexed. If your application has different use cases for document types, it may call for different default indexing behaviors. For these cases, multiple collections may be appropriate.  

For many application scenarios a single collection can be sufficient to meet the desired performance and access patterns. DocumentDB provides efficient query and indexing of JSON documents at scale. This provides the ability to store, query and retrieve documents by entity type, tenant ID or any other property which may have necessitated a separate container (table, collection) in another system.
